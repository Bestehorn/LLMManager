{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Extended Context Window Demonstration\n",
                "\n",
                "This notebook demonstrates the **Extended Context Window** feature for AWS Bedrock's Claude Sonnet 4 model, which supports up to **1 million tokens** in a single request.\n",
                "\n",
                "## Key Features\n",
                "\n",
                "- üöÄ **Simple Flag**: Use `enable_extended_context=True` for easy activation\n",
                "- üîß **Manual Control**: Pass custom `additionalModelRequestFields` for fine-grained control\n",
                "- üìä **Token Usage**: Monitor input/output token consumption\n",
                "- üîÑ **Automatic Fallback**: Graceful handling when extended context isn't supported\n",
                "- üéØ **Parameter Tracking**: Learn which model/region combinations support which parameters\n",
                "\n",
                "## ‚ö†Ô∏è Important Notes\n",
                "\n",
                "- **Beta Feature**: Extended context is a beta service as defined in AWS Service Terms\n",
                "- **Model Support**: Currently only Claude Sonnet 4 supports 1M token context\n",
                "- **Pricing**: Separate pricing applies for extended context usage\n",
                "- **Quotas**: Separate service quotas apply\n",
                "- **Region Availability**: Not all regions may support this beta feature\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- AWS credentials configured with Bedrock access\n",
                "- Access to Claude Sonnet 4 model in your AWS account\n",
                "- Sufficient quota for extended context usage"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Imports successful!\n",
                        "üìÅ Working directory: d:\\Users\\bestem\\Code Workspace\\LLMManager\\notebooks\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import json\n",
                "from pathlib import Path\n",
                "import logging\n",
                "\n",
                "# Add the src directory to path for imports\n",
                "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
                "\n",
                "# Import the LLMManager and related classes\n",
                "from bestehorn_llmmanager import LLMManager, ParallelLLMManager\n",
                "from bestehorn_llmmanager.bedrock.models.llm_manager_structures import (\n",
                "    AuthConfig, RetryConfig, AuthenticationType, RetryStrategy\n",
                ")\n",
                "from bestehorn_llmmanager.bedrock.models.model_specific_structures import ModelSpecificConfig\n",
                "from bestehorn_llmmanager.bedrock.models.parallel_structures import BedrockConverseRequest\n",
                "from bestehorn_llmmanager.bedrock.tracking.parameter_compatibility_tracker import (\n",
                "    ParameterCompatibilityTracker\n",
                ")\n",
                "\n",
                "# Configure logging for better visibility\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Imports successful!\")\n",
                "print(f\"üìÅ Working directory: {Path.cwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Helper functions defined!\n"
                    ]
                }
            ],
            "source": [
                "def display_response(response, title=\"Response\"):\n",
                "    \"\"\"Display a formatted response from LLMManager.\"\"\"\n",
                "    print(f\"\\n{title}\")\n",
                "    print(\"=\" * len(title))\n",
                "    \n",
                "    if response.success:\n",
                "        print(f\"‚úÖ Success: {response.success}\")\n",
                "        print(f\"ü§ñ Model: {response.model_used}\")\n",
                "        print(f\"üåç Region: {response.region_used}\")\n",
                "        print(f\"‚è±Ô∏è  Total Duration: {response.total_duration_ms:.2f}ms\")\n",
                "        \n",
                "        # Display content\n",
                "        content = response.get_content()\n",
                "        if content:\n",
                "            print(f\"\\nüí¨ Response Content:\")\n",
                "            print(\"-\" * 20)\n",
                "            # Truncate long responses for readability\n",
                "            if len(content) > 500:\n",
                "                print(content[:500] + \"...\\n[truncated]\")\n",
                "            else:\n",
                "                print(content)\n",
                "        \n",
                "        # Display usage statistics\n",
                "        input_tokens = response.get_input_tokens()\n",
                "        output_tokens = response.get_output_tokens()\n",
                "        total_tokens = response.get_total_tokens()\n",
                "        \n",
                "        if total_tokens > 0:\n",
                "            print(f\"\\nüìä Token Usage:\")\n",
                "            print(f\"   Input Tokens: {input_tokens:,}\")\n",
                "            print(f\"   Output Tokens: {output_tokens:,}\")\n",
                "            print(f\"   Total Tokens: {total_tokens:,}\")\n",
                "        \n",
                "        # Display parameter information\n",
                "        if response.had_parameters_removed():\n",
                "            print(f\"\\n‚ö†Ô∏è  Parameters Removed: {response.parameters_removed}\")\n",
                "            warnings = response.get_parameter_warnings()\n",
                "            if warnings:\n",
                "                print(f\"   Warnings:\")\n",
                "                for warning in warnings:\n",
                "                    print(f\"      - {warning}\")\n",
                "    else:\n",
                "        print(f\"‚ùå Success: {response.success}\")\n",
                "        print(f\"üîÑ Attempts: {len(response.attempts)}\")\n",
                "    \n",
                "    if response.warnings:\n",
                "        print(f\"\\n‚ö†Ô∏è  Warnings:\")\n",
                "        for warning in response.warnings:\n",
                "            print(f\"   - {warning}\")\n",
                "\n",
                "def create_large_text(size_kb: int = 100) -> str:\n",
                "    \"\"\"Create a large text string for testing extended context.\"\"\"\n",
                "    # Approximate: 1 token ‚âà 4 characters\n",
                "    # 1 KB ‚âà 250 tokens\n",
                "    base_text = \"This is a sample text for testing extended context windows. \"\n",
                "    repetitions = (size_kb * 1024) // len(base_text)\n",
                "    return base_text * repetitions\n",
                "\n",
                "print(\"‚úÖ Helper functions defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 1: Simple Extended Context with Flag üöÄ\n",
                "\n",
                "The easiest way to enable extended context is using the `enable_extended_context=True` flag.\n",
                "This automatically adds the required beta header for Claude Sonnet 4."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Example 1: Simple Extended Context with Flag\n",
                        "==================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-21 10:54:26,096 - bestehorn_llmmanager.bedrock.catalog.api_fetcher - WARNING - Region ap-southeast-3 failed: Unexpected error fetching foundation models: Failed to create authenticated session: Failed to create session: An error occurred (InvalidClientTokenId) when calling the GetCallerIdentity operation: The security token included in the request is invalid. Details: {'auth_type': 'profile', 'region': 'ap-southeast-3'}. Details: {'auth_type': 'profile', 'region': 'ap-southeast-3'}. Details: {'region': 'ap-southeast-3'}\n",
                        "2026-01-21 10:54:33,197 - bestehorn_llmmanager.bedrock.catalog.api_fetcher - WARNING - Region ap-southeast-4 failed: Unexpected error fetching foundation models: Failed to create authenticated session: Failed to create session: An error occurred (InvalidClientTokenId) when calling the GetCallerIdentity operation: The security token included in the request is invalid. Details: {'auth_type': 'profile', 'region': 'ap-southeast-4'}. Details: {'auth_type': 'profile', 'region': 'ap-southeast-4'}. Details: {'region': 'ap-southeast-4'}\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÑ Created text of approximately 2,559,960 characters\n",
                        "   Estimated tokens: ~639,990\n",
                        "\n",
                        "üîÑ Sending request with enable_extended_context=True...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-21 10:54:56,265 - bestehorn_llmmanager.bedrock.retry.retry_manager - WARNING - Request failed with model 'Claude Sonnet 4' in region 'us-east-1': An error occurred (ValidationException) when calling the Converse operation: Invocation of model ID anthropic.claude-sonnet-4-20250514-v1:0 with on-demand throughput isn‚Äôt supported. Retry your request with the ID or ARN of an inference profile that contains this model.\n",
                        "2026-01-21 10:54:56,267 - bestehorn_llmmanager.bedrock.retry.retry_manager - WARNING - Profile requirement detected for model 'Claude Sonnet 4' in region 'us-east-1'. Model ID from error: anthropic.claude-sonnet-4-20250514-v1:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üöÄ Extended Context Response\n",
                        "===========================\n",
                        "‚úÖ Success: True\n",
                        "ü§ñ Model: Claude Sonnet 4\n",
                        "üåç Region: us-east-1\n",
                        "‚è±Ô∏è  Total Duration: 27005.01ms\n",
                        "\n",
                        "üí¨ Response Content:\n",
                        "--------------------\n",
                        "The provided text consists entirely of the same sentence repeated thousands of times: \"This is a sample text for testing extended context windows.\" This appears to be a test document designed to evaluate how well AI systems can handle and process very long, repetitive content within their context window limitations. The repetitive nature suggests it's specifically created to test extended context processing capabilities rather than convey any meaningful information.\n",
                        "\n",
                        "üìä Token Usage:\n",
                        "   Input Tokens: 469,349\n",
                        "   Output Tokens: 82\n",
                        "   Total Tokens: 469,431\n",
                        "\n",
                        "‚ö†Ô∏è  Warnings:\n",
                        "   - Model 'Claude Sonnet 4' in region 'us-east-1' requires inference profile access. Using regional_cris profile.\n",
                        "\n",
                        "‚úÖ Extended context feature was successfully enabled!\n",
                        "   The model can now handle up to 1 million tokens.\n"
                    ]
                }
            ],
            "source": [
                "print(\"üöÄ Example 1: Simple Extended Context with Flag\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Initialize manager with Claude Sonnet 4\n",
                "manager = LLMManager(\n",
                "    models=[\"Claude Sonnet 4\"],\n",
                "    regions=[\"us-east-1\"],\n",
                "    auth_config=AuthConfig(\n",
                "        auth_type=AuthenticationType.PROFILE,\n",
                "        profile_name=\"default\"\n",
                "    )\n",
                ")\n",
                "\n",
                "# Create a large text (simulating a large document)\n",
                "# For demo purposes, we'll use a moderate size\n",
                "# In production, you could use up to ~1M tokens\n",
                "large_text = create_large_text(size_kb=2500)  # 50kb ~ 12,500 tokens\n",
                "print(f\"üìÑ Created text of approximately {len(large_text):,} characters\")\n",
                "print(f\"   Estimated tokens: ~{len(large_text) // 4:,}\")\n",
                "\n",
                "# Create message\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\n",
                "                \"text\": f\"Please summarize the following text in 2-3 sentences:\\n\\n{large_text}\"\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "try:\n",
                "    # Use extended context with simple flag\n",
                "    print(\"\\nüîÑ Sending request with enable_extended_context=True...\")\n",
                "    response = manager.converse(\n",
                "        messages=messages,\n",
                "        enable_extended_context=True,\n",
                "        inference_config={\n",
                "            \"maxTokens\": 512,\n",
                "            \"temperature\": 0.3\n",
                "        }\n",
                "    )\n",
                "    \n",
                "    display_response(response, \"üöÄ Extended Context Response\")\n",
                "    \n",
                "    # Show that extended context was used\n",
                "    print(\"\\n‚úÖ Extended context feature was successfully enabled!\")\n",
                "    print(\"   The model can now handle up to 1 million tokens.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")\n",
                "    print(f\"   Type: {type(e).__name__}\")\n",
                "    print(\"\\nüí° Troubleshooting:\")\n",
                "    print(\"   - Ensure you have access to Claude Sonnet 4\")\n",
                "    print(\"   - Check that your region supports the extended context beta\")\n",
                "    print(\"   - Verify your AWS credentials are configured correctly\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 2: Manual additionalModelRequestFields üîß\n",
                "\n",
                "For more control, you can manually specify `additionalModelRequestFields`.\n",
                "This allows you to combine multiple beta features or use other model-specific parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîß Example 2: Manual additionalModelRequestFields\n",
                        "==================================================\n",
                        "\n",
                        "üîÑ Sending request with manual additionalModelRequestFields...\n",
                        "\n",
                        "üîß Manual Configuration Response\n",
                        "===============================\n",
                        "‚úÖ Success: True\n",
                        "ü§ñ Model: Claude Sonnet 4\n",
                        "üåç Region: us-east-1\n",
                        "‚è±Ô∏è  Total Duration: 11355.49ms\n",
                        "\n",
                        "üí¨ Response Content:\n",
                        "--------------------\n",
                        "# Extended Context Windows in Large Language Models\n",
                        "\n",
                        "## What is a Context Window?\n",
                        "\n",
                        "A **context window** is the maximum amount of text (measured in tokens) that a language model can process and remember at one time. Think of it as the model's \"working memory\" - it determines how much previous conversation, document content, or input the model can reference when generating responses.\n",
                        "\n",
                        "## Traditional Limitations\n",
                        "\n",
                        "Early language models had relatively small context windows:\n",
                        "- **GPT-3**: ~4,000 tokens...\n",
                        "[truncated]\n",
                        "\n",
                        "üìä Token Usage:\n",
                        "   Input Tokens: 20\n",
                        "   Output Tokens: 500\n",
                        "   Total Tokens: 520\n",
                        "\n",
                        "‚úÖ Successfully used manual additionalModelRequestFields!\n",
                        "   This approach gives you full control over model-specific parameters.\n"
                    ]
                }
            ],
            "source": [
                "print(\"üîß Example 2: Manual additionalModelRequestFields\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Create a message\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\n",
                "                \"text\": \"Explain the concept of extended context windows in large language models.\"\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "try:\n",
                "    # Pass custom model-specific parameters directly\n",
                "    print(\"\\nüîÑ Sending request with manual additionalModelRequestFields...\")\n",
                "    response = manager.converse(\n",
                "        messages=messages,\n",
                "        additional_model_request_fields={\n",
                "            \"anthropic_beta\": [\n",
                "                \"context-1m-2025-08-07\"  # Extended context beta header\n",
                "            ]\n",
                "        },\n",
                "        inference_config={\n",
                "            \"maxTokens\": 500,\n",
                "            \"temperature\": 0.5\n",
                "        }\n",
                "    )\n",
                "    \n",
                "    display_response(response, \"üîß Manual Configuration Response\")\n",
                "    \n",
                "    print(\"\\n‚úÖ Successfully used manual additionalModelRequestFields!\")\n",
                "    print(\"   This approach gives you full control over model-specific parameters.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")\n",
                "    print(f\"   Type: {type(e).__name__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 3: Token Usage Reporting üìä\n",
                "\n",
                "Monitor token consumption to understand the cost and performance of extended context requests."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üìä Example 3: Token Usage Reporting\")\n",
                "print(\"=\" * 40)\n",
                "\n",
                "# Create texts of different sizes\n",
                "test_sizes = [\n",
                "    (100, \"Small\"),\n",
                "    (500, \"Medium\"),\n",
                "    (1000, \"Large\")\n",
                "]\n",
                "\n",
                "results = []\n",
                "\n",
                "for size_kb, label in test_sizes:\n",
                "    text = create_large_text(size_kb=size_kb)\n",
                "    estimated_tokens = len(text) // 4\n",
                "    \n",
                "    messages = [\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": [\n",
                "                {\n",
                "                    \"text\": f\"Provide a one-sentence summary of this text:\\n\\n{text}\"\n",
                "                }\n",
                "            ]\n",
                "        }\n",
                "    ]\n",
                "    \n",
                "    try:\n",
                "        print(f\"\\nüîÑ Testing {label} text (~{estimated_tokens:,} tokens)...\")\n",
                "        response = manager.converse(\n",
                "            messages=messages,\n",
                "            enable_extended_context=True,\n",
                "            inference_config={\"maxTokens\": 100}\n",
                "        )\n",
                "        \n",
                "        if response.success:\n",
                "            results.append({\n",
                "                \"label\": label,\n",
                "                \"estimated\": estimated_tokens,\n",
                "                \"actual_input\": response.get_input_tokens(),\n",
                "                \"output\": response.get_output_tokens(),\n",
                "                \"total\": response.get_total_tokens()\n",
                "            })\n",
                "            print(f\"   ‚úÖ Input: {response.get_input_tokens():,} tokens\")\n",
                "            print(f\"   ‚úÖ Output: {response.get_output_tokens():,} tokens\")\n",
                "    \n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ùå Error: {e}\")\n",
                "        results.append({\n",
                "            \"label\": label,\n",
                "            \"estimated\": estimated_tokens,\n",
                "            \"error\": str(e)\n",
                "        })\n",
                "\n",
                "# Display summary\n",
                "print(\"\\nüìä Token Usage Summary\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"{'Size':<10} {'Estimated':<12} {'Actual Input':<15} {'Output':<10} {'Total':<10}\")\n",
                "print(\"-\" * 60)\n",
                "for result in results:\n",
                "    if 'error' not in result:\n",
                "        print(f\"{result['label']:<10} {result['estimated']:<12,} \"\n",
                "              f\"{result['actual_input']:<15,} {result['output']:<10,} \"\n",
                "              f\"{result['total']:<10,}\")\n",
                "    else:\n",
                "        print(f\"{result['label']:<10} {result['estimated']:<12,} ERROR\")\n",
                "\n",
                "print(\"\\nüí° Note: Extended context allows up to 1M tokens, but costs scale with usage.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 4: Handling Parameter Incompatibility üîÑ\n",
                "\n",
                "The system automatically handles cases where extended context isn't supported,\n",
                "falling back gracefully to standard context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîÑ Example 4: Handling Parameter Incompatibility\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Initialize manager with multiple models (some may not support extended context)\n",
                "multi_model_manager = LLMManager(\n",
                "    models=[\"Claude Sonnet 4\", \"Claude 3 Haiku\"],\n",
                "    regions=[\"us-east-1\", \"us-west-2\"],\n",
                "    auth_config=AuthConfig(\n",
                "        auth_type=AuthenticationType.PROFILE,\n",
                "        profile_name=\"default\"\n",
                "    ),\n",
                "    retry_config=RetryConfig(\n",
                "        max_retries=3,\n",
                "        retry_strategy=RetryStrategy.MODEL_FIRST\n",
                "    )\n",
                ")\n",
                "\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\n",
                "                \"text\": \"What are the benefits of extended context windows in AI models?\"\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "try:\n",
                "    print(\"\\nüîÑ Sending request with enable_extended_context=True...\")\n",
                "    print(\"   The system will try Claude Sonnet 4 first, then fall back if needed.\")\n",
                "    \n",
                "    response = multi_model_manager.converse(\n",
                "        messages=messages,\n",
                "        enable_extended_context=True,\n",
                "        inference_config={\"maxTokens\": 400}\n",
                "    )\n",
                "    \n",
                "    display_response(response, \"üîÑ Fallback Handling Response\")\n",
                "    \n",
                "    # Check if parameters were removed\n",
                "    if response.had_parameters_removed():\n",
                "        print(\"\\n‚ö†Ô∏è  Parameter Removal Detected:\")\n",
                "        print(f\"   Removed parameters: {response.parameters_removed}\")\n",
                "        print(f\"   Model used: {response.model_used}\")\n",
                "        print(f\"   Region used: {response.region_used}\")\n",
                "        print(\"\\nüí° The system automatically retried without extended context.\")\n",
                "    else:\n",
                "        print(\"\\n‚úÖ Extended context was supported by the selected model/region!\")\n",
                "        print(f\"   Model: {response.model_used}\")\n",
                "        print(f\"   Region: {response.region_used}\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")\n",
                "    print(f\"   Type: {type(e).__name__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 5: ModelSpecificConfig for Reusable Configuration üéØ\n",
                "\n",
                "Use `ModelSpecificConfig` to create reusable parameter configurations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üéØ Example 5: ModelSpecificConfig\")\n",
                "print(\"=\" * 35)\n",
                "\n",
                "# Create reusable configuration\n",
                "config = ModelSpecificConfig(\n",
                "    enable_extended_context=True,\n",
                "    custom_fields={\n",
                "        # You can add other model-specific parameters here\n",
                "    }\n",
                ")\n",
                "\n",
                "print(\"üìã Created ModelSpecificConfig:\")\n",
                "print(f\"   enable_extended_context: {config.enable_extended_context}\")\n",
                "print(f\"   custom_fields: {config.custom_fields}\")\n",
                "\n",
                "# Initialize manager with default config\n",
                "config_manager = LLMManager(\n",
                "    models=[\"Claude Sonnet 4\"],\n",
                "    regions=[\"us-east-1\"],\n",
                "    auth_config=AuthConfig(\n",
                "        auth_type=AuthenticationType.PROFILE,\n",
                "        profile_name=\"default\"\n",
                "    ),\n",
                "    model_specific_config=config  # Set as default\n",
                ")\n",
                "\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\n",
                "                \"text\": \"Explain how configuration objects improve code maintainability.\"\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "try:\n",
                "    print(\"\\nüîÑ Sending request (config applied automatically)...\")\n",
                "    # All requests use the config by default\n",
                "    response = config_manager.converse(\n",
                "        messages=messages,\n",
                "        inference_config={\"maxTokens\": 300}\n",
                "    )\n",
                "    \n",
                "    display_response(response, \"üéØ Config-Based Response\")\n",
                "    \n",
                "    print(\"\\n‚úÖ ModelSpecificConfig was applied automatically!\")\n",
                "    print(\"   This approach is great for consistent parameter usage across requests.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")\n",
                "    print(f\"   Type: {type(e).__name__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 6: Querying Parameter Compatibility üîç\n",
                "\n",
                "The system tracks which model/region combinations support which parameters.\n",
                "You can query this information for optimization and debugging."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîç Example 6: Querying Parameter Compatibility\")\n",
                "print(\"=\" * 45)\n",
                "\n",
                "# Get compatibility tracker\n",
                "tracker = ParameterCompatibilityTracker.get_instance()\n",
                "\n",
                "# Get statistics\n",
                "stats = tracker.get_statistics()\n",
                "\n",
                "print(\"\\nüìä Parameter Compatibility Statistics:\")\n",
                "print(f\"   Total tracked combinations: {stats.get('total_combinations', 0)}\")\n",
                "print(f\"   Compatible combinations: {stats.get('compatible_count', 0)}\")\n",
                "print(f\"   Incompatible combinations: {stats.get('incompatible_count', 0)}\")\n",
                "\n",
                "# Check specific combination\n",
                "test_params = {\"anthropic_beta\": [\"context-1m-2025-08-07\"]}\n",
                "is_incompatible = tracker.is_known_incompatible(\n",
                "    model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
                "    region=\"us-east-1\",\n",
                "    parameters=test_params\n",
                ")\n",
                "\n",
                "print(f\"\\nüîç Checking specific combination:\")\n",
                "print(f\"   Model: Claude Sonnet 4\")\n",
                "print(f\"   Region: us-east-1\")\n",
                "print(f\"   Parameters: {test_params}\")\n",
                "print(f\"   Known incompatible: {'‚ùå Yes' if is_incompatible else '‚úÖ No'}\")\n",
                "\n",
                "print(\"\\nüí° The tracker learns from each request to optimize future attempts.\")\n",
                "print(\"   This helps skip known incompatible combinations during retry.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary and Best Practices üìù\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "1. **Simple Flag**: Use `enable_extended_context=True` for easy activation\n",
                "2. **Manual Control**: Use `additionalModelRequestFields` for fine-grained control\n",
                "3. **Automatic Fallback**: System handles incompatibility gracefully\n",
                "4. **Token Monitoring**: Always check token usage for cost management\n",
                "5. **Configuration Objects**: Use `ModelSpecificConfig` for reusable settings\n",
                "\n",
                "### Best Practices\n",
                "\n",
                "- ‚úÖ **Monitor token usage** to understand costs\n",
                "- ‚úÖ **Use multi-model configuration** for automatic fallback\n",
                "- ‚úÖ **Check response warnings** to detect parameter removal\n",
                "- ‚úÖ **Test with smaller contexts** before scaling to 1M tokens\n",
                "- ‚úÖ **Verify region support** for beta features\n",
                "\n",
                "### Common Pitfalls\n",
                "\n",
                "- ‚ùå Assuming all regions support extended context\n",
                "- ‚ùå Not monitoring token consumption\n",
                "- ‚ùå Ignoring parameter removal warnings\n",
                "- ‚ùå Using extended context when standard context is sufficient\n",
                "\n",
                "### Additional Resources\n",
                "\n",
                "- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
                "- [Claude Model Documentation](https://docs.anthropic.com/)\n",
                "- [LLMManager Documentation](../docs/forLLMConsumption.md)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
