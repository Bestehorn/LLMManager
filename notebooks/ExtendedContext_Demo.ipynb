{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Extended Context Window Demonstration\n",
                "\n",
                "This notebook demonstrates the **Extended Context Window** feature for AWS Bedrock's Claude Sonnet 4 model, which supports up to **1 million tokens** in a single request.\n",
                "\n",
                "## Key Features\n",
                "\n",
                "- üöÄ **Simple Flag**: Use `enable_extended_context=True` for easy activation\n",
                "- üîß **Manual Control**: Pass custom `additionalModelRequestFields` for fine-grained control\n",
                "- üìä **Token Usage**: Monitor input/output token consumption\n",
                "- üîÑ **Automatic Fallback**: Graceful handling when extended context isn't supported\n",
                "- üéØ **Parameter Tracking**: Learn which model/region combinations support which parameters\n",
                "\n",
                "## ‚ö†Ô∏è Important Notes\n",
                "\n",
                "- **Beta Feature**: Extended context is a beta service as defined in AWS Service Terms\n",
                "- **Model Support**: Currently only Claude Sonnet 4 supports 1M token context\n",
                "- **Pricing**: Separate pricing applies for extended context usage\n",
                "- **Quotas**: Separate service quotas apply\n",
                "- **Region Availability**: Not all regions may support this beta feature\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- AWS credentials configured with Bedrock access\n",
                "- Access to Claude Sonnet 4 model in your AWS account\n",
                "- Sufficient quota for extended context usage"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import json\n",
                "from pathlib import Path\n",
                "import logging\n",
                "\n",
                "# Add the src directory to path for imports\n",
                "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
                "\n",
                "# Import the LLMManager and related classes\n",
                "from bestehorn_llmmanager import LLMManager, ParallelLLMManager\n",
                "from bestehorn_llmmanager.bedrock.models.llm_manager_structures import (\n",
                "    AuthConfig, RetryConfig, AuthenticationType, RetryStrategy\n",
                ")\n",
                "from bestehorn_llmmanager.bedrock.models.model_specific_structures import ModelSpecificConfig\n",
                "from bestehorn_llmmanager.bedrock.models.parallel_structures import BedrockConverseRequest\n",
                "from bestehorn_llmmanager.bedrock.tracking.parameter_compatibility_tracker import (\n",
                "    ParameterCompatibilityTracker\n",
                ")\n",
                "\n",
                "# Configure logging for better visibility\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Imports successful!\")\n",
                "print(f\"üìÅ Working directory: {Path.cwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def display_response(response, title=\"Response\"):\n",
                "    \"\"\"Display a formatted response from LLMManager.\"\"\"\n",
                "    print(f\"\\n{title}\")\n",
                "    print(\"=\" * len(title))\n",
                "    \n",
                "    if response.success:\n",
                "        print(f\"‚úÖ Success: {response.success}\")\n",
                "        print(f\"ü§ñ Model: {response.model_used}\")\n",
                "        print(f\"üåç Region: {response.region_used}\")\n",
                "        print(f\"‚è±Ô∏è  Total Duration: {response.total_duration_ms:.2f}ms\")\n",
                "        \n",
                "        # Display content\n",
                "        content = response.get_content()\n",
                "        if content:\n",
                "            print(f\"\\nüí¨ Response Content:\")\n",
                "            print(\"-\" * 20)\n",
                "            # Truncate long responses for readability\n",
                "            if len(content) > 500:\n",
                "                print(content[:500] + \"...\\n[truncated]\")\n",
                "            else:\n",
                "                print(content)\n",
                "        \n",
                "        # Display usage statistics\n",
                "        input_tokens = response.get_input_tokens()\n",
                "        output_tokens = response.get_output_tokens()\n",
                "        total_tokens = response.get_total_tokens()\n",
                "        \n",
                "        if total_tokens > 0:\n",
                "            print(f\"\\nüìä Token Usage:\")\n",
                "            print(f\"   Input Tokens: {input_tokens:,}\")\n",
                "            print(f\"   Output Tokens: {output_tokens:,}\")\n",
                "            print(f\"   Total Tokens: {total_tokens:,}\")\n",
                "        \n",
                "        # Display parameter information\n",
                "        if response.had_parameters_removed():\n",
                "            print(f\"\\n‚ö†Ô∏è  Parameters Removed: {response.parameters_removed}\")\n",
                "            warnings = response.get_parameter_warnings()\n",
                "            if warnings:\n",
                "                print(f\"   Warnings:\")\n",
                "                for warning in warnings:\n",
                "                    print(f\"      - {warning}\")\n",
                "    else:\n",
                "        print(f\"‚ùå Success: {response.success}\")\n",
                "        print(f\"üîÑ Attempts: {len(response.attempts)}\")\n",
                "    \n",
                "    if response.warnings:\n",
                "        print(f\"\\n‚ö†Ô∏è  Warnings:\")\n",
                "        for warning in response.warnings:\n",
                "            print(f\"   - {warning}\")\n",
                "\n",
                "def create_large_text(size_kb: int = 100) -> str:\n",
                "    \"\"\"Create a large text string for testing extended context.\"\"\"\n",
                "    # Approximate: 1 token ‚âà 4 characters\n",
                "    # 1 KB ‚âà 250 tokens\n",
                "    base_text = \"This is a sample text for testing extended context windows. \"\n",
                "    repetitions = (size_kb * 1024) // len(base_text)\n",
                "    return base_text * repetitions\n",
                "\n",
                "print(\"‚úÖ Helper functions defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 1: Simple Extended Context with Flag üöÄ\n",
                "\n",
                "The easiest way to enable extended context is using the `enable_extended_context=True` flag.\n",
                "This automatically adds the required beta header for Claude Sonnet 4."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üöÄ Example 1: Simple Extended Context with Flag\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Initialize manager with Claude Sonnet 4\n",
                "manager = LLMManager(\n",
                "    models=[\"Claude Sonnet 4\"],\n",
                "    regions=[\"us-east-1\"],\n",
                "    auth_config=AuthConfig(\n",
                "        auth_type=AuthenticationType.PROFILE,\n",
                "        profile_name=\"default\"\n",
                "    )\n",
                ")\n",
                "\n",
                "# Create a large text (simulating a large document)\n",
                "# For demo purposes, we'll use a moderate size\n",
                "# In production, you could use up to ~1M tokens\n",
                "large_text = create_large_text(size_kb=50)  # ~12,500 tokens\n",
                "print(f\"üìÑ Created text of approximately {len(large_text):,} characters\")\n",
                "print(f\"   Estimated tokens: ~{len(large_text) // 4:,}\")\n",
                "\n",
                "# Create message\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\n",
                "                \"text\": f\"Please summarize the following text in 2-3 sentences:\\n\\n{large_text}\"\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "try:\n",
                "    # Use extended context with simple flag\n",
                "    print(\"\\nüîÑ Sending request with enable_extended_context=True...\")\n",
                "    response = manager.converse(\n",
                "        messages=messages,\n",
                "        enable_extended_context=True,\n",
                "        inference_config={\n",
                "            \"maxTokens\": 512,\n",
                "            \"temperature\": 0.3\n",
                "        }\n",
                "    )\n",
                "    \n",
                "    display_response(response, \"üöÄ Extended Context Response\")\n",
                "    \n",
                "    # Show that extended context was used\n",
                "    print(\"\\n‚úÖ Extended context feature was successfully enabled!\")\n",
                "    print(\"   The model can now handle up to 1 million tokens.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")\n",
                "    print(f\"   Type: {type(e).__name__}\")\n",
                "    print(\"\\nüí° Troubleshooting:\")\n",
                "    print(\"   - Ensure you have access to Claude Sonnet 4\")\n",
                "    print(\"   - Check that your region supports the extended context beta\")\n",
                "    print(\"   - Verify your AWS credentials are configured correctly\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 2: Manual additionalModelRequestFields üîß\n",
                "\n",
                "For more control, you can manually specify `additionalModelRequestFields`.\n",
                "This allows you to combine multiple beta features or use other model-specific parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîß Example 2: Manual additionalModelRequestFields\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Create a message\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\n",
                "                \"text\": \"Explain the concept of extended context windows in large language models.\"\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "try:\n",
                "    # Pass custom model-specific parameters directly\n",
                "    print(\"\\nüîÑ Sending request with manual additionalModelRequestFields...\")\n",
                "    response = manager.converse(\n",
                "        messages=messages,\n",
                "        additional_model_request_fields={\n",
                "            \"anthropic_beta\": [\n",
                "                \"context-1m-2025-08-07\"  # Extended context beta header\n",
                "            ]\n",
                "        },\n",
                "        inference_config={\n",
                "            \"maxTokens\": 500,\n",
                "            \"temperature\": 0.5\n",
                "        }\n",
                "    )\n",
                "    \n",
                "    display_response(response, \"üîß Manual Configuration Response\")\n",
                "    \n",
                "    print(\"\\n‚úÖ Successfully used manual additionalModelRequestFields!\")\n",
                "    print(\"   This approach gives you full control over model-specific parameters.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")\n",
                "    print(f\"   Type: {type(e).__name__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 3: Token Usage Reporting üìä\n",
                "\n",
                "Monitor token consumption to understand the cost and performance of extended context requests."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üìä Example 3: Token Usage Reporting\")\n",
                "print(\"=\" * 40)\n",
                "\n",
                "# Create texts of different sizes\n",
                "test_sizes = [\n",
                "    (10, \"Small\"),\n",
                "    (50, \"Medium\"),\n",
                "    (100, \"Large\")\n",
                "]\n",
                "\n",
                "results = []\n",
                "\n",
                "for size_kb, label in test_sizes:\n",
                "    text = create_large_text(size_kb=size_kb)\n",
                "    estimated_tokens = len(text) // 4\n",
                "    \n",
                "    messages = [\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": [\n",
                "                {\n",
                "                    \"text\": f\"Provide a one-sentence summary of this text:\\n\\n{text}\"\n",
                "                }\n",
                "            ]\n",
                "        }\n",
                "    ]\n",
                "    \n",
                "    try:\n",
                "        print(f\"\\nüîÑ Testing {label} text (~{estimated_tokens:,} tokens)...\")\n",
                "        response = manager.converse(\n",
                "            messages=messages,\n",
                "            enable_extended_context=True,\n",
                "            inference_config={\"maxTokens\": 100}\n",
                "        )\n",
                "        \n",
                "        if response.success:\n",
                "            results.append({\n",
                "                \"label\": label,\n",
                "                \"estimated\": estimated_tokens,\n",
                "                \"actual_input\": response.get_input_tokens(),\n",
                "                \"output\": response.get_output_tokens(),\n",
                "                \"total\": response.get_total_tokens()\n",
                "            })\n",
                "            print(f\"   ‚úÖ Input: {response.get_input_tokens():,} tokens\")\n",
                "            print(f\"   ‚úÖ Output: {response.get_output_tokens():,} tokens\")\n",
                "    \n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ùå Error: {e}\")\n",
                "        results.append({\n",
                "            \"label\": label,\n",
                "            \"estimated\": estimated_tokens,\n",
                "            \"error\": str(e)\n",
                "        })\n",
                "\n",
                "# Display summary\n",
                "print(\"\\nüìä Token Usage Summary\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"{'Size':<10} {'Estimated':<12} {'Actual Input':<15} {'Output':<10} {'Total':<10}\")\n",
                "print(\"-\" * 60)\n",
                "for result in results:\n",
                "    if 'error' not in result:\n",
                "        print(f\"{result['label']:<10} {result['estimated']:<12,} \"\n",
                "              f\"{result['actual_input']:<15,} {result['output']:<10,} \"\n",
                "              f\"{result['total']:<10,}\")\n",
                "    else:\n",
                "        print(f\"{result['label']:<10} {result['estimated']:<12,} ERROR\")\n",
                "\n",
                "print(\"\\nüí° Note: Extended context allows up to 1M tokens, but costs scale with usage.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 4: Handling Parameter Incompatibility üîÑ\n",
                "\n",
                "The system automatically handles cases where extended context isn't supported,\n",
                "falling back gracefully to standard context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîÑ Example 4: Handling Parameter Incompatibility\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Initialize manager with multiple models (some may not support extended context)\n",
                "multi_model_manager = LLMManager(\n",
                "    models=[\"Claude Sonnet 4\", \"Claude 3 Haiku\"],\n",
                "    regions=[\"us-east-1\", \"us-west-2\"],\n",
                "    auth_config=AuthConfig(\n",
                "        auth_type=AuthenticationType.PROFILE,\n",
                "        profile_name=\"default\"\n",
                "    ),\n",
                "    retry_config=RetryConfig(\n",
                "        max_retries=3,\n",
                "        retry_strategy=RetryStrategy.MODEL_FIRST\n",
                "    )\n",
                ")\n",
                "\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\n",
                "                \"text\": \"What are the benefits of extended context windows in AI models?\"\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "try:\n",
                "    print(\"\\nüîÑ Sending request with enable_extended_context=True...\")\n",
                "    print(\"   The system will try Claude Sonnet 4 first, then fall back if needed.\")\n",
                "    \n",
                "    response = multi_model_manager.converse(\n",
                "        messages=messages,\n",
                "        enable_extended_context=True,\n",
                "        inference_config={\"maxTokens\": 400}\n",
                "    )\n",
                "    \n",
                "    display_response(response, \"üîÑ Fallback Handling Response\")\n",
                "    \n",
                "    # Check if parameters were removed\n",
                "    if response.had_parameters_removed():\n",
                "        print(\"\\n‚ö†Ô∏è  Parameter Removal Detected:\")\n",
                "        print(f\"   Removed parameters: {response.parameters_removed}\")\n",
                "        print(f\"   Model used: {response.model_used}\")\n",
                "        print(f\"   Region used: {response.region_used}\")\n",
                "        print(\"\\nüí° The system automatically retried without extended context.\")\n",
                "    else:\n",
                "        print(\"\\n‚úÖ Extended context was supported by the selected model/region!\")\n",
                "        print(f\"   Model: {response.model_used}\")\n",
                "        print(f\"   Region: {response.region_used}\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")\n",
                "    print(f\"   Type: {type(e).__name__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 5: ModelSpecificConfig for Reusable Configuration üéØ\n",
                "\n",
                "Use `ModelSpecificConfig` to create reusable parameter configurations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üéØ Example 5: ModelSpecificConfig\")\n",
                "print(\"=\" * 35)\n",
                "\n",
                "# Create reusable configuration\n",
                "config = ModelSpecificConfig(\n",
                "    enable_extended_context=True,\n",
                "    custom_fields={\n",
                "        # You can add other model-specific parameters here\n",
                "    }\n",
                ")\n",
                "\n",
                "print(\"üìã Created ModelSpecificConfig:\")\n",
                "print(f\"   enable_extended_context: {config.enable_extended_context}\")\n",
                "print(f\"   custom_fields: {config.custom_fields}\")\n",
                "\n",
                "# Initialize manager with default config\n",
                "config_manager = LLMManager(\n",
                "    models=[\"Claude Sonnet 4\"],\n",
                "    regions=[\"us-east-1\"],\n",
                "    auth_config=AuthConfig(\n",
                "        auth_type=AuthenticationType.PROFILE,\n",
                "        profile_name=\"default\"\n",
                "    ),\n",
                "    model_specific_config=config  # Set as default\n",
                ")\n",
                "\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\n",
                "                \"text\": \"Explain how configuration objects improve code maintainability.\"\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "try:\n",
                "    print(\"\\nüîÑ Sending request (config applied automatically)...\")\n",
                "    # All requests use the config by default\n",
                "    response = config_manager.converse(\n",
                "        messages=messages,\n",
                "        inference_config={\"maxTokens\": 300}\n",
                "    )\n",
                "    \n",
                "    display_response(response, \"üéØ Config-Based Response\")\n",
                "    \n",
                "    print(\"\\n‚úÖ ModelSpecificConfig was applied automatically!\")\n",
                "    print(\"   This approach is great for consistent parameter usage across requests.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")\n",
                "    print(f\"   Type: {type(e).__name__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 6: Querying Parameter Compatibility üîç\n",
                "\n",
                "The system tracks which model/region combinations support which parameters.\n",
                "You can query this information for optimization and debugging."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîç Example 6: Querying Parameter Compatibility\")\n",
                "print(\"=\" * 45)\n",
                "\n",
                "# Get compatibility tracker\n",
                "tracker = ParameterCompatibilityTracker.get_instance()\n",
                "\n",
                "# Get statistics\n",
                "stats = tracker.get_statistics()\n",
                "\n",
                "print(\"\\nüìä Parameter Compatibility Statistics:\")\n",
                "print(f\"   Total tracked combinations: {stats.get('total_combinations', 0)}\")\n",
                "print(f\"   Compatible combinations: {stats.get('compatible_count', 0)}\")\n",
                "print(f\"   Incompatible combinations: {stats.get('incompatible_count', 0)}\")\n",
                "\n",
                "# Check specific combination\n",
                "test_params = {\"anthropic_beta\": [\"context-1m-2025-08-07\"]}\n",
                "is_incompatible = tracker.is_known_incompatible(\n",
                "    model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
                "    region=\"us-east-1\",\n",
                "    parameters=test_params\n",
                ")\n",
                "\n",
                "print(f\"\\nüîç Checking specific combination:\")\n",
                "print(f\"   Model: Claude Sonnet 4\")\n",
                "print(f\"   Region: us-east-1\")\n",
                "print(f\"   Parameters: {test_params}\")\n",
                "print(f\"   Known incompatible: {'‚ùå Yes' if is_incompatible else '‚úÖ No'}\")\n",
                "\n",
                "print(\"\\nüí° The tracker learns from each request to optimize future attempts.\")\n",
                "print(\"   This helps skip known incompatible combinations during retry.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary and Best Practices üìù\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "1. **Simple Flag**: Use `enable_extended_context=True` for easy activation\n",
                "2. **Manual Control**: Use `additionalModelRequestFields` for fine-grained control\n",
                "3. **Automatic Fallback**: System handles incompatibility gracefully\n",
                "4. **Token Monitoring**: Always check token usage for cost management\n",
                "5. **Configuration Objects**: Use `ModelSpecificConfig` for reusable settings\n",
                "\n",
                "### Best Practices\n",
                "\n",
                "- ‚úÖ **Monitor token usage** to understand costs\n",
                "- ‚úÖ **Use multi-model configuration** for automatic fallback\n",
                "- ‚úÖ **Check response warnings** to detect parameter removal\n",
                "- ‚úÖ **Test with smaller contexts** before scaling to 1M tokens\n",
                "- ‚úÖ **Verify region support** for beta features\n",
                "\n",
                "### Common Pitfalls\n",
                "\n",
                "- ‚ùå Assuming all regions support extended context\n",
                "- ‚ùå Not monitoring token consumption\n",
                "- ‚ùå Ignoring parameter removal warnings\n",
                "- ‚ùå Using extended context when standard context is sufficient\n",
                "\n",
                "### Additional Resources\n",
                "\n",
                "- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
                "- [Claude Model Documentation](https://docs.anthropic.com/)\n",
                "- [LLMManager Documentation](../docs/forLLMConsumption.md)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}