{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Bedrock LLMManager Example Notebook\n",
    "\n",
    "This notebook demonstrates how to use the LLMManager class to interact with AWS Bedrock's Converse API with built-in resilience and failover capabilities. The LLMManager provides:\n",
    "\n",
    "1. **Multi-region support** - Try models across different AWS regions\n",
    "2. **Model fallback** - Automatically switch to alternative models if the primary choice fails\n",
    "3. **Cross-Region Inference (CRIS) optimization** - Automatically use CRIS when available\n",
    "4. **Error handling** - Gracefully handle API errors with appropriate fallbacks\n",
    "5. **Flexible authentication** - Support for AWS CLI profiles, access keys, or IAM roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the LLMManager class and other required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Import from our package\n",
    "from src import LLMManager, Fields, Roles, PerformanceConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure AWS Profile\n",
    "\n",
    "Set your AWS CLI profile name below. This profile should have permissions to access AWS Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your AWS CLI profile name\n",
    "aws_profile = \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Initialization of LLMManager\n",
    "\n",
    "Create an instance of the LLMManager with the AWS profile and default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLMManager with AWS profile\n",
    "llm_manager = LLMManager(\n",
    "    profile_name=aws_profile,\n",
    "    regions=[\"us-east-1\", \"us-west-2\", \"eu-west-1\"],  # Try multiple regions in order of preference\n",
    "    model_ids=[\"anthropic.claude-3-sonnet-20240229-v1:0\", \"anthropic.claude-3-haiku-20240307-v1:0\"],  # List models in order of preference\n",
    "    log_level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a Simple Text Prompt\n",
    "\n",
    "Let's send a simple prompt to the LLM and see the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"Write a paragraph on the planets of the solar system.\"\n",
    "\n",
    "# Create a text message using the helper method\n",
    "messages = [llm_manager.create_text_message(prompt)]\n",
    "\n",
    "# Send the prompt to the model\n",
    "try:\n",
    "    response = llm_manager.converse(messages=messages)\n",
    "    \n",
    "    # Print the response text\n",
    "    print(\"Model response text:\")\n",
    "    print(response.get_content_text())\n",
    "    \n",
    "    print(\"\\nMetadata:\")\n",
    "    print(f\"Model used: {response.model_id}\")\n",
    "    print(f\"Region used: {response.region}\")\n",
    "    print(f\"Used CRIS: {response.is_cris}\")\n",
    "    print(f\"Execution time: {response.execution_time:.3f} seconds\")\n",
    "    print(f\"Input tokens: {response.get_input_tokens()}\")\n",
    "    print(f\"Output tokens: {response.get_output_tokens()}\")\n",
    "    print(f\"Total tokens: {response.get_total_tokens()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using System Prompts\n",
    "\n",
    "System prompts can be used to provide context or instructions to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a system message using the helper method\n",
    "system = [llm_manager.create_system_message(\n",
    "    \"You are a helpful AI assistant specializing in astronomy. \"\n",
    "    \"Provide concise and accurate information.\"\n",
    ")]\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"What are the most interesting facts about Jupiter?\"\n",
    "messages = [llm_manager.create_text_message(prompt)]\n",
    "\n",
    "# Send the prompt with the system message\n",
    "try:\n",
    "    response = llm_manager.converse(\n",
    "        messages=messages,\n",
    "        system=system\n",
    "    )\n",
    "    \n",
    "    print(\"Response with system prompt:\")\n",
    "    print(response.get_content_text())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Different Inference Parameters\n",
    "\n",
    "The LLMManager allows you to customize inference parameters such as temperature, max tokens, top_p, and stop sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"Write a creative short story about space exploration.\"\n",
    "messages = [llm_manager.create_text_message(prompt)]\n",
    "\n",
    "# Define inference config with different parameters\n",
    "inference_config = {\n",
    "    Fields.TEMPERATURE: 0.9,  # Higher temperature for more creative output\n",
    "    Fields.MAX_TOKENS: 300,   # Limit the response length\n",
    "    Fields.TOP_P: 0.95,       # Sample from a wider range of tokens\n",
    "    Fields.STOP_SEQUENCES: [\"The End\"]  # Stop generation at \"The End\"\n",
    "}\n",
    "\n",
    "# Send the prompt with custom inference parameters\n",
    "try:\n",
    "    response = llm_manager.converse(\n",
    "        messages=messages,\n",
    "        inference_config=inference_config\n",
    "    )\n",
    "    \n",
    "    print(\"Response with custom inference parameters:\")\n",
    "    print(response.get_content_text())\n",
    "    print(f\"\\nStop reason: {response.get_stop_reason()}\")\n",
    "    print(f\"Output tokens: {response.get_output_tokens()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Different Performance Settings\n",
    "\n",
    "You can optimize for latency using the performance_config parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "messages = [llm_manager.create_text_message(prompt)]\n",
    "\n",
    "# Define performance config for latency optimization\n",
    "performance_config = {\n",
    "    Fields.LATENCY: PerformanceConfig.OPTIMIZED\n",
    "}\n",
    "\n",
    "# Send the prompt with optimized latency\n",
    "try:\n",
    "    response = llm_manager.converse(\n",
    "        messages=messages,\n",
    "        performance_config=performance_config\n",
    "    )\n",
    "    \n",
    "    print(\"Response with optimized latency:\")\n",
    "    print(response.get_content_text())\n",
    "    print(f\"\\nLatency: {response.get_latency_ms()} ms\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying Different Models and Regions\n",
    "\n",
    "You can override the default models and regions when calling the converse method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"What are three benefits of quantum computing?\"\n",
    "messages = [llm_manager.create_text_message(prompt)]\n",
    "\n",
    "# Specify different models and regions\n",
    "alternative_models = [\"amazon.nova-pro-v1:0\", \"anthropic.claude-3-haiku-20240307-v1:0\"]\n",
    "alternative_regions = [\"us-west-2\", \"us-east-1\"]\n",
    "\n",
    "try:\n",
    "    response = llm_manager.converse(\n",
    "        messages=messages,\n",
    "        model_ids=alternative_models,\n",
    "        regions=alternative_regions\n",
    "    )\n",
    "    \n",
    "    print(\"Response with alternative models and regions:\")\n",
    "    print(response.get_content_text())\n",
    "    print(f\"\\nModel used: {response.model_id}\")\n",
    "    print(f\"Region used: {response.region}\")\n",
    "    print(f\"Used CRIS: {response.is_cris}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Cross-Region Inference Profiles\n",
    "\n",
    "The LLMManager automatically uses Cross-Region Inference Service (CRIS) profiles when available. Let's explicitly create an instance that focuses on CRIS-enabled regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LLMManager instance focused on regions that support CRIS\n",
    "cris_manager = LLMManager(\n",
    "    profile_name=aws_profile,\n",
    "    # These regions are common source regions for CRIS profiles\n",
    "    regions=[\"us-east-1\", \"us-east-2\", \"us-west-2\", \"eu-central-1\", \"eu-west-1\", \"ap-northeast-1\"],\n",
    "    # Models that are widely available through CRIS\n",
    "    model_ids=[\"anthropic.claude-3-sonnet-20240229-v1:0\", \"anthropic.claude-3-haiku-20240307-v1:0\"]\n",
    ")\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Summarize the key benefits of AWS Cross-Region Inference Service.\"\n",
    "messages = [cris_manager.create_text_message(prompt)]\n",
    "\n",
    "try:\n",
    "    response = cris_manager.converse(messages=messages)\n",
    "    \n",
    "    print(\"Response from CRIS-focused manager:\")\n",
    "    print(response.get_content_text())\n",
    "    print(f\"\\nModel used: {response.model_id}\")\n",
    "    print(f\"Region used: {response.region}\")\n",
    "    print(f\"Used CRIS: {response.is_cris}\")\n",
    "    if response.is_cris:\n",
    "        print(f\"The model is being accessed through CRIS!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling and Fallback Mechanisms\n",
    "\n",
    "The LLMManager automatically handles errors and falls back to alternative models and regions. Let's see how it behaves when we provide an invalid model ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"What is AWS Bedrock?\"\n",
    "messages = [llm_manager.create_text_message(prompt)]\n",
    "\n",
    "# Specify a non-existent model ID first, followed by a valid one\n",
    "test_models = [\"invalid.model-v1:0\", \"anthropic.claude-3-haiku-20240307-v1:0\"]\n",
    "\n",
    "try:\n",
    "    response = llm_manager.converse(\n",
    "        messages=messages,\n",
    "        model_ids=test_models\n",
    "    )\n",
    "    \n",
    "    print(\"Response after fallback:\")\n",
    "    print(response.get_content_text())\n",
    "    print(f\"\\nModel used: {response.model_id}\")\n",
    "    print(f\"Region used: {response.region}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Additional Features (Tool Config, Guardrails, etc.)\n",
    "\n",
    "The LLMManager supports additional features such as tool configuration, guardrails, and request metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a request with metadata\n",
    "prompt = \"What's the capital of Germany?\"\n",
    "messages = [llm_manager.create_text_message(prompt)]\n",
    "\n",
    "# Define request metadata\n",
    "request_metadata = {\n",
    "    \"requestId\": f\"test-{int(time.time())}\",\n",
    "    \"context\": \"geography-query\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = llm_manager.converse(\n",
    "        messages=messages,\n",
    "        request_metadata=request_metadata\n",
    "    )\n",
    "    \n",
    "    print(\"Response with request metadata:\")\n",
    "    print(response.get_content_text())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Images (Multimodal Models)\n",
    "\n",
    "If you have image bytes, you can use the create_image_message helper to create messages with images for multimodal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder to demonstrate the image message creation\n",
    "# In a real implementation, you would load an actual image file\n",
    "'''\n",
    "# Example: Load an image file\n",
    "with open(\"path/to/image.jpg\", \"rb\") as f:\n",
    "    image_bytes = f.read()\n",
    "\n",
    "# Create an image message\n",
    "image_message = llm_manager.create_image_message(\n",
    "    image_bytes=image_bytes,\n",
    "    text=\"What's in this image?\",\n",
    "    image_format=\"jpeg\"\n",
    ")\n",
    "\n",
    "# Send the message to a multimodal model\n",
    "multimodal_models = [\"anthropic.claude-3-sonnet-20240229-v1:0\"]\n",
    "response = llm_manager.converse(\n",
    "    messages=[image_message],\n",
    "    model_ids=multimodal_models\n",
    ")\n",
    "\n",
    "print(response.get_content_text())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the capabilities of the LLMManager class for interacting with AWS Bedrock models. The key features include:\n",
    "\n",
    "1. Automatic model and region fallbacks for improved reliability\n",
    "2. Cross-Region Inference Service (CRIS) optimization\n",
    "3. Support for various inference parameters and configurations\n",
    "4. Helper methods for creating messages with different content types\n",
    "5. Comprehensive error handling and response metadata\n",
    "\n",
    "Use the LLMManager to build robust applications that can leverage AWS Bedrock's powerful LLM capabilities with built-in resilience and optimizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
