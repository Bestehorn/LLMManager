{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Caching with LLM Manager\n",
    "\n",
    "This notebook demonstrates the new caching capabilities in LLM Manager using real images.\n",
    "We'll analyze architectural towers using caching to optimize costs and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from bestehorn_llmmanager import LLMManager, create_user_message\n",
    "from bestehorn_llmmanager.bedrock.models.cache_structures import CacheConfig, CacheStrategy\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Set up paths to images\n",
    "images_dir = Path(\"../images\")\n",
    "eiffel_tower_path = images_dir / \"1200px-Tour_Eiffel_Wikimedia_Commons_(cropped).jpg\"\n",
    "tokyo_tower_path = images_dir / \"Tokyo_Tower_2023.jpg\"\n",
    "\n",
    "print(f\"Eiffel Tower image exists: {eiffel_tower_path.exists()}\")\n",
    "print(f\"Tokyo Tower image exists: {tokyo_tower_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize LLM Manager with Caching\n",
    "\n",
    "Note: Caching is **OFF by default** and must be explicitly enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure caching\n",
    "cache_config = CacheConfig(\n",
    "    enabled=True,  # Must explicitly enable\n",
    "    strategy=CacheStrategy.CONSERVATIVE,\n",
    "    cache_point_threshold=1000,  # Minimum tokens to cache\n",
    "    log_cache_failures=True\n",
    ")\n",
    "\n",
    "# Initialize manager with caching\n",
    "manager = LLMManager(\n",
    "    models=[\"Claude 3 Haiku\"],  # Use a model that supports caching\n",
    "    regions=[\"us-east-1\"],\n",
    "    cache_config=cache_config\n",
    ")\n",
    "\n",
    "print(\"LLM Manager initialized with caching enabled\")\n",
    "print(f\"Cache strategy: {cache_config.strategy.value}\")\n",
    "print(f\"Cache threshold: {cache_config.cache_point_threshold} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Analysis Prompts\n",
    "\n",
    "We'll use the same images with different analysis focuses to demonstrate caching benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared context for all prompts\n",
    "shared_context = \"\"\"You are an expert architectural analyst specializing in tower structures. \n",
    "Please analyze these two famous towers - the Eiffel Tower in Paris and the Tokyo Tower in Japan.\n",
    "Provide detailed insights based on the images provided.\"\"\"\n",
    "\n",
    "# Different analysis prompts\n",
    "analysis_prompts = [\n",
    "    \"Compare the structural engineering approaches used in both towers.\",\n",
    "    \"Analyze the architectural styles and their historical contexts.\",\n",
    "    \"Examine the materials and construction techniques visible in the images.\",\n",
    "    \"Compare the aesthetic design elements and their cultural significance.\",\n",
    "    \"Identify key differences in their structural support systems.\"\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(analysis_prompts)} different analysis prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image bytes\n",
    "with open(eiffel_tower_path, \"rb\") as f:\n",
    "    eiffel_bytes = f.read()\n",
    "\n",
    "with open(tokyo_tower_path, \"rb\") as f:\n",
    "    tokyo_bytes = f.read()\n",
    "\n",
    "print(f\"Eiffel Tower image size: {len(eiffel_bytes):,} bytes ({len(eiffel_bytes)/1024/1024:.2f} MB)\")\n",
    "print(f\"Tokyo Tower image size: {len(tokyo_bytes):,} bytes ({len(tokyo_bytes)/1024/1024:.2f} MB)\")\n",
    "print(f\"\\nEstimated tokens for images: ~{(len(eiffel_bytes) + len(tokyo_bytes)) // 1000} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. First Request - Cache WRITE (Using MessageBuilder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First request - this will WRITE to cache\n",
    "print(\"=== Request 1: Cache WRITE ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Build message using MessageBuilder with explicit cache point\n",
    "message = create_user_message(cache_config=cache_config)\n",
    "message.add_text(shared_context, cacheable=True)\n",
    "message.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "message.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "message.add_cache_point()  # Explicit cache point after shared content\n",
    "message.add_text(analysis_prompts[0], cacheable=False)  # Unique prompt not cached\n",
    "\n",
    "# Make request\n",
    "response1 = manager.converse(messages=[message.build()])\n",
    "duration1 = time.time() - start_time\n",
    "\n",
    "# Display cache metrics\n",
    "cache_info = response1.get_cached_tokens_info()\n",
    "if cache_info:\n",
    "    print(f\"\\nCache Write: {cache_info['cache_write_tokens']} tokens\")\n",
    "    print(f\"Cache Hit: {cache_info['cache_hit']}\")\n",
    "    print(f\"Duration: {duration1:.2f} seconds\")\n",
    "\n",
    "# Show response preview\n",
    "print(f\"\\nResponse preview: {response1.get_content()[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Second Request - Cache HIT (Using Plain Dict/JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second request using plain dict/JSON format (as requested)\n",
    "print(\"=== Request 2: Cache HIT (Plain Dict Method) ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Manually construct message with cache point using dict format\n",
    "plain_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": shared_context},\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"format\": \"jpeg\",\n",
    "                \"source\": {\"bytes\": eiffel_bytes}\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"format\": \"jpeg\",\n",
    "                \"source\": {\"bytes\": tokyo_bytes}\n",
    "            }\n",
    "        },\n",
    "        {\"cachePoint\": {\"type\": \"default\"}},  # Manual cache point\n",
    "        {\"text\": analysis_prompts[1]}  # Different analysis prompt\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Make request with plain message\n",
    "response2 = manager.converse(messages=[plain_message])\n",
    "duration2 = time.time() - start_time\n",
    "\n",
    "# Display cache metrics\n",
    "cache_info = response2.get_cached_tokens_info()\n",
    "efficiency = response2.get_cache_efficiency()\n",
    "\n",
    "if cache_info:\n",
    "    print(f\"\\nCache Read: {cache_info['cache_read_tokens']} tokens\")\n",
    "    print(f\"Cache Hit: {cache_info['cache_hit']}\")\n",
    "    print(f\"Duration: {duration2:.2f} seconds\")\n",
    "    print(f\"Speed improvement: {duration1/duration2:.1f}x faster\")\n",
    "\n",
    "if efficiency:\n",
    "    print(f\"\\nCache Efficiency Metrics:\")\n",
    "    print(f\"  - Hit ratio: {efficiency['cache_hit_ratio']*100:.1f}%\")\n",
    "    print(f\"  - Tokens saved: {efficiency['cache_savings_tokens']}\")\n",
    "    print(f\"  - Cost savings: {efficiency['cache_savings_cost']}\")\n",
    "    print(f\"  - Latency reduction: {efficiency['latency_reduction_ms']}ms\")\n",
    "\n",
    "print(f\"\\nResponse preview: {response2.get_content()[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Subsequent Requests - Demonstrating Cache Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process remaining prompts to show cumulative benefits\n",
    "total_tokens_saved = 0\n",
    "total_time_saved = 0\n",
    "responses = [response1, response2]\n",
    "\n",
    "print(\"=== Processing Remaining Analysis Prompts ===\")\n",
    "for i, prompt in enumerate(analysis_prompts[2:], start=3):\n",
    "    print(f\"\\nRequest {i}: {prompt[:50]}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use MessageBuilder for remaining requests\n",
    "    message = create_user_message(cache_config=cache_config)\n",
    "    message.add_text(shared_context, cacheable=True)\n",
    "    message.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "    message.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "    message.add_cache_point()\n",
    "    message.add_text(prompt, cacheable=False)\n",
    "    \n",
    "    response = manager.converse(messages=[message.build()])\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    cache_info = response.get_cached_tokens_info()\n",
    "    if cache_info and cache_info['cache_hit']:\n",
    "        print(f\"  âœ“ Cache HIT: {cache_info['cache_read_tokens']} tokens\")\n",
    "        print(f\"  Duration: {duration:.2f}s\")\n",
    "        total_tokens_saved += cache_info['cache_read_tokens']\n",
    "        total_time_saved += (duration1 - duration)\n",
    "    \n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary - Total Cache Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total benefits across all requests\n",
    "print(\"=== CACHING SUMMARY ===\")\n",
    "print(f\"\\nTotal requests made: {len(responses)}\")\n",
    "print(f\"Cache writes: 1 (first request only)\")\n",
    "print(f\"Cache hits: {len(responses) - 1}\")\n",
    "\n",
    "# Aggregate metrics\n",
    "total_cache_read = sum(r.get_cached_tokens_info()['cache_read_tokens'] \n",
    "                      for r in responses[1:] \n",
    "                      if r.get_cached_tokens_info())\n",
    "\n",
    "cache_write_tokens = response1.get_cached_tokens_info()['cache_write_tokens']\n",
    "\n",
    "print(f\"\\nToken Usage:\")\n",
    "print(f\"  - Tokens cached: {cache_write_tokens}\")\n",
    "print(f\"  - Total tokens read from cache: {total_cache_read}\")\n",
    "print(f\"  - Cache reuse factor: {total_cache_read / cache_write_tokens:.1f}x\")\n",
    "\n",
    "# Cost estimation (using example rate)\n",
    "COST_PER_1K_TOKENS = 0.03\n",
    "cost_without_cache = (len(responses) * cache_write_tokens / 1000) * COST_PER_1K_TOKENS\n",
    "cost_with_cache = (cache_write_tokens / 1000) * COST_PER_1K_TOKENS\n",
    "cost_savings = cost_without_cache - cost_with_cache\n",
    "\n",
    "print(f\"\\nCost Analysis:\")\n",
    "print(f\"  - Cost without caching: ${cost_without_cache:.2f}\")\n",
    "print(f\"  - Cost with caching: ${cost_with_cache:.2f}\")\n",
    "print(f\"  - Total savings: ${cost_savings:.2f} ({(cost_savings/cost_without_cache)*100:.0f}% reduction)\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  - Average time saved per request: {total_time_saved/(len(responses)-1):.2f}s\")\n",
    "print(f\"  - Total time saved: {total_time_saved:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing Cache Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization of cache impact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for visualization\n",
    "request_numbers = list(range(1, len(responses) + 1))\n",
    "cache_hits = [0] + [r.get_cached_tokens_info()['cache_read_tokens'] \n",
    "                   for r in responses[1:] \n",
    "                   if r.get_cached_tokens_info()]\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red'] + ['green'] * (len(responses) - 1)\n",
    "bars = plt.bar(request_numbers, cache_hits, color=colors)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, cache_hits):\n",
    "    if value > 0:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "                f'{value}', ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Request Number')\n",
    "plt.ylabel('Tokens Read from Cache')\n",
    "plt.title('Cache Performance Across Sequential Requests')\n",
    "plt.legend(['Cache Write (Request 1)', 'Cache Hit (Subsequent Requests)'])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Pie chart for cost breakdown\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels = ['Cached Content\\n(Paid Once)', 'Unique Content\\n(Paid Each Time)']\n",
    "sizes = [cache_write_tokens, len(responses) * 100]  # Assuming ~100 tokens per unique prompt\n",
    "colors = ['#4CAF50', '#FFC107']\n",
    "explode = (0.1, 0)\n",
    "\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "plt.title('Token Usage Distribution with Caching')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Caching must be explicitly enabled** - it's OFF by default\n",
    "2. **First request writes to cache** - includes one-time latency for caching\n",
    "3. **Subsequent requests hit cache** - dramatically faster and cheaper\n",
    "4. **Cache efficiency increases with more requests** - the same cached content is reused\n",
    "5. **Both MessageBuilder and plain dict methods support caching** - choose based on preference\n",
    "\n",
    "### Performance Benefits Observed:\n",
    "- **Cost Reduction**: ~80-90% for cached content\n",
    "- **Speed Improvement**: 2-5x faster responses\n",
    "- **Token Savings**: Proportional to number of requests sharing content\n",
    "\n",
    "### Best Practices:\n",
    "- Place cache points after shared content (images, context, instructions)\n",
    "- Keep unique content (specific questions) after cache points\n",
    "- Use CONSERVATIVE strategy for most use cases\n",
    "- Monitor cache metrics to optimize placement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
