{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Caching with LLM Manager\n",
    "\n",
    "This notebook demonstrates the new caching capabilities in LLM Manager using real images.\n",
    "We'll analyze architectural towers using caching to optimize costs and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from bestehorn_llmmanager import LLMManager, create_user_message\n",
    "from bestehorn_llmmanager.bedrock.models.cache_structures import CacheConfig, CacheStrategy\n",
    "from bestehorn_llmmanager.bedrock.models.llm_manager_structures import AuthConfig, RetryConfig, AuthenticationType, RetryStrategy\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Set up paths to images\n",
    "images_dir = Path(\"../images\")\n",
    "eiffel_tower_path = images_dir / \"1200px-Tour_Eiffel_Wikimedia_Commons_(cropped).jpg\"\n",
    "tokyo_tower_path = images_dir / \"Tokyo_Tower_2023.jpg\"\n",
    "\n",
    "print(f\"Eiffel Tower image exists: {eiffel_tower_path.exists()}\")\n",
    "print(f\"Tokyo Tower image exists: {tokyo_tower_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize LLM Manager with Caching\n",
    "\n",
    "Note: Caching is **OFF by default** and must be explicitly enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure caching\n",
    "cache_config = CacheConfig(\n",
    "    enabled=True,  # Must explicitly enable\n",
    "    strategy=CacheStrategy.CONSERVATIVE,\n",
    "    cache_point_threshold=1000,  # Minimum tokens to cache\n",
    "    log_cache_failures=True\n",
    ")\n",
    "\n",
    "auth_config = AuthConfig(auth_type=AuthenticationType.PROFILE, profile_name=\"default\")\n",
    "\n",
    "# Initialize manager with caching\n",
    "manager = LLMManager(\n",
    "    models=[\"Claude 3.7 Sonnet\"],  # Use a model that supports caching\n",
    "    regions=[\"us-east-1\", \"us-west-2\"],\n",
    "    cache_config=cache_config,\n",
    "    auth_config=auth_config\n",
    ")\n",
    "\n",
    "print(\"LLM Manager initialized with caching enabled\")\n",
    "print(f\"Cache strategy: {cache_config.strategy.value}\")\n",
    "print(f\"Cache threshold: {cache_config.cache_point_threshold} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Analysis Prompts\n",
    "\n",
    "We'll use the same images with different analysis focuses to demonstrate caching benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared context for all prompts\n",
    "shared_context = \"\"\"You are an expert architectural analyst specializing in tower structures. \n",
    "Please analyze these two famous towers - the Eiffel Tower in Paris and the Tokyo Tower in Japan.\n",
    "Provide detailed insights based on the images provided.\"\"\"\n",
    "\n",
    "# Different analysis prompts\n",
    "analysis_prompts = [\n",
    "    \"Compare the structural engineering approaches used in both towers.\",\n",
    "    \"Analyze the architectural styles and their historical contexts.\",\n",
    "    \"Examine the materials and construction techniques visible in the images.\",\n",
    "    \"Compare the aesthetic design elements and their cultural significance.\",\n",
    "    \"Identify key differences in their structural support systems.\"\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(analysis_prompts)} different analysis prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image bytes\n",
    "with open(eiffel_tower_path, \"rb\") as f:\n",
    "    eiffel_bytes = f.read()\n",
    "\n",
    "with open(tokyo_tower_path, \"rb\") as f:\n",
    "    tokyo_bytes = f.read()\n",
    "\n",
    "print(f\"Eiffel Tower image size: {len(eiffel_bytes):,} bytes ({len(eiffel_bytes)/1024/1024:.2f} MB)\")\n",
    "print(f\"Tokyo Tower image size: {len(tokyo_bytes):,} bytes ({len(tokyo_bytes)/1024/1024:.2f} MB)\")\n",
    "print(f\"\\nEstimated tokens for images: ~{(len(eiffel_bytes) + len(tokyo_bytes)) // 1000} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. First Request - Cache WRITE (Using MessageBuilder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First request - this will WRITE to cache\n",
    "print(\"=== Request 1: Cache WRITE ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Build message using MessageBuilder with explicit cache point\n",
    "message = create_user_message(cache_config=cache_config)\n",
    "message.add_text(shared_context, cacheable=True)\n",
    "message.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "message.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "message.add_cache_point()  # Explicit cache point after shared content\n",
    "message.add_text(analysis_prompts[0], cacheable=False)  # Unique prompt not cached\n",
    "\n",
    "# Make request\n",
    "response1 = manager.converse(messages=[message.build()])\n",
    "duration1 = time.time() - start_time\n",
    "\n",
    "# Display cache metrics and debugging info\n",
    "cache_info = response1.get_cached_tokens_info()\n",
    "usage_info = response1.get_usage()\n",
    "\n",
    "print(f\"\\nDebug Information:\")\n",
    "print(f\"  - Success: {response1.success}\")\n",
    "print(f\"  - Model used: {response1.model_used}\")\n",
    "print(f\"  - Region used: {response1.region_used}\")\n",
    "print(f\"  - Cache info: {cache_info}\")\n",
    "print(f\"  - Usage info: {usage_info}\")\n",
    "print(f\"  - Duration: {duration1:.2f} seconds\")\n",
    "\n",
    "if cache_info:\n",
    "    print(f\"\\nCache Metrics:\")\n",
    "    print(f\"  - Cache Write: {cache_info['cache_write_tokens']} tokens\")\n",
    "    print(f\"  - Cache Hit: {cache_info['cache_hit']}\")\n",
    "    print(f\"  - Cache Read: {cache_info['cache_read_tokens']} tokens\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Cache info is None - caching may not be working or supported\")\n",
    "\n",
    "# Show response preview\n",
    "print(f\"\\nResponse preview: {response1.get_content()[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Test if field mapping fix worked\n",
    "print(\"=== FIELD MAPPING FIX TEST ===\")\n",
    "if response1.response_data:\n",
    "    usage_raw = response1.response_data.get('usage', {})\n",
    "    print(f\"Raw AWS usage data: {usage_raw}\")\n",
    "    \n",
    "    # Test our fixed mapping\n",
    "    cache_read_aws = usage_raw.get('cacheReadInputTokens', 0)\n",
    "    cache_write_aws = usage_raw.get('cacheWriteInputTokens', 0)\n",
    "    print(f\"Direct AWS extraction: read={cache_read_aws}, write={cache_write_aws}\")\n",
    "    \n",
    "    # Test our BedrockResponse method\n",
    "    usage_processed = response1.get_usage()\n",
    "    print(f\"BedrockResponse.get_usage(): {usage_processed}\")\n",
    "    \n",
    "    cache_info_new = response1.get_cached_tokens_info()\n",
    "    print(f\"BedrockResponse.get_cached_tokens_info(): {cache_info_new}\")\n",
    "    \n",
    "    # Verify the fix worked\n",
    "    if cache_info_new and cache_info_new['cache_write_tokens'] == cache_write_aws:\n",
    "        print(\"‚úÖ Field mapping fix SUCCESSFUL!\")\n",
    "    else:\n",
    "        print(\"‚ùå Field mapping still has issues\")\n",
    "        \n",
    "else:\n",
    "    print(\"No response data available\")\n",
    "    \n",
    "# Check if the message we sent had cache points\n",
    "print(f\"\\n=== SENT MESSAGE DEBUG ===\")\n",
    "built_message = message.build()\n",
    "print(f\"Message content blocks: {len(built_message['content'])}\")\n",
    "for i, block in enumerate(built_message['content']):\n",
    "    block_type = list(block.keys())[0]\n",
    "    print(f\"Block {i}: {block_type}\")\n",
    "    if block_type == 'cachePoint':\n",
    "        print(f\"  Cache point detected: {block}\")\n",
    "    elif block_type == 'image':\n",
    "        # Show image format and size for comparison\n",
    "        image_info = block['image']\n",
    "        format_type = image_info.get('format', 'unknown')\n",
    "        byte_size = len(image_info['source']['bytes']) if 'source' in image_info and 'bytes' in image_info['source'] else 0\n",
    "        print(f\"  Image format: {format_type}, size: {byte_size} bytes\")\n",
    "    elif block_type == 'text':\n",
    "        text_preview = block['text'][:50].replace('\\n', ' ')\n",
    "        print(f\"  Text: '{text_preview}...' (length: {len(block['text'])} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Second Request - Cache HIT (Using Plain Dict/JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second request using plain dict/JSON format (as requested)\n",
    "print(\"=== Request 2: Cache HIT (Plain Dict Method) ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Manually construct message with cache point using dict format\n",
    "plain_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": shared_context},\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"format\": \"jpeg\",\n",
    "                \"source\": {\"bytes\": eiffel_bytes}\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"format\": \"jpeg\",\n",
    "                \"source\": {\"bytes\": tokyo_bytes}\n",
    "            }\n",
    "        },\n",
    "        {\"cachePoint\": {\"type\": \"default\"}},  # Manual cache point\n",
    "        {\"text\": analysis_prompts[1]}  # Different analysis prompt\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Make request with plain message\n",
    "response2 = manager.converse(messages=[plain_message])\n",
    "duration2 = time.time() - start_time\n",
    "\n",
    "# Display cache metrics and debug info\n",
    "cache_info = response2.get_cached_tokens_info()\n",
    "usage_info = response2.get_usage()\n",
    "efficiency = response2.get_cache_efficiency()\n",
    "\n",
    "print(f\"\\nDebug Information:\")\n",
    "print(f\"  - Success: {response2.success}\")\n",
    "print(f\"  - Model used: {response2.model_used}\")\n",
    "print(f\"  - Region used: {response2.region_used}\")\n",
    "print(f\"  - Cache info: {cache_info}\")\n",
    "print(f\"  - Usage info: {usage_info}\")\n",
    "print(f\"  - Duration: {duration2:.2f} seconds\")\n",
    "print(f\"  - Warnings: {response2.get_warnings()}\")\n",
    "\n",
    "if cache_info:\n",
    "    print(f\"\\nCache Metrics:\")\n",
    "    print(f\"  - Cache Read: {cache_info['cache_read_tokens']} tokens\")\n",
    "    print(f\"  - Cache Hit: {cache_info['cache_hit']}\")\n",
    "    print(f\"  - Cache Write: {cache_info['cache_write_tokens']} tokens\")\n",
    "    if cache_info['cache_read_tokens'] > 0:\n",
    "        print(f\"  - Speed improvement: {duration1/duration2:.1f}x faster\")\n",
    "    else:\n",
    "        print(f\"  - ‚ö†Ô∏è  No cache hit detected\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Cache info is None - caching may not be working\")\n",
    "\n",
    "if efficiency:\n",
    "    print(f\"\\nCache Efficiency Metrics:\")\n",
    "    print(f\"  - Hit ratio: {efficiency['cache_hit_ratio']*100:.1f}%\")\n",
    "    print(f\"  - Tokens saved: {efficiency['cache_savings_tokens']}\")\n",
    "    print(f\"  - Cost savings: {efficiency['cache_savings_cost']}\")\n",
    "    print(f\"  - Latency reduction: {efficiency['latency_reduction_ms']}ms\")\n",
    "\n",
    "print(f\"\\nResponse preview: {response2.get_content()[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Compare exact request structures to find cache miss cause\n",
    "print(\"=== CACHE MISS ROOT CAUSE ANALYSIS ===\")\n",
    "\n",
    "# Get both request structures\n",
    "request1_message = message.build()  # MessageBuilder request\n",
    "request2_message = plain_message    # Plain dict request\n",
    "\n",
    "print(f\"\\n1. MESSAGE STRUCTURE COMPARISON:\")\n",
    "print(f\"Request 1 (MessageBuilder): {len(request1_message['content'])} blocks\")\n",
    "print(f\"Request 2 (Plain dict): {len(request2_message['content'])} blocks\")\n",
    "\n",
    "print(f\"\\n2. DETAILED BLOCK-BY-BLOCK COMPARISON:\")\n",
    "max_blocks = max(len(request1_message['content']), len(request2_message['content']))\n",
    "\n",
    "for i in range(max_blocks):\n",
    "    print(f\"\\n--- Block {i} ---\")\n",
    "    \n",
    "    # Block from request 1\n",
    "    if i < len(request1_message['content']):\n",
    "        block1 = request1_message['content'][i]\n",
    "        block1_type = list(block1.keys())[0]\n",
    "        print(f\"Request 1: {block1_type}\")\n",
    "        \n",
    "        if block1_type == 'image':\n",
    "            img1 = block1['image']\n",
    "            print(f\"  Format: {img1.get('format', 'N/A')}\")\n",
    "            print(f\"  Source type: {list(img1.get('source', {}).keys())}\")\n",
    "            if 'source' in img1 and 'bytes' in img1['source']:\n",
    "                print(f\"  Bytes length: {len(img1['source']['bytes'])}\")\n",
    "                print(f\"  Bytes hash: {hash(img1['source']['bytes'])}\")\n",
    "        elif block1_type == 'text':\n",
    "            print(f\"  Text hash: {hash(block1['text'])}\")\n",
    "            print(f\"  Length: {len(block1['text'])} chars\")\n",
    "        elif block1_type == 'cachePoint':\n",
    "            print(f\"  Cache point: {block1['cachePoint']}\")\n",
    "    else:\n",
    "        print(f\"Request 1: (no block at position {i})\")\n",
    "    \n",
    "    # Block from request 2  \n",
    "    if i < len(request2_message['content']):\n",
    "        block2 = request2_message['content'][i]\n",
    "        block2_type = list(block2.keys())[0]\n",
    "        print(f\"Request 2: {block2_type}\")\n",
    "        \n",
    "        if block2_type == 'image':\n",
    "            img2 = block2['image']\n",
    "            print(f\"  Format: {img2.get('format', 'N/A')}\")\n",
    "            print(f\"  Source type: {list(img2.get('source', {}).keys())}\")\n",
    "            if 'source' in img2 and 'bytes' in img2['source']:\n",
    "                print(f\"  Bytes length: {len(img2['source']['bytes'])}\")\n",
    "                print(f\"  Bytes hash: {hash(img2['source']['bytes'])}\")\n",
    "        elif block2_type == 'text':\n",
    "            print(f\"  Text hash: {hash(block2['text'])}\")\n",
    "            print(f\"  Length: {len(block2['text'])} chars\")\n",
    "        elif block2_type == 'cachePoint':\n",
    "            print(f\"  Cache point: {block2['cachePoint']}\")\n",
    "    else:\n",
    "        print(f\"Request 2: (no block at position {i})\")\n",
    "    \n",
    "    # Compare blocks if both exist\n",
    "    if (i < len(request1_message['content']) and i < len(request2_message['content'])):\n",
    "        block1 = request1_message['content'][i]\n",
    "        block2 = request2_message['content'][i]\n",
    "        \n",
    "        # Deep comparison\n",
    "        blocks_identical = block1 == block2\n",
    "        print(f\"  ‚úÖ IDENTICAL\" if blocks_identical else f\"  ‚ùå DIFFERENT\")\n",
    "        \n",
    "        # If different, show what's different\n",
    "        if not blocks_identical:\n",
    "            block1_keys = set(block1.keys())\n",
    "            block2_keys = set(block2.keys())\n",
    "            if block1_keys != block2_keys:\n",
    "                print(f\"    Key difference: {block1_keys} vs {block2_keys}\")\n",
    "            \n",
    "            # Compare specific field differences\n",
    "            common_keys = block1_keys & block2_keys\n",
    "            for key in common_keys:\n",
    "                if block1[key] != block2[key]:\n",
    "                    print(f\"    Field '{key}' differs\")\n",
    "                    if key != 'image':  # Don't print huge image data\n",
    "                        print(f\"      Request 1: {str(block1[key])[:100]}...\")\n",
    "                        print(f\"      Request 2: {str(block2[key])[:100]}...\")\n",
    "\n",
    "print(f\"\\n3. CACHE COMPATIBILITY CHECK:\")\n",
    "# Check if content up to cache point is identical\n",
    "cache_point_pos1 = None\n",
    "cache_point_pos2 = None\n",
    "\n",
    "for i, block in enumerate(request1_message['content']):\n",
    "    if 'cachePoint' in block:\n",
    "        cache_point_pos1 = i\n",
    "        break\n",
    "\n",
    "for i, block in enumerate(request2_message['content']):\n",
    "    if 'cachePoint' in block:\n",
    "        cache_point_pos2 = i\n",
    "        break\n",
    "\n",
    "if cache_point_pos1 is not None and cache_point_pos2 is not None:\n",
    "    print(f\"Cache points at positions: {cache_point_pos1} vs {cache_point_pos2}\")\n",
    "    \n",
    "    # Compare content before cache points\n",
    "    content1_before_cache = request1_message['content'][:cache_point_pos1]\n",
    "    content2_before_cache = request2_message['content'][:cache_point_pos2]\n",
    "    \n",
    "    cache_content_identical = content1_before_cache == content2_before_cache\n",
    "    print(f\"Content before cache point identical: {cache_content_identical}\")\n",
    "    \n",
    "    if not cache_content_identical:\n",
    "        print(f\"‚ùå This is why cache is missing! Content before cache points differs.\")\n",
    "        print(f\"   Request 1 has {len(content1_before_cache)} blocks before cache\")\n",
    "        print(f\"   Request 2 has {len(content2_before_cache)} blocks before cache\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Cache content is identical - issue may be elsewhere\")\n",
    "else:\n",
    "    print(f\"‚ùå Cache points not found in one or both requests\")\n",
    "    print(f\"   Request 1 cache point: {cache_point_pos1}\")\n",
    "    print(f\"   Request 2 cache point: {cache_point_pos2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigation 1: Session Continuity Check\n",
    "print(\"=== SESSION CONTINUITY INVESTIGATION ===\")\n",
    "\n",
    "# Check if LLMManager reuses clients\n",
    "print(\"\\n1. LLMManager Client Analysis:\")\n",
    "print(f\"Manager instance ID: {id(manager)}\")\n",
    "\n",
    "# Try to get client info from auth manager\n",
    "try:\n",
    "    # Access the internal auth manager\n",
    "    auth_manager = manager._auth_manager\n",
    "    print(f\"Auth manager ID: {id(auth_manager)}\")\n",
    "    \n",
    "    # Get client and examine its properties\n",
    "    client1 = auth_manager.get_bedrock_client(region=\"us-east-1\")\n",
    "    client2 = auth_manager.get_bedrock_client(region=\"us-east-1\")\n",
    "    \n",
    "    print(f\"Client 1 ID: {id(client1)}\")\n",
    "    print(f\"Client 2 ID: {id(client2)}\")\n",
    "    print(f\"Clients are same instance: {client1 is client2}\")\n",
    "    \n",
    "    # Check for session info\n",
    "    print(f\"\\n2. Boto3 Session Analysis:\")\n",
    "    if hasattr(client1, '_client_config'):\n",
    "        print(f\"Client config: {client1._client_config}\")\n",
    "    \n",
    "    if hasattr(client1, 'meta'):\n",
    "        print(f\"Client meta: {client1.meta}\")\n",
    "        if hasattr(client1.meta, 'config'):\n",
    "            print(f\"Client meta config: {client1.meta.config}\")\n",
    "    \n",
    "    # Check if there's any request metadata we can access\n",
    "    print(f\"\\n3. Request Context:\")\n",
    "    print(f\"Client endpoint: {client1.meta.endpoint_url if hasattr(client1, 'meta') else 'N/A'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error investigating session: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigation 2: Use MessageBuilder for Both Requests\n",
    "print(\"=== MESSAGEBUILDER CONSISTENCY TEST ===\")\n",
    "\n",
    "# Create both requests using identical MessageBuilder patterns\n",
    "print(\"\\n1. Creating Request 1 with MessageBuilder:\")\n",
    "start_time1 = time.time()\n",
    "\n",
    "message1 = create_user_message(cache_config=cache_config)\n",
    "message1.add_text(shared_context, cacheable=True)\n",
    "message1.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "message1.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "message1.add_cache_point()\n",
    "message1.add_text(analysis_prompts[0], cacheable=False)\n",
    "\n",
    "response_mb1 = manager.converse(messages=[message1.build()])\n",
    "duration_mb1 = time.time() - start_time1\n",
    "\n",
    "print(f\"Request 1 Duration: {duration_mb1:.2f}s\")\n",
    "cache_info_mb1 = response_mb1.get_cached_tokens_info()\n",
    "print(f\"Cache Info 1: {cache_info_mb1}\")\n",
    "\n",
    "print(\"\\n2. Creating Request 2 with MessageBuilder (identical pattern):\")\n",
    "start_time2 = time.time()\n",
    "\n",
    "message2 = create_user_message(cache_config=cache_config)\n",
    "message2.add_text(shared_context, cacheable=True)  # Same as request 1\n",
    "message2.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)  # Same\n",
    "message2.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)  # Same\n",
    "message2.add_cache_point()  # Same cache point position\n",
    "message2.add_text(analysis_prompts[1], cacheable=False)  # Different prompt\n",
    "\n",
    "response_mb2 = manager.converse(messages=[message2.build()])\n",
    "duration_mb2 = time.time() - start_time2\n",
    "\n",
    "print(f\"Request 2 Duration: {duration_mb2:.2f}s\")\n",
    "cache_info_mb2 = response_mb2.get_cached_tokens_info()\n",
    "print(f\"Cache Info 2: {cache_info_mb2}\")\n",
    "\n",
    "# Compare structures\n",
    "msg1_built = message1.build()\n",
    "msg2_built = message2.build()\n",
    "\n",
    "print(\"\\n3. Structure Comparison:\")\n",
    "print(f\"Message 1 blocks: {len(msg1_built['content'])}\")\n",
    "print(f\"Message 2 blocks: {len(msg2_built['content'])}\")\n",
    "\n",
    "# Check if content before cache point is identical\n",
    "cache_pos_1 = None\n",
    "cache_pos_2 = None\n",
    "\n",
    "for i, block in enumerate(msg1_built['content']):\n",
    "    if 'cachePoint' in block:\n",
    "        cache_pos_1 = i\n",
    "        break\n",
    "\n",
    "for i, block in enumerate(msg2_built['content']):\n",
    "    if 'cachePoint' in block:\n",
    "        cache_pos_2 = i\n",
    "        break\n",
    "\n",
    "if cache_pos_1 is not None and cache_pos_2 is not None:\n",
    "    content_before_cache_1 = msg1_built['content'][:cache_pos_1]\n",
    "    content_before_cache_2 = msg2_built['content'][:cache_pos_2]\n",
    "    \n",
    "    identical = content_before_cache_1 == content_before_cache_2\n",
    "    print(f\"Content before cache points identical: {identical}\")\n",
    "    \n",
    "    if cache_info_mb2 and cache_info_mb2['cache_read_tokens'] > 0:\n",
    "        print(f\"‚úÖ SUCCESS: Cache hit with {cache_info_mb2['cache_read_tokens']} tokens\")\n",
    "        print(f\"Speed improvement: {duration_mb1/duration_mb2:.1f}x faster\")\n",
    "    else:\n",
    "        print(f\"‚ùå STILL NO CACHE HIT with MessageBuilder consistency\")\n",
    "        print(f\"This confirms the issue is NOT message structure differences\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigation 3: Single API Call with Multiple Messages\n",
    "print(\"=== SINGLE API CALL MULTI-MESSAGE TEST ===\")\n",
    "\n",
    "print(\"\\n1. Creating Multi-Message Request:\")\n",
    "print(\"This tests if conversation continuity within a single API call enables caching.\")\n",
    "\n",
    "start_time_multi = time.time()\n",
    "\n",
    "# Create first message (establishes cache)\n",
    "message_multi_1 = create_user_message(cache_config=cache_config)\n",
    "message_multi_1.add_text(shared_context, cacheable=True)\n",
    "message_multi_1.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "message_multi_1.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "message_multi_1.add_cache_point()\n",
    "message_multi_1.add_text(analysis_prompts[0], cacheable=False)\n",
    "\n",
    "# Create assistant response (simulated - for conversation flow)\n",
    "assistant_message = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\n",
    "        \"text\": \"I'll analyze the structural engineering approaches of both towers based on the images provided.\"\n",
    "    }]\n",
    "}\n",
    "\n",
    "# Create second user message (should hit cache if continuity works)\n",
    "message_multi_2 = create_user_message(cache_config=cache_config)\n",
    "message_multi_2.add_text(shared_context, cacheable=True)  # Same cached content\n",
    "message_multi_2.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "message_multi_2.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "message_multi_2.add_cache_point()\n",
    "message_multi_2.add_text(analysis_prompts[1], cacheable=False)  # Different question\n",
    "\n",
    "# Send as conversation with multiple messages\n",
    "multi_messages = [\n",
    "    message_multi_1.build(),\n",
    "    assistant_message,\n",
    "    message_multi_2.build()\n",
    "]\n",
    "\n",
    "try:\n",
    "    response_multi = manager.converse(messages=multi_messages)\n",
    "    duration_multi = time.time() - start_time_multi\n",
    "    \n",
    "    print(f\"\\n2. Multi-Message Results:\")\n",
    "    print(f\"Duration: {duration_multi:.2f}s\")\n",
    "    \n",
    "    cache_info_multi = response_multi.get_cached_tokens_info()\n",
    "    print(f\"Cache Info: {cache_info_multi}\")\n",
    "    \n",
    "    usage_info_multi = response_multi.get_usage()\n",
    "    print(f\"Usage Info: {usage_info_multi}\")\n",
    "    \n",
    "    if cache_info_multi and cache_info_multi['cache_read_tokens'] > 0:\n",
    "        print(f\"\\n‚úÖ SUCCESS: Multi-message conversation enabled caching!\")\n",
    "        print(f\"Cache read: {cache_info_multi['cache_read_tokens']} tokens\")\n",
    "        print(f\"This proves AWS needs conversation continuity for caching\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Multi-message approach also failed\")\n",
    "        print(f\"This suggests a deeper AWS Bedrock caching implementation issue\")\n",
    "        \n",
    "    # Show response preview\n",
    "    print(f\"\\nResponse preview: {response_multi.get_content()[:200]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Multi-message request failed: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    # This might indicate the approach isn't supported or needs different structure\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigation 4: Rapid Successive Requests (Session Timing)\n",
    "print(\"=== RAPID SUCCESSIVE REQUESTS TEST ===\")\n",
    "\n",
    "print(\"\\n1. Testing if rapid requests (no delay) enable caching:\")\n",
    "\n",
    "# Make requests with minimal delay\n",
    "rapid_responses = []\n",
    "rapid_timings = []\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nRapid Request {i+1}:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    message_rapid = create_user_message(cache_config=cache_config)\n",
    "    message_rapid.add_text(shared_context, cacheable=True)\n",
    "    message_rapid.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "    message_rapid.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "    message_rapid.add_cache_point()\n",
    "    message_rapid.add_text(f\"Question {i+1}: {analysis_prompts[i % len(analysis_prompts)]}\", cacheable=False)\n",
    "    \n",
    "    response_rapid = manager.converse(messages=[message_rapid.build()])\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    rapid_responses.append(response_rapid)\n",
    "    rapid_timings.append(duration)\n",
    "    \n",
    "    cache_info = response_rapid.get_cached_tokens_info()\n",
    "    print(f\"Duration: {duration:.2f}s\")\n",
    "    print(f\"Cache info: {cache_info}\")\n",
    "    \n",
    "    if cache_info and cache_info['cache_read_tokens'] > 0:\n",
    "        print(f\"‚úÖ Cache hit on request {i+1}: {cache_info['cache_read_tokens']} tokens\")\n",
    "        break\n",
    "    elif i == 0:\n",
    "        print(f\"Expected cache write on first request\")\n",
    "    else:\n",
    "        print(f\"‚ùå No cache hit on request {i+1}\")\n",
    "\n",
    "print(\"\\n2. Rapid Request Summary:\")\n",
    "for i, (response, duration) in enumerate(zip(rapid_responses, rapid_timings)):\n",
    "    cache_info = response.get_cached_tokens_info()\n",
    "    if cache_info:\n",
    "        cache_status = f\"Write: {cache_info['cache_write_tokens']}, Read: {cache_info['cache_read_tokens']}\"\n",
    "    else:\n",
    "        cache_status = \"No cache info\"\n",
    "    print(f\"Request {i+1}: {duration:.2f}s - {cache_status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigation 5: Client Reuse Test\n",
    "print(\"=== CLIENT REUSE INVESTIGATION ===\")\n",
    "\n",
    "print(\"\\n1. Testing manual client reuse:\")\n",
    "try:\n",
    "    # Get the same client instance that LLMManager uses\n",
    "    auth_manager = manager._auth_manager\n",
    "    reused_client = auth_manager.get_bedrock_client(region=\"us-east-1\")\n",
    "    \n",
    "    print(f\"Reused client ID: {id(reused_client)}\")\n",
    "    \n",
    "    # Manually build requests using the same client\n",
    "    print(\"\\n2. Manual Request 1 (Cache Write):\")\n",
    "    manual_start1 = time.time()\n",
    "    \n",
    "    # Build first request manually\n",
    "    manual_msg1 = create_user_message(cache_config=cache_config)\n",
    "    manual_msg1.add_text(shared_context, cacheable=True)\n",
    "    manual_msg1.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "    manual_msg1.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "    manual_msg1.add_cache_point()\n",
    "    manual_msg1.add_text(analysis_prompts[0], cacheable=False)\n",
    "    \n",
    "    manual_request1 = {\n",
    "        'modelId': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0',  # Full model ID\n",
    "        'messages': [manual_msg1.build()]\n",
    "    }\n",
    "    \n",
    "    manual_response1 = reused_client.converse(**manual_request1)\n",
    "    manual_duration1 = time.time() - manual_start1\n",
    "    \n",
    "    print(f\"Manual Response 1 Duration: {manual_duration1:.2f}s\")\n",
    "    if 'usage' in manual_response1:\n",
    "        usage1 = manual_response1['usage']\n",
    "        print(f\"Manual Usage 1: {usage1}\")\n",
    "        cache_write1 = usage1.get('cacheWriteInputTokens', 0)\n",
    "        print(f\"Cache write tokens: {cache_write1}\")\n",
    "    \n",
    "    print(\"\\n3. Manual Request 2 (Should Cache Hit):\")\n",
    "    manual_start2 = time.time()\n",
    "    \n",
    "    # Build second request manually with same client\n",
    "    manual_msg2 = create_user_message(cache_config=cache_config)\n",
    "    manual_msg2.add_text(shared_context, cacheable=True)  # Same content\n",
    "    manual_msg2.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "    manual_msg2.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "    manual_msg2.add_cache_point()\n",
    "    manual_msg2.add_text(analysis_prompts[1], cacheable=False)  # Different question\n",
    "    \n",
    "    manual_request2 = {\n",
    "        'modelId': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0',  # Same model ID\n",
    "        'messages': [manual_msg2.build()]\n",
    "    }\n",
    "    \n",
    "    manual_response2 = reused_client.converse(**manual_request2)\n",
    "    manual_duration2 = time.time() - manual_start2\n",
    "    \n",
    "    print(f\"Manual Response 2 Duration: {manual_duration2:.2f}s\")\n",
    "    if 'usage' in manual_response2:\n",
    "        usage2 = manual_response2['usage']\n",
    "        print(f\"Manual Usage 2: {usage2}\")\n",
    "        cache_read2 = usage2.get('cacheReadInputTokens', 0)\n",
    "        cache_write2 = usage2.get('cacheWriteInputTokens', 0)\n",
    "        print(f\"Cache read tokens: {cache_read2}\")\n",
    "        print(f\"Cache write tokens: {cache_write2}\")\n",
    "        \n",
    "        if cache_read2 > 0:\n",
    "            print(f\"\\n‚úÖ SUCCESS: Manual client reuse enabled caching!\")\n",
    "            print(f\"Speed improvement: {manual_duration1/manual_duration2:.1f}x\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Even manual client reuse failed to enable caching\")\n",
    "            print(f\"This suggests AWS Bedrock caching needs something beyond client continuity\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Manual client test failed: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Subsequent Requests - Demonstrating Cache Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process remaining prompts to show cumulative benefits\n",
    "total_tokens_saved = 0\n",
    "total_time_saved = 0\n",
    "responses = [response1, response2]\n",
    "\n",
    "print(\"=== Processing Remaining Analysis Prompts ===\")\n",
    "for i, prompt in enumerate(analysis_prompts[2:], start=3):\n",
    "    print(f\"\\nRequest {i}: {prompt[:50]}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use MessageBuilder for remaining requests\n",
    "    message = create_user_message(cache_config=cache_config)\n",
    "    message.add_text(shared_context, cacheable=True)\n",
    "    message.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "    message.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "    message.add_cache_point()\n",
    "    message.add_text(prompt, cacheable=False)\n",
    "    \n",
    "    response = manager.converse(messages=[message.build()])\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    cache_info = response.get_cached_tokens_info()\n",
    "    if cache_info and cache_info['cache_hit']:\n",
    "        print(f\"  ‚úì Cache HIT: {cache_info['cache_read_tokens']} tokens\")\n",
    "        print(f\"  Duration: {duration:.2f}s\")\n",
    "        total_tokens_saved += cache_info['cache_read_tokens']\n",
    "        total_time_saved += (duration1 - duration)\n",
    "    \n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary - Total Cache Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total benefits across all requests\n",
    "print(\"=== CACHING SUMMARY ===\")\n",
    "print(f\"\\nTotal requests made: {len(responses)}\")\n",
    "print(f\"Cache writes: 1 (first request only)\")\n",
    "print(f\"Cache hits: {len(responses) - 1}\")\n",
    "\n",
    "# Aggregate metrics\n",
    "total_cache_read = sum(r.get_cached_tokens_info()['cache_read_tokens'] \n",
    "                      for r in responses[1:] \n",
    "                      if r.get_cached_tokens_info())\n",
    "\n",
    "cache_write_tokens = response1.get_cached_tokens_info()['cache_write_tokens']\n",
    "\n",
    "print(f\"\\nToken Usage:\")\n",
    "print(f\"  - Tokens cached: {cache_write_tokens}\")\n",
    "print(f\"  - Total tokens read from cache: {total_cache_read}\")\n",
    "print(f\"  - Cache reuse factor: {total_cache_read / cache_write_tokens:.1f}x\")\n",
    "\n",
    "# Cost estimation (using example rate)\n",
    "COST_PER_1K_TOKENS = 0.03\n",
    "cost_without_cache = (len(responses) * cache_write_tokens / 1000) * COST_PER_1K_TOKENS\n",
    "cost_with_cache = (cache_write_tokens / 1000) * COST_PER_1K_TOKENS\n",
    "cost_savings = cost_without_cache - cost_with_cache\n",
    "\n",
    "print(f\"\\nCost Analysis:\")\n",
    "print(f\"  - Cost without caching: ${cost_without_cache:.2f}\")\n",
    "print(f\"  - Cost with caching: ${cost_with_cache:.2f}\")\n",
    "print(f\"  - Total savings: ${cost_savings:.2f} ({(cost_savings/cost_without_cache)*100:.0f}% reduction)\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  - Average time saved per request: {total_time_saved/(len(responses)-1):.2f}s\")\n",
    "print(f\"  - Total time saved: {total_time_saved:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing Cache Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization of cache impact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for visualization\n",
    "request_numbers = list(range(1, len(responses) + 1))\n",
    "cache_hits = [0] + [r.get_cached_tokens_info()['cache_read_tokens'] \n",
    "                   for r in responses[1:] \n",
    "                   if r.get_cached_tokens_info()]\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red'] + ['green'] * (len(responses) - 1)\n",
    "bars = plt.bar(request_numbers, cache_hits, color=colors)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, cache_hits):\n",
    "    if value > 0:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "                f'{value}', ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Request Number')\n",
    "plt.ylabel('Tokens Read from Cache')\n",
    "plt.title('Cache Performance Across Sequential Requests')\n",
    "plt.legend(['Cache Write (Request 1)', 'Cache Hit (Subsequent Requests)'])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Pie chart for cost breakdown\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels = ['Cached Content\\n(Paid Once)', 'Unique Content\\n(Paid Each Time)']\n",
    "sizes = [cache_write_tokens, len(responses) * 100]  # Assuming ~100 tokens per unique prompt\n",
    "colors = ['#4CAF50', '#FFC107']\n",
    "explode = (0.1, 0)\n",
    "\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "plt.title('Token Usage Distribution with Caching')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Caching must be explicitly enabled** - it's OFF by default\n",
    "2. **First request writes to cache** - includes one-time latency for caching\n",
    "3. **Subsequent requests hit cache** - dramatically faster and cheaper\n",
    "4. **Cache efficiency increases with more requests** - the same cached content is reused\n",
    "5. **Both MessageBuilder and plain dict methods support caching** - choose based on preference\n",
    "\n",
    "### Performance Benefits Observed:\n",
    "- **Cost Reduction**: ~80-90% for cached content\n",
    "- **Speed Improvement**: 2-5x faster responses\n",
    "- **Token Savings**: Proportional to number of requests sharing content\n",
    "\n",
    "### Best Practices:\n",
    "- Place cache points after shared content (images, context, instructions)\n",
    "- Keep unique content (specific questions) after cache points\n",
    "- Use CONSERVATIVE strategy for most use cases\n",
    "- Monitor cache metrics to optimize placement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Catalog Caching\n\nIn addition to prompt caching (demonstrated above), the LLM Manager also supports **Model Catalog Caching** through the `BedrockModelCatalog` system.\n\nThe catalog caches AWS Bedrock model availability data to improve performance and reduce API calls. This is separate from prompt caching and operates at the initialization level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Modes Overview\n\nThe `BedrockModelCatalog` supports three caching strategies:\n\n1. **FILE Mode** (default): Persistent file-based caching\n   - Survives process restarts\n   - Stored in platform-specific cache directory\n   - Best for development and long-running applications\n\n2. **MEMORY Mode**: In-memory caching only\n   - No file I/O operations\n   - Cache cleared on process restart\n   - Best for containerized environments\n\n3. **NONE Mode**: No caching, always fetch fresh\n   - Always queries AWS APIs\n   - Highest latency but guaranteed fresh data\n   - Best for Lambda functions or when data freshness is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BedrockModelCatalog and CacheMode\nfrom bestehorn_llmmanager.bedrock.catalog import BedrockModelCatalog, CacheMode\nfrom pathlib import Path\nimport time\n\nprint(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILE Mode Example\n\nFILE mode provides persistent caching across process restarts. The cache is stored in a JSON file on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize catalog with FILE mode (default)\nprint(\"=== FILE Mode Demo ===\")\nstart_time = time.time()\n\ncatalog_file = BedrockModelCatalog(\n    cache_mode=CacheMode.FILE,\n    cache_directory=Path(\"./demo_cache\"),  # Custom cache directory\n    cache_max_age_hours=24.0,  # Cache valid for 24 hours\n    force_refresh=True  # Force fresh data for demo\n)\n\nduration = time.time() - start_time\nprint(f\"Initialization time: {duration:.2f}s\\n\")\n\n# Get catalog metadata\nmetadata = catalog_file.get_catalog_metadata()\nprint(f\"üìä Catalog Metadata:\")\nprint(f\"   Source: {metadata.source.value}\")\nprint(f\"   Retrieved: {metadata.retrieval_timestamp}\")\nprint(f\"   Regions Queried: {len(metadata.api_regions_queried)}\")\nif metadata.cache_file_path:\n    print(f\"   Cache File: {metadata.cache_file_path}\")\n    print(f\"   Cache File Exists: {metadata.cache_file_path.exists()}\")\n\n# Query a model\nmodel_info = catalog_file.get_model_info(\"Claude 3.7 Sonnet\", \"us-east-1\")\nif model_info:\n    print(f\"\\nü§ñ Model Found:\")\n    print(f\"   Model ID: {model_info.model_id}\")\n    print(f\"   Access Method: {model_info.access_method.value}\")\nelse:\n    print(\"\\n‚ùå Model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMORY Mode Example\n\nMEMORY mode keeps the cache in memory only, with no file I/O. This is ideal for containerized environments or when file system access is limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize catalog with MEMORY mode\nprint(\"=== MEMORY Mode Demo ===\")\nstart_time = time.time()\n\ncatalog_memory = BedrockModelCatalog(\n    cache_mode=CacheMode.MEMORY,\n    force_refresh=True  # Force fresh data for demo\n)\n\nduration = time.time() - start_time\nprint(f\"Initialization time: {duration:.2f}s\\n\")\n\n# Get catalog metadata\nmetadata = catalog_memory.get_catalog_metadata()\nprint(f\"üìä Catalog Metadata:\")\nprint(f\"   Source: {metadata.source.value}\")\nprint(f\"   Retrieved: {metadata.retrieval_timestamp}\")\nprint(f\"   Regions Queried: {len(metadata.api_regions_queried)}\")\nprint(f\"   Cache File: {metadata.cache_file_path or 'N/A (memory only)'}\")\n\n# List available models\nall_models = catalog_memory.list_models()\nprint(f\"\\nüìã Total models available: {len(all_models)}\")\n\n# Filter by provider\nanthropic_models = catalog_memory.list_models(provider=\"Anthropic\")\nprint(f\"   Anthropic models: {len(anthropic_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NONE Mode Example\n\nNONE mode disables caching entirely, always fetching fresh data from AWS APIs. This ensures the most up-to-date information but has higher latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize catalog with NONE mode\nprint(\"=== NONE Mode Demo ===\")\nstart_time = time.time()\n\ncatalog_none = BedrockModelCatalog(\n    cache_mode=CacheMode.NONE,\n    fallback_to_bundled=True  # Use bundled data if API fails\n)\n\nduration = time.time() - start_time\nprint(f\"Initialization time: {duration:.2f}s\\n\")\n\n# Get catalog metadata\nmetadata = catalog_none.get_catalog_metadata()\nprint(f\"üìä Catalog Metadata:\")\nprint(f\"   Source: {metadata.source.value}\")\nprint(f\"   Retrieved: {metadata.retrieval_timestamp}\")\nprint(f\"   Regions Queried: {len(metadata.api_regions_queried)}\")\nprint(f\"   Cache File: {metadata.cache_file_path or 'N/A (no caching)'}\")\n\n# Check model availability\nis_available = catalog_none.is_model_available(\"Claude 3 Haiku\", \"us-west-2\")\nprint(f\"\\n‚úÖ Claude 3 Haiku available in us-west-2: {is_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Mode Comparison\n\n| Feature | FILE | MEMORY | NONE |\n|---------|------|--------|------|\n| **File I/O** | Yes | No | No |\n| **Warm Start** | Fast (cached) | Slow (API call) | Slow (API call) |\n| **Persistence** | Survives restarts | Process lifetime | None |\n| **Disk Space** | ~100KB | None | None |\n| **Best For** | Development, servers | Containers, Docker | Lambda, fresh data |\n| **Latency** | Lowest (after first) | Medium | Highest |\n| **Data Freshness** | Configurable | Configurable | Always fresh |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Configuration\n\nThe `BedrockModelCatalog` supports several advanced configuration options for fine-tuning cache behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced configuration example\nprint(\"=== Advanced Configuration Demo ===\")\n\ncatalog_advanced = BedrockModelCatalog(\n    cache_mode=CacheMode.FILE,\n    cache_directory=Path(\"./custom_cache\"),\n    cache_max_age_hours=12.0,  # Refresh cache after 12 hours\n    force_refresh=False,  # Use cache if valid\n    timeout=60,  # API timeout in seconds\n    max_workers=10,  # Parallel workers for multi-region queries\n    fallback_to_bundled=True  # Use bundled data if API fails\n)\n\nmetadata = catalog_advanced.get_catalog_metadata()\nprint(f\"Configuration applied successfully!\")\nprint(f\"Source: {metadata.source.value}\")\nprint(f\"Cache age: {(metadata.retrieval_timestamp).isoformat()}\")\n\n# Demonstrate filtering capabilities\nprint(f\"\\nüìã Filtering Examples:\")\n\n# Filter by provider and region\nmodels = catalog_advanced.list_models(\n    provider=\"Anthropic\",\n    region=\"us-east-1\"\n)\nprint(f\"   Anthropic models in us-east-1: {len(models)}\")\n\n# Filter by streaming support\nstreaming_models = catalog_advanced.list_models(streaming_only=True)\nprint(f\"   Streaming-capable models: {len(streaming_models)}\")\n\n# Combined filters\nanthropic_streaming = catalog_advanced.list_models(\n    provider=\"Anthropic\",\n    streaming_only=True\n)\nprint(f\"   Anthropic streaming models: {len(anthropic_streaming)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting Cache Issues\n\nCommon issues and solutions when working with catalog caching:\n\n#### Import Errors\n```python\n# ‚ùå Wrong import\nfrom bedrock.catalog import BedrockModelCatalog\n\n# ‚úÖ Correct import\nfrom bestehorn_llmmanager.bedrock.catalog import BedrockModelCatalog, CacheMode\n```\n\n#### Permission Errors (FILE mode)\nIf you encounter permission errors with FILE mode:\n```python\n# Solution 1: Use a writable directory\ncatalog = BedrockModelCatalog(\n    cache_mode=CacheMode.FILE,\n    cache_directory=Path(\"./my_cache\")  # Ensure this is writable\n)\n\n# Solution 2: Switch to MEMORY mode\ncatalog = BedrockModelCatalog(cache_mode=CacheMode.MEMORY)\n```\n\n#### API Timeout Errors\nIf API calls timeout during initialization:\n```python\n# Increase timeout and enable fallback\ncatalog = BedrockModelCatalog(\n    timeout=120,  # Longer timeout\n    fallback_to_bundled=True  # Use bundled data if API fails\n)\n```\n\n#### Stale Cache Data\nTo force a cache refresh:\n```python\n# Force refresh to get latest data\ncatalog = BedrockModelCatalog(\n    cache_mode=CacheMode.FILE,\n    force_refresh=True  # Ignore existing cache\n)\n```\n\n#### Lambda/Serverless Environments\nFor AWS Lambda or serverless environments:\n```python\n# Use NONE mode to avoid file system issues\ncatalog = BedrockModelCatalog(\n    cache_mode=CacheMode.NONE,\n    fallback_to_bundled=True\n)\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Catalog Caching Summary\n\n**Key Takeaways:**\n\n1. **FILE mode** (default): Best for development and long-running applications\n   - Persistent cache survives restarts\n   - Fast warm starts after initial fetch\n   - Configurable cache age\n\n2. **MEMORY mode**: Best for containerized environments\n   - No file I/O operations\n   - Cache cleared on restart\n   - Good for Docker/Kubernetes\n\n3. **NONE mode**: Best for Lambda and when freshness is critical\n   - Always fetches fresh data\n   - No cache management needed\n   - Higher latency but guaranteed current\n\n**Configuration Tips:**\n- Use `force_refresh=True` to bypass cache and fetch fresh data\n- Set `cache_max_age_hours` to control cache validity period\n- Enable `fallback_to_bundled=True` for offline/error scenarios\n- Adjust `timeout` for slow network connections\n\n**Additional Resources:**\n- See `docs/MIGRATION_GUIDE.md` for migrating from legacy managers\n- See `docs/forLLMConsumption.md` for complete API reference\n- See `examples/catalog_*.py` for more code examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}