{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Caching with LLM Manager\n",
    "\n",
    "This notebook demonstrates the new caching capabilities in LLM Manager using real images.\n",
    "We'll analyze architectural towers using caching to optimize costs and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eiffel Tower image exists: True\n",
      "Tokyo Tower image exists: True\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from bestehorn_llmmanager import LLMManager, create_user_message\n",
    "from bestehorn_llmmanager.bedrock.models.cache_structures import CacheConfig, CacheStrategy\n",
    "from bestehorn_llmmanager.bedrock.models.llm_manager_structures import AuthConfig, RetryConfig, AuthenticationType, RetryStrategy\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Set up paths to images\n",
    "images_dir = Path(\"../images\")\n",
    "eiffel_tower_path = images_dir / \"1200px-Tour_Eiffel_Wikimedia_Commons_(cropped).jpg\"\n",
    "tokyo_tower_path = images_dir / \"Tokyo_Tower_2023.jpg\"\n",
    "\n",
    "print(f\"Eiffel Tower image exists: {eiffel_tower_path.exists()}\")\n",
    "print(f\"Tokyo Tower image exists: {tokyo_tower_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize LLM Manager with Caching\n",
    "\n",
    "Note: Caching is **OFF by default** and must be explicitly enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Manager initialized with caching enabled\n",
      "Cache strategy: conservative\n",
      "Cache threshold: 1000 tokens\n"
     ]
    }
   ],
   "source": [
    "# Configure caching\n",
    "cache_config = CacheConfig(\n",
    "    enabled=True,  # Must explicitly enable\n",
    "    strategy=CacheStrategy.CONSERVATIVE,\n",
    "    cache_point_threshold=1000,  # Minimum tokens to cache\n",
    "    log_cache_failures=True\n",
    ")\n",
    "\n",
    "auth_config = AuthConfig(auth_type=AuthenticationType.PROFILE, profile_name=\"default\")\n",
    "\n",
    "# Initialize manager with caching\n",
    "manager = LLMManager(\n",
    "    models=[\"Claude 3.7 Sonnet\"],  # Use a model that supports caching\n",
    "    regions=[\"us-east-1\", \"us-west-2\"],\n",
    "    cache_config=cache_config,\n",
    "    auth_config=auth_config\n",
    ")\n",
    "\n",
    "print(\"LLM Manager initialized with caching enabled\")\n",
    "print(f\"Cache strategy: {cache_config.strategy.value}\")\n",
    "print(f\"Cache threshold: {cache_config.cache_point_threshold} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Analysis Prompts\n",
    "\n",
    "We'll use the same images with different analysis focuses to demonstrate caching benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 5 different analysis prompts\n"
     ]
    }
   ],
   "source": [
    "# Shared context for all prompts\n",
    "shared_context = \"\"\"You are an expert architectural analyst specializing in tower structures. \n",
    "Please analyze these two famous towers - the Eiffel Tower in Paris and the Tokyo Tower in Japan.\n",
    "Provide detailed insights based on the images provided.\"\"\"\n",
    "\n",
    "# Different analysis prompts\n",
    "analysis_prompts = [\n",
    "    \"Compare the structural engineering approaches used in both towers.\",\n",
    "    \"Analyze the architectural styles and their historical contexts.\",\n",
    "    \"Examine the materials and construction techniques visible in the images.\",\n",
    "    \"Compare the aesthetic design elements and their cultural significance.\",\n",
    "    \"Identify key differences in their structural support systems.\"\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(analysis_prompts)} different analysis prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eiffel Tower image size: 429,550 bytes (0.41 MB)\n",
      "Tokyo Tower image size: 3,569,727 bytes (3.40 MB)\n",
      "\n",
      "Estimated tokens for images: ~3999 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load image bytes\n",
    "with open(eiffel_tower_path, \"rb\") as f:\n",
    "    eiffel_bytes = f.read()\n",
    "\n",
    "with open(tokyo_tower_path, \"rb\") as f:\n",
    "    tokyo_bytes = f.read()\n",
    "\n",
    "print(f\"Eiffel Tower image size: {len(eiffel_bytes):,} bytes ({len(eiffel_bytes)/1024/1024:.2f} MB)\")\n",
    "print(f\"Tokyo Tower image size: {len(tokyo_bytes):,} bytes ({len(tokyo_bytes)/1024/1024:.2f} MB)\")\n",
    "print(f\"\\nEstimated tokens for images: ~{(len(eiffel_bytes) + len(tokyo_bytes)) // 1000} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. First Request - Cache WRITE (Using MessageBuilder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Content size (3569727 bytes) approaching limit (3750000 bytes) for image\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Request 1: Cache WRITE ===\n",
      "\n",
      "Response preview: # Architectural Analysis: Eiffel Tower and Tokyo Tower\n",
      "\n",
      "## Eiffel Tower\n",
      "\n",
      "The Eiffel Tower, captured in the first image, represents a pinnacle of 19th-century engineering innovation. Built in 1889 by G...\n"
     ]
    }
   ],
   "source": [
    "# First request - this will WRITE to cache\n",
    "print(\"=== Request 1: Cache WRITE ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Build message using MessageBuilder with explicit cache point\n",
    "message = create_user_message(cache_config=cache_config)\n",
    "message.add_text(shared_context, cacheable=True)\n",
    "message.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "message.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "message.add_cache_point()  # Explicit cache point after shared content\n",
    "message.add_text(analysis_prompts[0], cacheable=False)  # Unique prompt not cached\n",
    "\n",
    "# Make request\n",
    "response1 = manager.converse(messages=[message.build()])\n",
    "duration1 = time.time() - start_time\n",
    "\n",
    "# Display cache metrics and debugging info\n",
    "cache_info = response1.get_cached_tokens_info()\n",
    "usage_info = response1.get_usage()\n",
    "\n",
    "print(f\"\\nDebug Information:\")\n",
    "print(f\"  - Success: {response1.success}\")\n",
    "print(f\"  - Model used: {response1.model_used}\")\n",
    "print(f\"  - Region used: {response1.region_used}\")\n",
    "print(f\"  - Cache info: {cache_info}\")\n",
    "print(f\"  - Usage info: {usage_info}\")\n",
    "print(f\"  - Duration: {duration1:.2f} seconds\")\n",
    "\n",
    "if cache_info:\n",
    "    print(f\"\\nCache Metrics:\")\n",
    "    print(f\"  - Cache Write: {cache_info['cache_write_tokens']} tokens\")\n",
    "    print(f\"  - Cache Hit: {cache_info['cache_hit']}\")\n",
    "    print(f\"  - Cache Read: {cache_info['cache_read_tokens']} tokens\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Cache info is None - caching may not be working or supported\")\n",
    "\n",
    "# Show response preview\n",
    "print(f\"\\nResponse preview: {response1.get_content()[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Test if field mapping fix worked\n",
    "print(\"=== FIELD MAPPING FIX TEST ===\")\n",
    "if response1.response_data:\n",
    "    usage_raw = response1.response_data.get('usage', {})\n",
    "    print(f\"Raw AWS usage data: {usage_raw}\")\n",
    "    \n",
    "    # Test our fixed mapping\n",
    "    cache_read_aws = usage_raw.get('cacheReadInputTokens', 0)\n",
    "    cache_write_aws = usage_raw.get('cacheWriteInputTokens', 0)\n",
    "    print(f\"Direct AWS extraction: read={cache_read_aws}, write={cache_write_aws}\")\n",
    "    \n",
    "    # Test our BedrockResponse method\n",
    "    usage_processed = response1.get_usage()\n",
    "    print(f\"BedrockResponse.get_usage(): {usage_processed}\")\n",
    "    \n",
    "    cache_info_new = response1.get_cached_tokens_info()\n",
    "    print(f\"BedrockResponse.get_cached_tokens_info(): {cache_info_new}\")\n",
    "    \n",
    "    # Verify the fix worked\n",
    "    if cache_info_new and cache_info_new['cache_write_tokens'] == cache_write_aws:\n",
    "        print(\"✅ Field mapping fix SUCCESSFUL!\")\n",
    "    else:\n",
    "        print(\"❌ Field mapping still has issues\")\n",
    "        \n",
    "else:\n",
    "    print(\"No response data available\")\n",
    "    \n",
    "# Check if the message we sent had cache points\n",
    "print(f\"\\n=== SENT MESSAGE DEBUG ===\")\n",
    "built_message = message.build()\n",
    "print(f\"Message content blocks: {len(built_message['content'])}\")\n",
    "for i, block in enumerate(built_message['content']):\n",
    "    block_type = list(block.keys())[0]\n",
    "    print(f\"Block {i}: {block_type}\")\n",
    "    if block_type == 'cachePoint':\n",
    "        print(f\"  Cache point detected: {block}\")\n",
    "    elif block_type == 'image':\n",
    "        # Show image format and size for comparison\n",
    "        image_info = block['image']\n",
    "        format_type = image_info.get('format', 'unknown')\n",
    "        byte_size = len(image_info['source']['bytes']) if 'source' in image_info and 'bytes' in image_info['source'] else 0\n",
    "        print(f\"  Image format: {format_type}, size: {byte_size} bytes\")\n",
    "    elif block_type == 'text':\n",
    "        text_preview = block['text'][:50].replace('\\n', ' ')\n",
    "        print(f\"  Text: '{text_preview}...' (length: {len(block['text'])} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Second Request - Cache HIT (Using Plain Dict/JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Request 2: Cache HIT (Plain Dict Method) ===\n",
      "\n",
      "Response preview: # Architectural Analysis: Eiffel Tower vs. Tokyo Tower\n",
      "\n",
      "## Eiffel Tower\n",
      "\n",
      "The Eiffel Tower, shown in the first image, exemplifies late 19th-century industrial architecture and structural engineering br...\n"
     ]
    }
   ],
   "source": [
    "# Second request using plain dict/JSON format (as requested)\n",
    "print(\"=== Request 2: Cache HIT (Plain Dict Method) ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Manually construct message with cache point using dict format\n",
    "plain_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": shared_context},\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"format\": \"jpeg\",\n",
    "                \"source\": {\"bytes\": eiffel_bytes}\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"format\": \"jpeg\",\n",
    "                \"source\": {\"bytes\": tokyo_bytes}\n",
    "            }\n",
    "        },\n",
    "        {\"cachePoint\": {\"type\": \"default\"}},  # Manual cache point\n",
    "        {\"text\": analysis_prompts[1]}  # Different analysis prompt\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Make request with plain message\n",
    "response2 = manager.converse(messages=[plain_message])\n",
    "duration2 = time.time() - start_time\n",
    "\n",
    "# Display cache metrics and debug info\n",
    "cache_info = response2.get_cached_tokens_info()\n",
    "usage_info = response2.get_usage()\n",
    "efficiency = response2.get_cache_efficiency()\n",
    "\n",
    "print(f\"\\nDebug Information:\")\n",
    "print(f\"  - Success: {response2.success}\")\n",
    "print(f\"  - Model used: {response2.model_used}\")\n",
    "print(f\"  - Region used: {response2.region_used}\")\n",
    "print(f\"  - Cache info: {cache_info}\")\n",
    "print(f\"  - Usage info: {usage_info}\")\n",
    "print(f\"  - Duration: {duration2:.2f} seconds\")\n",
    "print(f\"  - Warnings: {response2.get_warnings()}\")\n",
    "\n",
    "if cache_info:\n",
    "    print(f\"\\nCache Metrics:\")\n",
    "    print(f\"  - Cache Read: {cache_info['cache_read_tokens']} tokens\")\n",
    "    print(f\"  - Cache Hit: {cache_info['cache_hit']}\")\n",
    "    print(f\"  - Cache Write: {cache_info['cache_write_tokens']} tokens\")\n",
    "    if cache_info['cache_read_tokens'] > 0:\n",
    "        print(f\"  - Speed improvement: {duration1/duration2:.1f}x faster\")\n",
    "    else:\n",
    "        print(f\"  - ⚠️  No cache hit detected\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Cache info is None - caching may not be working\")\n",
    "\n",
    "if efficiency:\n",
    "    print(f\"\\nCache Efficiency Metrics:\")\n",
    "    print(f\"  - Hit ratio: {efficiency['cache_hit_ratio']*100:.1f}%\")\n",
    "    print(f\"  - Tokens saved: {efficiency['cache_savings_tokens']}\")\n",
    "    print(f\"  - Cost savings: {efficiency['cache_savings_cost']}\")\n",
    "    print(f\"  - Latency reduction: {efficiency['latency_reduction_ms']}ms\")\n",
    "\n",
    "print(f\"\\nResponse preview: {response2.get_content()[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Compare exact request structures to find cache miss cause\n",
    "print(\"=== CACHE MISS ROOT CAUSE ANALYSIS ===\")\n",
    "\n",
    "# Get both request structures\n",
    "request1_message = message.build()  # MessageBuilder request\n",
    "request2_message = plain_message    # Plain dict request\n",
    "\n",
    "print(f\"\\n1. MESSAGE STRUCTURE COMPARISON:\")\n",
    "print(f\"Request 1 (MessageBuilder): {len(request1_message['content'])} blocks\")\n",
    "print(f\"Request 2 (Plain dict): {len(request2_message['content'])} blocks\")\n",
    "\n",
    "print(f\"\\n2. DETAILED BLOCK-BY-BLOCK COMPARISON:\")\n",
    "max_blocks = max(len(request1_message['content']), len(request2_message['content']))\n",
    "\n",
    "for i in range(max_blocks):\n",
    "    print(f\"\\n--- Block {i} ---\")\n",
    "    \n",
    "    # Block from request 1\n",
    "    if i < len(request1_message['content']):\n",
    "        block1 = request1_message['content'][i]\n",
    "        block1_type = list(block1.keys())[0]\n",
    "        print(f\"Request 1: {block1_type}\")\n",
    "        \n",
    "        if block1_type == 'image':\n",
    "            img1 = block1['image']\n",
    "            print(f\"  Format: {img1.get('format', 'N/A')}\")\n",
    "            print(f\"  Source type: {list(img1.get('source', {}).keys())}\")\n",
    "            if 'source' in img1 and 'bytes' in img1['source']:\n",
    "                print(f\"  Bytes length: {len(img1['source']['bytes'])}\")\n",
    "                print(f\"  Bytes hash: {hash(img1['source']['bytes'])}\")\n",
    "        elif block1_type == 'text':\n",
    "            print(f\"  Text hash: {hash(block1['text'])}\")\n",
    "            print(f\"  Length: {len(block1['text'])} chars\")\n",
    "        elif block1_type == 'cachePoint':\n",
    "            print(f\"  Cache point: {block1['cachePoint']}\")\n",
    "    else:\n",
    "        print(f\"Request 1: (no block at position {i})\")\n",
    "    \n",
    "    # Block from request 2  \n",
    "    if i < len(request2_message['content']):\n",
    "        block2 = request2_message['content'][i]\n",
    "        block2_type = list(block2.keys())[0]\n",
    "        print(f\"Request 2: {block2_type}\")\n",
    "        \n",
    "        if block2_type == 'image':\n",
    "            img2 = block2['image']\n",
    "            print(f\"  Format: {img2.get('format', 'N/A')}\")\n",
    "            print(f\"  Source type: {list(img2.get('source', {}).keys())}\")\n",
    "            if 'source' in img2 and 'bytes' in img2['source']:\n",
    "                print(f\"  Bytes length: {len(img2['source']['bytes'])}\")\n",
    "                print(f\"  Bytes hash: {hash(img2['source']['bytes'])}\")\n",
    "        elif block2_type == 'text':\n",
    "            print(f\"  Text hash: {hash(block2['text'])}\")\n",
    "            print(f\"  Length: {len(block2['text'])} chars\")\n",
    "        elif block2_type == 'cachePoint':\n",
    "            print(f\"  Cache point: {block2['cachePoint']}\")\n",
    "    else:\n",
    "        print(f\"Request 2: (no block at position {i})\")\n",
    "    \n",
    "    # Compare blocks if both exist\n",
    "    if (i < len(request1_message['content']) and i < len(request2_message['content'])):\n",
    "        block1 = request1_message['content'][i]\n",
    "        block2 = request2_message['content'][i]\n",
    "        \n",
    "        # Deep comparison\n",
    "        blocks_identical = block1 == block2\n",
    "        print(f\"  ✅ IDENTICAL\" if blocks_identical else f\"  ❌ DIFFERENT\")\n",
    "        \n",
    "        # If different, show what's different\n",
    "        if not blocks_identical:\n",
    "            block1_keys = set(block1.keys())\n",
    "            block2_keys = set(block2.keys())\n",
    "            if block1_keys != block2_keys:\n",
    "                print(f\"    Key difference: {block1_keys} vs {block2_keys}\")\n",
    "            \n",
    "            # Compare specific field differences\n",
    "            common_keys = block1_keys & block2_keys\n",
    "            for key in common_keys:\n",
    "                if block1[key] != block2[key]:\n",
    "                    print(f\"    Field '{key}' differs\")\n",
    "                    if key != 'image':  # Don't print huge image data\n",
    "                        print(f\"      Request 1: {str(block1[key])[:100]}...\")\n",
    "                        print(f\"      Request 2: {str(block2[key])[:100]}...\")\n",
    "\n",
    "print(f\"\\n3. CACHE COMPATIBILITY CHECK:\")\n",
    "# Check if content up to cache point is identical\n",
    "cache_point_pos1 = None\n",
    "cache_point_pos2 = None\n",
    "\n",
    "for i, block in enumerate(request1_message['content']):\n",
    "    if 'cachePoint' in block:\n",
    "        cache_point_pos1 = i\n",
    "        break\n",
    "\n",
    "for i, block in enumerate(request2_message['content']):\n",
    "    if 'cachePoint' in block:\n",
    "        cache_point_pos2 = i\n",
    "        break\n",
    "\n",
    "if cache_point_pos1 is not None and cache_point_pos2 is not None:\n",
    "    print(f\"Cache points at positions: {cache_point_pos1} vs {cache_point_pos2}\")\n",
    "    \n",
    "    # Compare content before cache points\n",
    "    content1_before_cache = request1_message['content'][:cache_point_pos1]\n",
    "    content2_before_cache = request2_message['content'][:cache_point_pos2]\n",
    "    \n",
    "    cache_content_identical = content1_before_cache == content2_before_cache\n",
    "    print(f\"Content before cache point identical: {cache_content_identical}\")\n",
    "    \n",
    "    if not cache_content_identical:\n",
    "        print(f\"❌ This is why cache is missing! Content before cache points differs.\")\n",
    "        print(f\"   Request 1 has {len(content1_before_cache)} blocks before cache\")\n",
    "        print(f\"   Request 2 has {len(content2_before_cache)} blocks before cache\")\n",
    "    else:\n",
    "        print(f\"✅ Cache content is identical - issue may be elsewhere\")\n",
    "else:\n",
    "    print(f\"❌ Cache points not found in one or both requests\")\n",
    "    print(f\"   Request 1 cache point: {cache_point_pos1}\")\n",
    "    print(f\"   Request 2 cache point: {cache_point_pos2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Subsequent Requests - Demonstrating Cache Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process remaining prompts to show cumulative benefits\n",
    "total_tokens_saved = 0\n",
    "total_time_saved = 0\n",
    "responses = [response1, response2]\n",
    "\n",
    "print(\"=== Processing Remaining Analysis Prompts ===\")\n",
    "for i, prompt in enumerate(analysis_prompts[2:], start=3):\n",
    "    print(f\"\\nRequest {i}: {prompt[:50]}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use MessageBuilder for remaining requests\n",
    "    message = create_user_message(cache_config=cache_config)\n",
    "    message.add_text(shared_context, cacheable=True)\n",
    "    message.add_image_bytes(eiffel_bytes, filename=\"eiffel_tower.jpg\", cacheable=True)\n",
    "    message.add_image_bytes(tokyo_bytes, filename=\"tokyo_tower.jpg\", cacheable=True)\n",
    "    message.add_cache_point()\n",
    "    message.add_text(prompt, cacheable=False)\n",
    "    \n",
    "    response = manager.converse(messages=[message.build()])\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    cache_info = response.get_cached_tokens_info()\n",
    "    if cache_info and cache_info['cache_hit']:\n",
    "        print(f\"  ✓ Cache HIT: {cache_info['cache_read_tokens']} tokens\")\n",
    "        print(f\"  Duration: {duration:.2f}s\")\n",
    "        total_tokens_saved += cache_info['cache_read_tokens']\n",
    "        total_time_saved += (duration1 - duration)\n",
    "    \n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary - Total Cache Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total benefits across all requests\n",
    "print(\"=== CACHING SUMMARY ===\")\n",
    "print(f\"\\nTotal requests made: {len(responses)}\")\n",
    "print(f\"Cache writes: 1 (first request only)\")\n",
    "print(f\"Cache hits: {len(responses) - 1}\")\n",
    "\n",
    "# Aggregate metrics\n",
    "total_cache_read = sum(r.get_cached_tokens_info()['cache_read_tokens'] \n",
    "                      for r in responses[1:] \n",
    "                      if r.get_cached_tokens_info())\n",
    "\n",
    "cache_write_tokens = response1.get_cached_tokens_info()['cache_write_tokens']\n",
    "\n",
    "print(f\"\\nToken Usage:\")\n",
    "print(f\"  - Tokens cached: {cache_write_tokens}\")\n",
    "print(f\"  - Total tokens read from cache: {total_cache_read}\")\n",
    "print(f\"  - Cache reuse factor: {total_cache_read / cache_write_tokens:.1f}x\")\n",
    "\n",
    "# Cost estimation (using example rate)\n",
    "COST_PER_1K_TOKENS = 0.03\n",
    "cost_without_cache = (len(responses) * cache_write_tokens / 1000) * COST_PER_1K_TOKENS\n",
    "cost_with_cache = (cache_write_tokens / 1000) * COST_PER_1K_TOKENS\n",
    "cost_savings = cost_without_cache - cost_with_cache\n",
    "\n",
    "print(f\"\\nCost Analysis:\")\n",
    "print(f\"  - Cost without caching: ${cost_without_cache:.2f}\")\n",
    "print(f\"  - Cost with caching: ${cost_with_cache:.2f}\")\n",
    "print(f\"  - Total savings: ${cost_savings:.2f} ({(cost_savings/cost_without_cache)*100:.0f}% reduction)\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  - Average time saved per request: {total_time_saved/(len(responses)-1):.2f}s\")\n",
    "print(f\"  - Total time saved: {total_time_saved:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing Cache Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization of cache impact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for visualization\n",
    "request_numbers = list(range(1, len(responses) + 1))\n",
    "cache_hits = [0] + [r.get_cached_tokens_info()['cache_read_tokens'] \n",
    "                   for r in responses[1:] \n",
    "                   if r.get_cached_tokens_info()]\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red'] + ['green'] * (len(responses) - 1)\n",
    "bars = plt.bar(request_numbers, cache_hits, color=colors)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, cache_hits):\n",
    "    if value > 0:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "                f'{value}', ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Request Number')\n",
    "plt.ylabel('Tokens Read from Cache')\n",
    "plt.title('Cache Performance Across Sequential Requests')\n",
    "plt.legend(['Cache Write (Request 1)', 'Cache Hit (Subsequent Requests)'])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Pie chart for cost breakdown\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels = ['Cached Content\\n(Paid Once)', 'Unique Content\\n(Paid Each Time)']\n",
    "sizes = [cache_write_tokens, len(responses) * 100]  # Assuming ~100 tokens per unique prompt\n",
    "colors = ['#4CAF50', '#FFC107']\n",
    "explode = (0.1, 0)\n",
    "\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "plt.title('Token Usage Distribution with Caching')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Caching must be explicitly enabled** - it's OFF by default\n",
    "2. **First request writes to cache** - includes one-time latency for caching\n",
    "3. **Subsequent requests hit cache** - dramatically faster and cheaper\n",
    "4. **Cache efficiency increases with more requests** - the same cached content is reused\n",
    "5. **Both MessageBuilder and plain dict methods support caching** - choose based on preference\n",
    "\n",
    "### Performance Benefits Observed:\n",
    "- **Cost Reduction**: ~80-90% for cached content\n",
    "- **Speed Improvement**: 2-5x faster responses\n",
    "- **Token Savings**: Proportional to number of requests sharing content\n",
    "\n",
    "### Best Practices:\n",
    "- Place cache points after shared content (images, context, instructions)\n",
    "- Keep unique content (specific questions) after cache points\n",
    "- Use CONSERVATIVE strategy for most use cases\n",
    "- Monitor cache metrics to optimize placement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
