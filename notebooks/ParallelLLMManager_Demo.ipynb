{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParallelLLMManager Comprehensive Demo\n",
    "\n",
    "This notebook demonstrates the **ParallelLLMManager** - an advanced solution for parallel processing of AWS Bedrock requests across multiple models and regions. We'll showcase concurrent execution, load balancing, fault tolerance, and performance optimization.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- 🚀 **Parallel Processing**: Execute multiple requests concurrently across regions\n",
    "- 🌍 **Multi-Region Support**: Intelligent load distribution across AWS regions  \n",
    "- 🔄 **Load Balancing**: Round-robin, random, and least-loaded strategies\n",
    "- 🛡️ **Fault Tolerance**: Multiple failure handling strategies\n",
    "- 📊 **Performance Analytics**: Detailed execution statistics and metrics\n",
    "- 🖼️ **Multimodal Support**: Parallel processing of text, image, and video content\n",
    "- ⚙️ **Flexible Configuration**: Customizable concurrency and timeout settings\n",
    "- 🔧 **MessageBuilder Integration**: Clean, fluent interface for creating requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import random\n",
    "\n",
    "# Add the src directory to path for imports\n",
    "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Import ParallelLLMManager and related classes\n",
    "from bestehorn_llmmanager import ParallelLLMManager\n",
    "from bestehorn_llmmanager.bedrock.models.parallel_structures import (\n",
    "    BedrockConverseRequest, \n",
    "    ParallelProcessingConfig,\n",
    "    FailureHandlingStrategy,\n",
    "    LoadBalancingStrategy\n",
    ")\n",
    "from bestehorn_llmmanager.bedrock.models.llm_manager_structures import (\n",
    "    AuthConfig, \n",
    "    RetryConfig, \n",
    "    AuthenticationType, \n",
    "    RetryStrategy\n",
    ")\n",
    "from bestehorn_llmmanager.bedrock.exceptions.parallel_exceptions import (\n",
    "    ParallelProcessingError, \n",
    "    ParallelExecutionError, \n",
    "    ParallelConfigurationError\n",
    ")\n",
    "\n",
    "# Import MessageBuilder components\n",
    "from bestehorn_llmmanager import create_user_message, create_assistant_message, create_message\n",
    "from bestehorn_llmmanager import RolesEnum, ImageFormatEnum, DocumentFormatEnum, VideoFormatEnum\n",
    "\n",
    "# Configure logging for better visibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "print(\"✅ Imports successful!\")\n",
    "print(f\"📁 Working directory: {Path.cwd()}\")\n",
    "print(\"🔧 MessageBuilder integration enabled for clean request creation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request Creation Functions\n",
    "\n",
    "Using the MessageBuilder system to create requests cleanly and avoid serialization bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_request_with_builder(text: str, inference_config: Dict[str, Any] = None) -> BedrockConverseRequest:\n",
    "    \"\"\"Create a text-based BedrockConverseRequest using MessageBuilder.\"\"\"\n",
    "    message = create_user_message() \\\n",
    "        .add_text(text) \\\n",
    "        .build()\n",
    "    \n",
    "    return BedrockConverseRequest(\n",
    "        messages=[message],\n",
    "        inference_config=inference_config or {\"maxTokens\": 500, \"temperature\": 0.7}\n",
    "    )\n",
    "\n",
    "def create_image_request_with_builder(text: str, image_path: str, inference_config: Dict[str, Any] = None) -> BedrockConverseRequest:\n",
    "    \"\"\"Create an image analysis BedrockConverseRequest using MessageBuilder.\"\"\"\n",
    "    try:\n",
    "        message = create_user_message() \\\n",
    "            .add_text(text) \\\n",
    "            .add_local_image(path_to_local_file=image_path, max_size_mb=5.0) \\\n",
    "            .build()\n",
    "        \n",
    "        return BedrockConverseRequest(\n",
    "            messages=[message],\n",
    "            inference_config=inference_config or {\"maxTokens\": 800, \"temperature\": 0.3}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating image request: {e}\")\n",
    "        # Fall back to text-only request\n",
    "        return create_text_request_with_builder(f\"{text} (Note: Image could not be loaded from {image_path})\")\n",
    "\n",
    "def display_parallel_response(response, title=\"Parallel Response\"):\n",
    "    \"\"\"Display a formatted parallel response.\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * len(title))\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"✅ Overall Success: {response.success}\")\n",
    "    print(f\"📊 Success Rate: {response.get_success_rate():.1f}%\")\n",
    "    print(f\"⏱️  Total Duration: {response.total_duration_ms:.2f}ms\")\n",
    "    print(f\"🔢 Total Requests: {len(response.request_responses)}\")\n",
    "    print(f\"✅ Successful Requests: {len(response.get_successful_responses())}\")\n",
    "    print(f\"❌ Failed Requests: {len(response.get_failed_responses())}\")\n",
    "    \n",
    "    # Execution statistics\n",
    "    if response.parallel_execution_stats:\n",
    "        stats = response.parallel_execution_stats\n",
    "        print(f\"\\n📈 Execution Statistics:\")\n",
    "        print(f\"   ⚡ Avg Duration: {stats.average_request_duration_ms:.2f}ms\")\n",
    "        print(f\"   📊 Max Duration: {stats.max_request_duration_ms:.2f}ms\")\n",
    "        print(f\"   ⚡ Min Duration: {stats.min_request_duration_ms:.2f}ms\")\n",
    "        print(f\"   🔀 Concurrent Executions: {stats.concurrent_executions}\")\n",
    "        \n",
    "        if stats.region_distribution:\n",
    "            print(f\"   🌍 Region Distribution:\")\n",
    "            for region, count in stats.region_distribution.items():\n",
    "                print(f\"      {region}: {count} requests\")\n",
    "    \n",
    "    # Token usage\n",
    "    total_tokens = response.get_total_tokens_used()\n",
    "    if any(total_tokens.values()):\n",
    "        print(f\"\\n🎯 Total Token Usage:\")\n",
    "        for key, value in total_tokens.items():\n",
    "            if value > 0:\n",
    "                print(f\"   {key}: {value:,}\")\n",
    "    \n",
    "    # Average latency\n",
    "    avg_latency = response.get_average_latency()\n",
    "    if avg_latency:\n",
    "        print(f\"\\n⚡ Average API Latency: {avg_latency:.2f}ms\")\n",
    "    \n",
    "    # Warnings\n",
    "    if response.warnings:\n",
    "        print(f\"\\n⚠️  Warnings ({len(response.warnings)}):\")\n",
    "        for warning in response.warnings[:3]:  # Show first 3 warnings\n",
    "            print(f\"   - {warning}\")\n",
    "        if len(response.warnings) > 3:\n",
    "            print(f\"   ... and {len(response.warnings) - 3} more\")\n",
    "\n",
    "def display_individual_responses(response, max_responses=3):\n",
    "    \"\"\"Display individual responses from the parallel execution.\"\"\"\n",
    "    successful_responses = response.get_successful_responses()\n",
    "    \n",
    "    print(f\"\\n💬 Sample Individual Responses (showing up to {max_responses}):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    count = 0\n",
    "    for req_id, bedrock_response in successful_responses.items():\n",
    "        if count >= max_responses:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\n🆔 Request ID: {req_id}\")\n",
    "        print(f\"🤖 Model: {bedrock_response.model_used}\")\n",
    "        print(f\"🌍 Region: {bedrock_response.region_used}\")\n",
    "        print(f\"⏱️  Duration: {bedrock_response.total_duration_ms:.2f}ms\")\n",
    "        \n",
    "        content = bedrock_response.get_content()\n",
    "        if content:\n",
    "            # Truncate long responses\n",
    "            if len(content) > 200:\n",
    "                content = content[:200] + \"...\"\n",
    "            print(f\"💭 Response: {content}\")\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    if len(successful_responses) > max_responses:\n",
    "        print(f\"\\n... and {len(successful_responses) - max_responses} more successful responses\")\n",
    "\n",
    "print(\"✅ MessageBuilder-based helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the ParallelLLMManager\n",
    "\n",
    "We'll create a ParallelLLMManager instance with multiple models and regions for optimal parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Initializing ParallelLLMManager...\")\n",
    "\n",
    "# Define models for parallel processing\n",
    "models = [\n",
    "    \"Claude 3.5 Sonnet v2\",  # High-quality responses\n",
    "    \"Claude 3 Haiku\",       # Fast processing\n",
    "    \"Nova Pro\",             # Multimodal capabilities\n",
    "    \"Nova Lite\"             # Backup option\n",
    "]\n",
    "\n",
    "# Define regions for load distribution\n",
    "regions = [\n",
    "    \"us-east-1\",\n",
    "    \"us-west-2\", \n",
    "    \"eu-west-1\"\n",
    "]\n",
    "\n",
    "# Configure authentication\n",
    "auth_config = AuthConfig(\n",
    "    auth_type=AuthenticationType.PROFILE,\n",
    "    profile_name=\"default\"\n",
    ")\n",
    "\n",
    "# Configure retry behavior\n",
    "retry_config = RetryConfig(\n",
    "    max_retries=2,\n",
    "    retry_strategy=RetryStrategy.REGION_FIRST\n",
    ")\n",
    "\n",
    "# Configure parallel processing behavior\n",
    "parallel_config = ParallelProcessingConfig(\n",
    "    max_concurrent_requests=8,\n",
    "    request_timeout_seconds=60,\n",
    "    failure_handling_strategy=FailureHandlingStrategy.CONTINUE_ON_FAILURE,\n",
    "    load_balancing_strategy=LoadBalancingStrategy.ROUND_ROBIN,\n",
    "    failure_threshold=0.3  # 30% failure threshold\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Initialize the ParallelLLMManager\n",
    "    parallel_manager = ParallelLLMManager(\n",
    "        models=models,\n",
    "        regions=regions,\n",
    "        auth_config=auth_config,\n",
    "        retry_config=retry_config,\n",
    "        parallel_config=parallel_config,\n",
    "        timeout=30\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ ParallelLLMManager initialized successfully!\")\n",
    "    print(f\"   🤖 Models: {len(parallel_manager.get_available_models())}\")\n",
    "    print(f\"   🌍 Regions: {len(parallel_manager.get_available_regions())}\")\n",
    "    print(f\"   🔀 Max Concurrent: {parallel_config.max_concurrent_requests}\")\n",
    "    print(f\"   ⚖️  Load Balancing: {parallel_config.load_balancing_strategy.value}\")\n",
    "    print(f\"\\n{parallel_manager}\")\n",
    "    \n",
    "    # Validate configuration\n",
    "    validation = parallel_manager.validate_configuration()\n",
    "    print(f\"\\n🔍 Configuration Validation:\")\n",
    "    print(f\"   Valid: {'✅' if validation['valid'] else '❌'} {validation['valid']}\")\n",
    "    print(f\"   Auth Status: {validation['auth_status']}\")\n",
    "    print(f\"   Model-Region Combinations: {validation['model_region_combinations']}\")\n",
    "    print(f\"   Parallel Config Valid: {'✅' if validation['parallel_config_valid'] else '❌'}\")\n",
    "    \n",
    "    if validation['warnings'] or validation.get('parallel_warnings', []):\n",
    "        all_warnings = validation['warnings'] + validation.get('parallel_warnings', [])\n",
    "        print(f\"   ⚠️  Warnings: {len(all_warnings)}\")\n",
    "        for warning in all_warnings[:2]:\n",
    "            print(f\"      - {warning}\")\n",
    "    \n",
    "    if validation['errors'] or validation.get('parallel_errors', []):\n",
    "        all_errors = validation['errors'] + validation.get('parallel_errors', [])\n",
    "        print(f\"   ❌ Errors: {len(all_errors)}\")\n",
    "        for error in all_errors:\n",
    "            print(f\"      - {error}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing ParallelLLMManager: {e}\")\n",
    "    print(\"\\n💡 Troubleshooting tips:\")\n",
    "    print(\"   1. Ensure AWS credentials are configured\")\n",
    "    print(\"   2. Check access to specified models and regions\")\n",
    "    print(\"   3. Verify network connectivity to AWS\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Parallel Text Processing 💬\n",
    "\n",
    "Let's start with parallel processing of multiple text requests using the MessageBuilder system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💬 Example 1: Basic Parallel Text Processing (MessageBuilder)\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "# Create multiple text requests using MessageBuilder\n",
    "text_prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"What are the benefits of renewable energy?\",\n",
    "    \"Describe the process of photosynthesis.\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"What is the significance of DNA in genetics?\",\n",
    "    \"Explain the water cycle.\"\n",
    "]\n",
    "\n",
    "# Create BedrockConverseRequest objects using MessageBuilder\n",
    "text_requests = []\n",
    "for prompt in text_prompts:\n",
    "    request = create_text_request_with_builder(text=prompt)\n",
    "    text_requests.append(request)\n",
    "\n",
    "print(f\"📝 Created {len(text_requests)} text requests using MessageBuilder\")\n",
    "for i, req in enumerate(text_requests, 1):\n",
    "    print(f\"   {i}. {req.request_id}: {text_prompts[i-1][:50]}...\")\n",
    "\n",
    "try:\n",
    "    # Execute requests in parallel\n",
    "    print(f\"\\n🚀 Executing {len(text_requests)} requests in parallel...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    parallel_response = parallel_manager.converse_parallel(\n",
    "        requests=text_requests\n",
    "        # Note: target_regions_per_request not specified - will auto-adjust based on \n",
    "        # available regions (3) and max_concurrent_requests (8), using min(8,3) = 3\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    sequential_estimate = len(text_requests) * 3  # Rough estimate: 3 seconds per request\n",
    "    \n",
    "    print(f\"✅ Parallel execution completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"⚡ Estimated sequential time: {sequential_estimate:.1f} seconds\")\n",
    "    print(f\"🚀 Speed improvement: ~{sequential_estimate / (end_time - start_time):.1f}x faster\")\n",
    "    \n",
    "    # Display results\n",
    "    display_parallel_response(parallel_response, \"🗣️  Parallel Text Processing Results\")\n",
    "    display_individual_responses(parallel_response, max_responses=2)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in parallel text processing: {e}\")\n",
    "    print(f\"   Type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Parallel Image Analysis 🖼️\n",
    "\n",
    "Now let's demonstrate parallel processing of image analysis requests using MessageBuilder's local image functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🖼️  Example 2: Parallel Image Analysis (MessageBuilder)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define image analysis tasks\n",
    "image_tasks = [\n",
    "    {\n",
    "        \"text\": \"Analyze the architectural features and historical significance of this landmark.\",\n",
    "        \"image_path\": \"../images/1200px-Tour_Eiffel_Wikimedia_Commons_(cropped).jpg\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Describe the structural design and engineering aspects of this tower.\",\n",
    "        \"image_path\": \"../images/Tokyo_Tower_2023.jpg\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"What can you tell me about the lighting and atmospheric conditions in this image?\",\n",
    "        \"image_path\": \"../images/1200px-Tour_Eiffel_Wikimedia_Commons_(cropped).jpg\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Compare this tower's design to other famous towers around the world.\",\n",
    "        \"image_path\": \"../images/Tokyo_Tower_2023.jpg\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create image analysis requests using MessageBuilder\n",
    "image_requests = []\n",
    "for task in image_tasks:\n",
    "    request = create_image_request_with_builder(\n",
    "        text=task[\"text\"],\n",
    "        image_path=task[\"image_path\"]\n",
    "    )\n",
    "    image_requests.append(request)\n",
    "\n",
    "print(f\"🖼️  Created {len(image_requests)} image analysis requests using MessageBuilder\")\n",
    "for i, task in enumerate(image_tasks, 1):\n",
    "    image_name = Path(task[\"image_path\"]).name\n",
    "    print(f\"   {i}. {image_name}: {task['text'][:40]}...\")\n",
    "\n",
    "try:\n",
    "    # Execute image analysis in parallel\n",
    "    print(f\"\\n🚀 Executing {len(image_requests)} image analysis requests in parallel...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    image_response = parallel_manager.converse_parallel(\n",
    "        requests=image_requests,\n",
    "        target_regions_per_request=3  # Use more regions for vision models\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"✅ Parallel image analysis completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Display results\n",
    "    display_parallel_response(image_response, \"🖼️  Parallel Image Analysis Results\")\n",
    "    display_individual_responses(image_response, max_responses=2)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in parallel image analysis: {e}\")\n",
    "    print(f\"   Type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Performance Benchmarking 📊\n",
    "\n",
    "Let's benchmark the performance improvements of parallel processing versus sequential execution using MessageBuilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Example 3: Performance Benchmarking (MessageBuilder)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create benchmark requests using MessageBuilder\n",
    "benchmark_prompts = [\n",
    "    \"Summarize the history of computer science.\",\n",
    "    \"Explain the principles of quantum mechanics.\",\n",
    "    \"Describe the process of cellular respiration.\",\n",
    "    \"What are the major causes of climate change?\",\n",
    "    \"How do neural networks learn?\",\n",
    "    \"Explain the theory of evolution.\",\n",
    "    \"What is the importance of biodiversity?\",\n",
    "    \"How does the internet work?\"\n",
    "]\n",
    "\n",
    "benchmark_requests = []\n",
    "for prompt in benchmark_prompts:\n",
    "    request = create_text_request_with_builder(\n",
    "        text=prompt,\n",
    "        inference_config={\"maxTokens\": 400, \"temperature\": 0.6}\n",
    "    )\n",
    "    benchmark_requests.append(request)\n",
    "\n",
    "print(f\"📝 Created {len(benchmark_requests)} benchmark requests using MessageBuilder\")\n",
    "\n",
    "# Test 1: Sequential execution using single requests\n",
    "print(\"\\n🐌 Sequential Execution Test...\")\n",
    "sequential_start = time.time()\n",
    "sequential_responses = []\n",
    "sequential_success_count = 0\n",
    "\n",
    "try:\n",
    "    for i, request in enumerate(benchmark_requests[:4], 1):  # Limit to 4 for time\n",
    "        print(f\"   Processing request {i}/4...\")\n",
    "        response = parallel_manager.converse_with_request(request)\n",
    "        sequential_responses.append(response)\n",
    "        if response.success:\n",
    "            sequential_success_count += 1\n",
    "    \n",
    "    sequential_end = time.time()\n",
    "    sequential_duration = sequential_end - sequential_start\n",
    "    \n",
    "    print(f\"✅ Sequential execution completed in {sequential_duration:.2f} seconds\")\n",
    "    print(f\"   Success rate: {(sequential_success_count/4)*100:.1f}%\")\n",
    "    print(f\"   Average per request: {sequential_duration/4:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in sequential execution: {e}\")\n",
    "    sequential_duration = float('inf')  # Set high value for comparison\n",
    "\n",
    "# Test 2: Parallel execution\n",
    "print(\"\\n🚀 Parallel Execution Test...\")\n",
    "parallel_start = time.time()\n",
    "\n",
    "try:\n",
    "    parallel_response = parallel_manager.converse_parallel(\n",
    "        requests=benchmark_requests[:4],  # Same 4 requests for fair comparison\n",
    "        target_regions_per_request=2\n",
    "    )\n",
    "    \n",
    "    parallel_end = time.time()\n",
    "    parallel_duration = parallel_end - parallel_start\n",
    "    \n",
    "    print(f\"✅ Parallel execution completed in {parallel_duration:.2f} seconds\")\n",
    "    print(f\"   Success rate: {parallel_response.get_success_rate():.1f}%\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    if sequential_duration != float('inf'):\n",
    "        speedup = sequential_duration / parallel_duration\n",
    "        efficiency = (speedup / parallel_config.max_concurrent_requests) * 100\n",
    "        \n",
    "        print(f\"\\n📈 Performance Comparison:\")\n",
    "        print(f\"   Sequential: {sequential_duration:.2f}s\")\n",
    "        print(f\"   Parallel: {parallel_duration:.2f}s\")\n",
    "        print(f\"   Speedup: {speedup:.2f}x\")\n",
    "        print(f\"   Parallel Efficiency: {efficiency:.1f}%\")\n",
    "        print(f\"   Time Saved: {sequential_duration - parallel_duration:.2f}s ({((sequential_duration - parallel_duration)/sequential_duration)*100:.1f}%)\")\n",
    "    \n",
    "    # Display detailed parallel statistics\n",
    "    display_parallel_response(parallel_response, \"📊 Benchmark Results\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in parallel execution: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
