{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParallelLLMManager Comprehensive Demo\n",
    "\n",
    "This notebook demonstrates the **ParallelLLMManager** - an advanced solution for parallel processing of AWS Bedrock requests across multiple models and regions. We'll showcase concurrent execution, load balancing, fault tolerance, and performance optimization.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- üöÄ **Parallel Processing**: Execute multiple requests concurrently across regions\n",
    "- üåç **Multi-Region Support**: Intelligent load distribution across AWS regions  \n",
    "- üîÑ **Load Balancing**: Round-robin, random, and least-loaded strategies\n",
    "- üõ°Ô∏è **Fault Tolerance**: Multiple failure handling strategies\n",
    "- üìä **Performance Analytics**: Detailed execution statistics and metrics\n",
    "- üñºÔ∏è **Multimodal Support**: Parallel processing of text, image, and video content\n",
    "- ‚öôÔ∏è **Flexible Configuration**: Customizable concurrency and timeout settings\n",
    "- üîß **MessageBuilder Integration**: Clean, fluent interface for creating requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "üìÅ Working directory: d:\\Users\\bestem\\Code Workspace\\LLMManager\\notebooks\n",
      "üîß MessageBuilder integration enabled for clean request creation!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import random\n",
    "\n",
    "# Add the src directory to path for imports\n",
    "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Import ParallelLLMManager and related classes\n",
    "from bestehorn_llmmanager import ParallelLLMManager\n",
    "from bestehorn_llmmanager.bedrock.models.parallel_structures import (\n",
    "    BedrockConverseRequest, \n",
    "    ParallelProcessingConfig,\n",
    "    FailureHandlingStrategy,\n",
    "    LoadBalancingStrategy\n",
    ")\n",
    "from bestehorn_llmmanager.bedrock.models.llm_manager_structures import (\n",
    "    AuthConfig, \n",
    "    RetryConfig, \n",
    "    AuthenticationType, \n",
    "    RetryStrategy\n",
    ")\n",
    "from bestehorn_llmmanager.bedrock.exceptions.parallel_exceptions import (\n",
    "    ParallelProcessingError, \n",
    "    ParallelExecutionError, \n",
    "    ParallelConfigurationError\n",
    ")\n",
    "\n",
    "# Import MessageBuilder components\n",
    "from bestehorn_llmmanager import create_user_message, create_assistant_message, create_message\n",
    "from bestehorn_llmmanager import RolesEnum, ImageFormatEnum, DocumentFormatEnum, VideoFormatEnum\n",
    "\n",
    "# Configure logging for better visibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(\"üîß MessageBuilder integration enabled for clean request creation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request Creation Functions\n",
    "\n",
    "Using the MessageBuilder system to create requests cleanly and avoid serialization bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MessageBuilder-based helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "def create_text_request_with_builder(text: str, inference_config: Dict[str, Any] = None) -> BedrockConverseRequest:\n",
    "    \"\"\"Create a text-based BedrockConverseRequest using MessageBuilder.\"\"\"\n",
    "    message = create_user_message() \\\n",
    "        .add_text(text) \\\n",
    "        .build()\n",
    "    \n",
    "    return BedrockConverseRequest(\n",
    "        messages=[message],\n",
    "        inference_config=inference_config or {\"maxTokens\": 500, \"temperature\": 0.7}\n",
    "    )\n",
    "\n",
    "def create_image_request_with_builder(text: str, image_path: str, inference_config: Dict[str, Any] = None) -> BedrockConverseRequest:\n",
    "    \"\"\"Create an image analysis BedrockConverseRequest using MessageBuilder.\"\"\"\n",
    "    try:\n",
    "        message = create_user_message() \\\n",
    "            .add_text(text) \\\n",
    "            .add_local_image(path_to_local_file=image_path, max_size_mb=5.0) \\\n",
    "            .build()\n",
    "        \n",
    "        return BedrockConverseRequest(\n",
    "            messages=[message],\n",
    "            inference_config=inference_config or {\"maxTokens\": 800, \"temperature\": 0.3}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating image request: {e}\")\n",
    "        # Fall back to text-only request\n",
    "        return create_text_request_with_builder(f\"{text} (Note: Image could not be loaded from {image_path})\")\n",
    "\n",
    "def display_parallel_response(response, title=\"Parallel Response\"):\n",
    "    \"\"\"Display a formatted parallel response.\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * len(title))\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"‚úÖ Overall Success: {response.success}\")\n",
    "    print(f\"üìä Success Rate: {response.get_success_rate():.1f}%\")\n",
    "    print(f\"‚è±Ô∏è  Total Duration: {response.total_duration_ms:.2f}ms\")\n",
    "    print(f\"üî¢ Total Requests: {len(response.request_responses)}\")\n",
    "    print(f\"‚úÖ Successful Requests: {len(response.get_successful_responses())}\")\n",
    "    print(f\"‚ùå Failed Requests: {len(response.get_failed_responses())}\")\n",
    "    \n",
    "    # Execution statistics\n",
    "    if response.parallel_execution_stats:\n",
    "        stats = response.parallel_execution_stats\n",
    "        print(f\"\\nüìà Execution Statistics:\")\n",
    "        print(f\"   ‚ö° Avg Duration: {stats.average_request_duration_ms:.2f}ms\")\n",
    "        print(f\"   üìä Max Duration: {stats.max_request_duration_ms:.2f}ms\")\n",
    "        print(f\"   ‚ö° Min Duration: {stats.min_request_duration_ms:.2f}ms\")\n",
    "        print(f\"   üîÄ Concurrent Executions: {stats.concurrent_executions}\")\n",
    "        \n",
    "        if stats.region_distribution:\n",
    "            print(f\"   üåç Region Distribution:\")\n",
    "            for region, count in stats.region_distribution.items():\n",
    "                print(f\"      {region}: {count} requests\")\n",
    "    \n",
    "    # Token usage\n",
    "    total_tokens = response.get_total_tokens_used()\n",
    "    if any(total_tokens.values()):\n",
    "        print(f\"\\nüéØ Total Token Usage:\")\n",
    "        for key, value in total_tokens.items():\n",
    "            if value > 0:\n",
    "                print(f\"   {key}: {value:,}\")\n",
    "    \n",
    "    # Average latency\n",
    "    avg_latency = response.get_average_latency()\n",
    "    if avg_latency:\n",
    "        print(f\"\\n‚ö° Average API Latency: {avg_latency:.2f}ms\")\n",
    "    \n",
    "    # Warnings\n",
    "    if response.warnings:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warnings ({len(response.warnings)}):\")\n",
    "        for warning in response.warnings[:3]:  # Show first 3 warnings\n",
    "            print(f\"   - {warning}\")\n",
    "        if len(response.warnings) > 3:\n",
    "            print(f\"   ... and {len(response.warnings) - 3} more\")\n",
    "\n",
    "def display_individual_responses(response, max_responses=3):\n",
    "    \"\"\"Display individual responses from the parallel execution.\"\"\"\n",
    "    successful_responses = response.get_successful_responses()\n",
    "    \n",
    "    print(f\"\\nüí¨ Sample Individual Responses (showing up to {max_responses}):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    count = 0\n",
    "    for req_id, bedrock_response in successful_responses.items():\n",
    "        if count >= max_responses:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nüÜî Request ID: {req_id}\")\n",
    "        print(f\"ü§ñ Model: {bedrock_response.model_used}\")\n",
    "        print(f\"üåç Region: {bedrock_response.region_used}\")\n",
    "        print(f\"‚è±Ô∏è  Duration: {bedrock_response.total_duration_ms:.2f}ms\")\n",
    "        \n",
    "        content = bedrock_response.get_content()\n",
    "        if content:\n",
    "            # Truncate long responses\n",
    "            if len(content) > 200:\n",
    "                content = content[:200] + \"...\"\n",
    "            print(f\"üí≠ Response: {content}\")\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    if len(successful_responses) > max_responses:\n",
    "        print(f\"\\n... and {len(successful_responses) - max_responses} more successful responses\")\n",
    "\n",
    "print(\"‚úÖ MessageBuilder-based helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the ParallelLLMManager\n",
    "\n",
    "We'll create a ParallelLLMManager instance with multiple models and regions for optimal parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:56:18,886 - bestehorn_llmmanager.llm_manager - WARNING - The 'force_download' parameter is deprecated. Using force_refresh=True with BedrockModelCatalog instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing ParallelLLMManager...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:56:30,434 - bestehorn_llmmanager.bedrock.catalog.api_fetcher - WARNING - Region ap-southeast-3 failed: Unexpected error fetching foundation models: Failed to create authenticated session: Failed to create session: An error occurred (InvalidClientTokenId) when calling the GetCallerIdentity operation: The security token included in the request is invalid. Details: {'auth_type': 'profile', 'region': 'ap-southeast-3'}. Details: {'auth_type': 'profile', 'region': 'ap-southeast-3'}. Details: {'region': 'ap-southeast-3'}\n",
      "2026-01-05 21:56:34,755 - bestehorn_llmmanager.bedrock.catalog.api_fetcher - WARNING - Region ap-southeast-4 failed: Unexpected error fetching foundation models: Failed to create authenticated session: Failed to create session: An error occurred (InvalidClientTokenId) when calling the GetCallerIdentity operation: The security token included in the request is invalid. Details: {'auth_type': 'profile', 'region': 'ap-southeast-4'}. Details: {'auth_type': 'profile', 'region': 'ap-southeast-4'}. Details: {'region': 'ap-southeast-4'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ParallelLLMManager initialized successfully!\n",
      "   ü§ñ Models: 4\n",
      "   üåç Regions: 3\n",
      "   üîÄ Max Concurrent: 8\n",
      "   ‚öñÔ∏è  Load Balancing: round_robin\n",
      "\n",
      "ParallelLLMManager(models=4, regions=3, max_concurrent=8, strategy=round_robin)\n",
      "\n",
      "üîç Configuration Validation:\n",
      "   Valid: ‚úÖ True\n",
      "   Auth Status: profile\n",
      "   Model-Region Combinations: 6\n",
      "   Parallel Config Valid: ‚úÖ\n",
      "   ‚ö†Ô∏è  Warnings: 1\n",
      "      - High concurrency (8) compared to available regions (3)\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Initializing ParallelLLMManager...\")\n",
    "\n",
    "# Define models for parallel processing\n",
    "models = [\n",
    "    \"Claude Sonnet 4.5\",\n",
    "    \"Claude Haiku 4.5\",\n",
    "    \"Nova Pro\",             # Multimodal capabilities\n",
    "    \"Nova Lite\",            # Backup option\n",
    "]\n",
    "\n",
    "# Define regions for load distribution\n",
    "regions = [\n",
    "    \"us-east-1\",\n",
    "    \"us-west-2\", \n",
    "    \"eu-west-1\"\n",
    "]\n",
    "\n",
    "# Configure authentication\n",
    "auth_config = AuthConfig(\n",
    "    auth_type=AuthenticationType.PROFILE,\n",
    "    profile_name=\"default\"\n",
    ")\n",
    "\n",
    "# Configure retry behavior\n",
    "retry_config = RetryConfig(\n",
    "    max_retries=2,\n",
    "    retry_strategy=RetryStrategy.REGION_FIRST\n",
    ")\n",
    "\n",
    "# Configure parallel processing behavior\n",
    "parallel_config = ParallelProcessingConfig(\n",
    "    max_concurrent_requests=8,\n",
    "    request_timeout_seconds=60,\n",
    "    failure_handling_strategy=FailureHandlingStrategy.CONTINUE_ON_FAILURE,\n",
    "    load_balancing_strategy=LoadBalancingStrategy.ROUND_ROBIN,\n",
    "    failure_threshold=0.3  # 30% failure threshold\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Initialize the ParallelLLMManager\n",
    "    parallel_manager = ParallelLLMManager(\n",
    "        models=models,\n",
    "        regions=regions,\n",
    "        auth_config=auth_config,\n",
    "        retry_config=retry_config,\n",
    "        parallel_config=parallel_config,\n",
    "        timeout=30,\n",
    "        force_download=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ ParallelLLMManager initialized successfully!\")\n",
    "    print(f\"   ü§ñ Models: {len(parallel_manager.get_available_models())}\")\n",
    "    print(f\"   üåç Regions: {len(parallel_manager.get_available_regions())}\")\n",
    "    print(f\"   üîÄ Max Concurrent: {parallel_config.max_concurrent_requests}\")\n",
    "    print(f\"   ‚öñÔ∏è  Load Balancing: {parallel_config.load_balancing_strategy.value}\")\n",
    "    print(f\"\\n{parallel_manager}\")\n",
    "    \n",
    "    # Validate configuration\n",
    "    validation = parallel_manager.validate_configuration()\n",
    "    print(f\"\\nüîç Configuration Validation:\")\n",
    "    print(f\"   Valid: {'‚úÖ' if validation['valid'] else '‚ùå'} {validation['valid']}\")\n",
    "    print(f\"   Auth Status: {validation['auth_status']}\")\n",
    "    print(f\"   Model-Region Combinations: {validation['model_region_combinations']}\")\n",
    "    print(f\"   Parallel Config Valid: {'‚úÖ' if validation['parallel_config_valid'] else '‚ùå'}\")\n",
    "    \n",
    "    if validation['warnings'] or validation.get('parallel_warnings', []):\n",
    "        all_warnings = validation['warnings'] + validation.get('parallel_warnings', [])\n",
    "        print(f\"   ‚ö†Ô∏è  Warnings: {len(all_warnings)}\")\n",
    "        for warning in all_warnings[:2]:\n",
    "            print(f\"      - {warning}\")\n",
    "    \n",
    "    if validation['errors'] or validation.get('parallel_errors', []):\n",
    "        all_errors = validation['errors'] + validation.get('parallel_errors', [])\n",
    "        print(f\"   ‚ùå Errors: {len(all_errors)}\")\n",
    "        for error in all_errors:\n",
    "            print(f\"      - {error}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing ParallelLLMManager: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting tips:\")\n",
    "    print(\"   1. Ensure AWS credentials are configured\")\n",
    "    print(\"   2. Check access to specified models and regions\")\n",
    "    print(\"   3. Verify network connectivity to AWS\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Parallel Text Processing üí¨\n",
    "\n",
    "Let's start with parallel processing of multiple text requests using the MessageBuilder system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:39:13,803 - bestehorn_llmmanager.parallel_llm_manager - WARNING - target_regions_per_request auto-adjusted to 3 due to limited region availability (available_regions=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Example 1: Basic Parallel Text Processing (MessageBuilder)\n",
      "==========================================================\n",
      "üìù Created 6 text requests using MessageBuilder\n",
      "   1. req_5f6cb0b00185_1766147953802246: Explain quantum computing in simple terms....\n",
      "   2. req_a5f1b5bc179c_1766147953802316: What are the benefits of renewable energy?...\n",
      "   3. req_2e6abd38ef07_1766147953802359: Describe the process of photosynthesis....\n",
      "   4. req_220a8f058e7d_1766147953802397: How does machine learning work?...\n",
      "   5. req_12c3b25b8eaf_1766147953802436: What is the significance of DNA in genetics?...\n",
      "   6. req_391b2dd20932_1766147953802473: Explain the water cycle....\n",
      "\n",
      "üöÄ Executing 6 requests in parallel...\n",
      "‚úÖ Parallel execution completed in 16.89 seconds\n",
      "‚ö° Estimated sequential time: 18.0 seconds\n",
      "üöÄ Speed improvement: ~1.1x faster\n",
      "\n",
      "üó£Ô∏è  Parallel Text Processing Results\n",
      "====================================\n",
      "‚úÖ Overall Success: True\n",
      "üìä Success Rate: 100.0%\n",
      "‚è±Ô∏è  Total Duration: 16891.67ms\n",
      "üî¢ Total Requests: 6\n",
      "‚úÖ Successful Requests: 6\n",
      "‚ùå Failed Requests: 0\n",
      "\n",
      "üìà Execution Statistics:\n",
      "   ‚ö° Avg Duration: 12057.59ms\n",
      "   üìä Max Duration: 16641.22ms\n",
      "   ‚ö° Min Duration: 8489.58ms\n",
      "   üîÄ Concurrent Executions: 6\n",
      "   üåç Region Distribution:\n",
      "      us-east-1: 6 requests\n",
      "      us-west-2: 6 requests\n",
      "      eu-west-1: 6 requests\n",
      "\n",
      "üéØ Total Token Usage:\n",
      "   input_tokens: 41\n",
      "   output_tokens: 2,758\n",
      "   total_tokens: 2,799\n",
      "\n",
      "‚ö° Average API Latency: 3890.17ms\n",
      "\n",
      "üí¨ Sample Individual Responses (showing up to 2):\n",
      "--------------------------------------------------\n",
      "\n",
      "üÜî Request ID: req_391b2dd20932_1766147953802473\n",
      "ü§ñ Model: Nova Pro\n",
      "üåç Region: us-east-1\n",
      "‚è±Ô∏è  Duration: 8489.58ms\n",
      "üí≠ Response: Certainly! The water cycle, also known as the hydrologic cycle, is the continuous process by which water is circulated throughout the Earth's atmosphere, oceans, rivers, and land. It involves several ...\n",
      "\n",
      "üÜî Request ID: req_12c3b25b8eaf_1766147953802436\n",
      "ü§ñ Model: Nova Pro\n",
      "üåç Region: us-east-1\n",
      "‚è±Ô∏è  Duration: 10617.55ms\n",
      "üí≠ Response: DNA (Deoxyribonucleic Acid) is a molecule that carries the genetic instructions used in the growth, development, functioning, and reproduction of all known living organisms and many viruses. The signi...\n",
      "\n",
      "... and 4 more successful responses\n"
     ]
    }
   ],
   "source": [
    "print(\"üí¨ Example 1: Basic Parallel Text Processing (MessageBuilder)\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "# Create multiple text requests using MessageBuilder\n",
    "text_prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"What are the benefits of renewable energy?\",\n",
    "    \"Describe the process of photosynthesis.\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"What is the significance of DNA in genetics?\",\n",
    "    \"Explain the water cycle.\"\n",
    "]\n",
    "\n",
    "# Create BedrockConverseRequest objects using MessageBuilder\n",
    "text_requests = []\n",
    "for prompt in text_prompts:\n",
    "    request = create_text_request_with_builder(text=prompt)\n",
    "    text_requests.append(request)\n",
    "\n",
    "print(f\"üìù Created {len(text_requests)} text requests using MessageBuilder\")\n",
    "for i, req in enumerate(text_requests, 1):\n",
    "    print(f\"   {i}. {req.request_id}: {text_prompts[i-1][:50]}...\")\n",
    "\n",
    "try:\n",
    "    # Execute requests in parallel\n",
    "    print(f\"\\nüöÄ Executing {len(text_requests)} requests in parallel...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    parallel_response = parallel_manager.converse_parallel(\n",
    "        requests=text_requests\n",
    "        # Note: target_regions_per_request not specified - will auto-adjust based on \n",
    "        # available regions (3) and max_concurrent_requests (8), using min(8,3) = 3\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    sequential_estimate = len(text_requests) * 3  # Rough estimate: 3 seconds per request\n",
    "    \n",
    "    print(f\"‚úÖ Parallel execution completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"‚ö° Estimated sequential time: {sequential_estimate:.1f} seconds\")\n",
    "    print(f\"üöÄ Speed improvement: ~{sequential_estimate / (end_time - start_time):.1f}x faster\")\n",
    "    \n",
    "    # Display results\n",
    "    display_parallel_response(parallel_response, \"üó£Ô∏è  Parallel Text Processing Results\")\n",
    "    display_individual_responses(parallel_response, max_responses=2)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in parallel text processing: {e}\")\n",
    "    print(f\"   Type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Parallel Image Analysis üñºÔ∏è\n",
    "\n",
    "Now let's demonstrate parallel processing of image analysis requests using MessageBuilder's local image functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üñºÔ∏è  Example 2: Parallel Image Analysis (MessageBuilder)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define image analysis tasks\n",
    "image_tasks = [\n",
    "    {\n",
    "        \"text\": \"Analyze the architectural features and historical significance of this landmark.\",\n",
    "        \"image_path\": \"../images/1200px-Tour_Eiffel_Wikimedia_Commons_(cropped).jpg\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Describe the structural design and engineering aspects of this tower.\",\n",
    "        \"image_path\": \"../images/Tokyo_Tower_2023.jpg\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"What can you tell me about the lighting and atmospheric conditions in this image?\",\n",
    "        \"image_path\": \"../images/1200px-Tour_Eiffel_Wikimedia_Commons_(cropped).jpg\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Compare this tower's design to other famous towers around the world.\",\n",
    "        \"image_path\": \"../images/Tokyo_Tower_2023.jpg\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create image analysis requests using MessageBuilder\n",
    "image_requests = []\n",
    "for task in image_tasks:\n",
    "    request = create_image_request_with_builder(\n",
    "        text=task[\"text\"],\n",
    "        image_path=task[\"image_path\"]\n",
    "    )\n",
    "    image_requests.append(request)\n",
    "\n",
    "print(f\"üñºÔ∏è  Created {len(image_requests)} image analysis requests using MessageBuilder\")\n",
    "for i, task in enumerate(image_tasks, 1):\n",
    "    image_name = Path(task[\"image_path\"]).name\n",
    "    print(f\"   {i}. {image_name}: {task['text'][:40]}...\")\n",
    "\n",
    "try:\n",
    "    # Execute image analysis in parallel\n",
    "    print(f\"\\nüöÄ Executing {len(image_requests)} image analysis requests in parallel...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    image_response = parallel_manager.converse_parallel(\n",
    "        requests=image_requests,\n",
    "        target_regions_per_request=3  # Use more regions for vision models\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"‚úÖ Parallel image analysis completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Display results\n",
    "    display_parallel_response(image_response, \"üñºÔ∏è  Parallel Image Analysis Results\")\n",
    "    display_individual_responses(image_response, max_responses=2)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in parallel image analysis: {e}\")\n",
    "    print(f\"   Type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Performance Benchmarking üìä\n",
    "\n",
    "Let's benchmark the performance improvements of parallel processing versus sequential execution using MessageBuilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Example 3: Performance Benchmarking (MessageBuilder)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create benchmark requests using MessageBuilder\n",
    "benchmark_prompts = [\n",
    "    \"Summarize the history of computer science.\",\n",
    "    \"Explain the principles of quantum mechanics.\",\n",
    "    \"Describe the process of cellular respiration.\",\n",
    "    \"What are the major causes of climate change?\",\n",
    "    \"How do neural networks learn?\",\n",
    "    \"Explain the theory of evolution.\",\n",
    "    \"What is the importance of biodiversity?\",\n",
    "    \"How does the internet work?\"\n",
    "]\n",
    "\n",
    "benchmark_requests = []\n",
    "for prompt in benchmark_prompts:\n",
    "    request = create_text_request_with_builder(\n",
    "        text=prompt,\n",
    "        inference_config={\"maxTokens\": 400, \"temperature\": 0.6}\n",
    "    )\n",
    "    benchmark_requests.append(request)\n",
    "\n",
    "print(f\"üìù Created {len(benchmark_requests)} benchmark requests using MessageBuilder\")\n",
    "\n",
    "# Test 1: Sequential execution using single requests\n",
    "print(\"\\nüêå Sequential Execution Test...\")\n",
    "sequential_start = time.time()\n",
    "sequential_responses = []\n",
    "sequential_success_count = 0\n",
    "\n",
    "try:\n",
    "    for i, request in enumerate(benchmark_requests[:4], 1):  # Limit to 4 for time\n",
    "        print(f\"   Processing request {i}/4...\")\n",
    "        response = parallel_manager.converse_with_request(request)\n",
    "        sequential_responses.append(response)\n",
    "        if response.success:\n",
    "            sequential_success_count += 1\n",
    "    \n",
    "    sequential_end = time.time()\n",
    "    sequential_duration = sequential_end - sequential_start\n",
    "    \n",
    "    print(f\"‚úÖ Sequential execution completed in {sequential_duration:.2f} seconds\")\n",
    "    print(f\"   Success rate: {(sequential_success_count/4)*100:.1f}%\")\n",
    "    print(f\"   Average per request: {sequential_duration/4:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in sequential execution: {e}\")\n",
    "    sequential_duration = float('inf')  # Set high value for comparison\n",
    "\n",
    "# Test 2: Parallel execution\n",
    "print(\"\\nüöÄ Parallel Execution Test...\")\n",
    "parallel_start = time.time()\n",
    "\n",
    "try:\n",
    "    parallel_response = parallel_manager.converse_parallel(\n",
    "        requests=benchmark_requests[:4],  # Same 4 requests for fair comparison\n",
    "        target_regions_per_request=2\n",
    "    )\n",
    "    \n",
    "    parallel_end = time.time()\n",
    "    parallel_duration = parallel_end - parallel_start\n",
    "    \n",
    "    print(f\"‚úÖ Parallel execution completed in {parallel_duration:.2f} seconds\")\n",
    "    print(f\"   Success rate: {parallel_response.get_success_rate():.1f}%\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    if sequential_duration != float('inf'):\n",
    "        speedup = sequential_duration / parallel_duration\n",
    "        efficiency = (speedup / parallel_config.max_concurrent_requests) * 100\n",
    "        \n",
    "        print(f\"\\nüìà Performance Comparison:\")\n",
    "        print(f\"   Sequential: {sequential_duration:.2f}s\")\n",
    "        print(f\"   Parallel: {parallel_duration:.2f}s\")\n",
    "        print(f\"   Speedup: {speedup:.2f}x\")\n",
    "        print(f\"   Parallel Efficiency: {efficiency:.1f}%\")\n",
    "        print(f\"   Time Saved: {sequential_duration - parallel_duration:.2f}s ({((sequential_duration - parallel_duration)/sequential_duration)*100:.1f}%)\")\n",
    "    \n",
    "    # Display detailed parallel statistics\n",
    "    display_parallel_response(parallel_response, \"üìä Benchmark Results\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in parallel execution: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
