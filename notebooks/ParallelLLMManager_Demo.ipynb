# ParallelLLMManager Comprehensive Demo



This notebook demonstrates the **ParallelLLMManager** - an advanced solution for parallel processing of AWS Bedrock requests across multiple models and regions. We'll showcase concurrent execution, load balancing, fault tolerance, and performance optimization using the modern **ConverseMessageBuilder** for request creation.



## Key Features



- 🚀 **Parallel Processing**: Execute multiple requests concurrently across regions

- 🌍 **Multi-Region Support**: Intelligent load distribution across AWS regions  

- 🔄 **Load Balancing**: Round-robin, random, and least-loaded strategies

- 🛡️ **Fault Tolerance**: Multiple failure handling strategies

- 📊 **Performance Analytics**: Detailed execution statistics and metrics

- 🖼️ **Multimodal Support**: Parallel processing of text, image, and video content

- ⚙️ **Flexible Configuration**: Customizable concurrency and timeout settings

- 🔧 **Modern Message Building**: Uses ConverseMessageBuilder for fluent request construction
## Setup and Imports
import sys

import json

import time

from pathlib import Path

import logging

from datetime import datetime

from typing import List, Dict, Any

import random



# Add the src directory to path for imports

sys.path.append(str(Path.cwd().parent / "src"))



# Import ParallelLLMManager and related classes

from bedrock.ParallelLLMManager import ParallelLLMManager

from bedrock.models.parallel_structures import (

    BedrockConverseRequest, 

    ParallelProcessingConfig,

    FailureHandlingStrategy,

    LoadBalancingStrategy

)

from bedrock.models.llm_manager_structures import (

    AuthConfig, 

    RetryConfig, 

    AuthenticationType, 

    RetryStrategy

)

from bedrock.exceptions.parallel_exceptions import (

    ParallelProcessingError, 

    ParallelExecutionError, 

    ParallelConfigurationError

)



# Import ConverseMessageBuilder and factory functions

from bedrock.models.message_builder_factory import (

    create_user_message, 

    create_assistant_message,

    create_message

)

from bedrock.models.message_builder_enums import (

    RolesEnum, 

    ImageFormatEnum, 

    DocumentFormatEnum,

    VideoFormatEnum

)



# Configure logging for better visibility

logging.basicConfig(

    level=logging.INFO,

    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'

)



print("✅ Imports successful!")

print(f"📁 Working directory: {Path.cwd()}")

print("🔧 Using modern ConverseMessageBuilder for request creation")
## Display Helper Functions



Let's define utility functions for displaying results (we no longer need helper functions for creating requests thanks to ConverseMessageBuilder).
def display_parallel_response(response, title="Parallel Response"):

    """Display a formatted parallel response."""

    print(f"\n{title}")

    print("=" * len(title))

    

    # Overall statistics

    print(f"✅ Overall Success: {response.success}")

    print(f"📊 Success Rate: {response.get_success_rate():.1f}%")

    print(f"⏱️  Total Duration: {response.total_duration_ms:.2f}ms")

    print(f"🔢 Total Requests: {len(response.request_responses)}")

    print(f"✅ Successful Requests: {len(response.get_successful_responses())}")

    print(f"❌ Failed Requests: {len(response.get_failed_responses())}")

    

    # Execution statistics

    if response.parallel_execution_stats:

        stats = response.parallel_execution_stats

        print(f"\n📈 Execution Statistics:")

        print(f"   ⚡ Avg Duration: {stats.average_request_duration_ms:.2f}ms")

        print(f"   📊 Max Duration: {stats.max_request_duration_ms:.2f}ms")

        print(f"   ⚡ Min Duration: {stats.min_request_duration_ms:.2f}ms")

        print(f"   🔀 Concurrent Executions: {stats.concurrent_executions}")

        

        if stats.region_distribution:

            print(f"   🌍 Region Distribution:")

            for region, count in stats.region_distribution.items():

                print(f"      {region}: {count} requests")

    

    # Token usage

    total_tokens = response.get_total_tokens_used()

    if any(total_tokens.values()):

        print(f"\n🎯 Total Token Usage:")

        for key, value in total_tokens.items():

            if value > 0:

                print(f"   {key}: {value:,}")

    

    # Average latency

    avg_latency = response.get_average_latency()

    if avg_latency:

        print(f"\n⚡ Average API Latency: {avg_latency:.2f}ms")

    

    # Warnings

    if response.warnings:

        print(f"\n⚠️  Warnings ({len(response.warnings)}):")

        for warning in response.warnings[:3]:  # Show first 3 warnings

            print(f"   - {warning}")

        if len(response.warnings) > 3:

            print(f"   ... and {len(response.warnings) - 3} more")



def display_individual_responses(response, max_responses=3):

    """Display individual responses from the parallel execution."""

    successful_responses = response.get_successful_responses()

    

    print(f"\n💬 Sample Individual Responses (showing up to {max_responses}):")

    print("-" * 50)

    

    count = 0

    for req_id, bedrock_response in successful_responses.items():

        if count >= max_responses:

            break

            

        print(f"\n🆔 Request ID: {req_id}")

        print(f"🤖 Model: {bedrock_response.model_used}")

        print(f"🌍 Region: {bedrock_response.region_used}")

        print(f"⏱️  Duration: {bedrock_response.total_duration_ms:.2f}ms")

        

        content = bedrock_response.get_content()

        if content:

            # Truncate long responses

            if len(content) > 200:

                content = content[:200] + "..."

            print(f"💭 Response: {content}")

        

        count += 1

    

    if len(successful_responses) > max_responses:

        print(f"\n... and {len(successful_responses) - max_responses} more successful responses")



def display_message_builder_demo():

    """Demonstrate ConverseMessageBuilder capabilities."""

    print("\n🔧 ConverseMessageBuilder Capabilities:")

    print("   📝 Fluent interface for message creation")

    print("   🎯 Automatic format detection for images/documents/videos")

    print("   📏 Built-in size validation and limits")

    print("   🏗️ Factory functions for convenience")

    print("   🔗 Method chaining for readability")

    print("   ✅ Type-safe enums for formats and roles")



print("✅ Display helper functions defined!")

display_message_builder_demo()
## Initialize the ParallelLLMManager



We'll create a ParallelLLMManager instance with multiple models and regions for optimal parallel processing.
print("🚀 Initializing ParallelLLMManager...")



# Define models for parallel processing

models = [

    "Claude 3.5 Sonnet v2",  # High-quality responses

    "Claude 3 Haiku",       # Fast processing

    "Nova Pro",             # Multimodal capabilities

    "Nova Lite"             # Backup option

]



# Define regions for load distribution

regions = [

    "us-east-1",

    "us-west-2", 

    "eu-west-1"

]



# Configure authentication

auth_config = AuthConfig(

    auth_type=AuthenticationType.PROFILE,

    profile_name="default"

)



# Configure retry behavior

retry_config = RetryConfig(

    max_retries=2,

    retry_strategy=RetryStrategy.REGION_FIRST

)



# Configure parallel processing behavior

parallel_config = ParallelProcessingConfig(

    max_concurrent_requests=8,

    request_timeout_seconds=60,

    failure_handling_strategy=FailureHandlingStrategy.CONTINUE_ON_FAILURE,

    load_balancing_strategy=LoadBalancingStrategy.ROUND_ROBIN,

    failure_threshold=0.3  # 30% failure threshold

)



try:

    # Initialize the ParallelLLMManager

    parallel_manager = ParallelLLMManager(

        models=models,

        regions=regions,

        auth_config=auth_config,

        retry_config=retry_config,

        parallel_config=parallel_config,

        timeout=30

    )

    

    print(f"✅ ParallelLLMManager initialized successfully!")

    print(f"   🤖 Models: {len(parallel_manager.get_available_models())}")

    print(f"   🌍 Regions: {len(parallel_manager.get_available_regions())}")

    print(f"   🔀 Max Concurrent: {parallel_config.max_concurrent_requests}")

    print(f"   ⚖️  Load Balancing: {parallel_config.load_balancing_strategy.value}")

    print(f"\n{parallel_manager}")

    

    # Validate configuration

    validation = parallel_manager.validate_configuration()

    print(f"\n🔍 Configuration Validation:")

    print(f"   Valid: {'✅' if validation['valid'] else '❌'} {validation['valid']}")

    print(f"   Auth Status: {validation['auth_status']}")

    print(f"   Model-Region Combinations: {validation['model_region_combinations']}")

    print(f"   Parallel Config Valid: {'✅' if validation['parallel_config_valid'] else '❌'}")

    

    if validation['warnings'] or validation.get('parallel_warnings', []):

        all_warnings = validation['warnings'] + validation.get('parallel_warnings', [])

        print(f"   ⚠️  Warnings: {len(all_warnings)}")

        for warning in all_warnings[:2]:

            print(f"      - {warning}")

    

    if validation['errors'] or validation.get('parallel_errors', []):

        all_errors = validation['errors'] + validation.get('parallel_errors', [])

        print(f"   ❌ Errors: {len(all_errors)}")

        for error in all_errors:

            print(f"      - {error}")



except Exception as e:

    print(f"❌ Error initializing ParallelLLMManager: {e}")

    print("\n💡 Troubleshooting tips:")

    print("   1. Ensure AWS credentials are configured")

    print("   2. Check access to specified models and regions")

    print("   3. Verify network connectivity to AWS")

    raise
## Example 1: Basic Parallel Text Processing with ConverseMessageBuilder 💬



Let's start with parallel processing of multiple text requests using the modern ConverseMessageBuilder approach.
print("💬 Example 1: Basic Parallel Text Processing with ConverseMessageBuilder")

print("=" * 70)



# Create multiple text requests using ConverseMessageBuilder

text_prompts = [

    "Explain quantum computing in simple terms.",

    "What are the benefits of renewable energy?",

    "Describe the process of photosynthesis.",

    "How does machine learning work?",

    "What is the significance of DNA in genetics?",

    "Explain the water cycle."

]



# Create BedrockConverseRequest objects using the fluent interface

text_requests = []

for prompt in text_prompts:

    # Build message using ConverseMessageBuilder

    message = create_user_message() \

        .add_text(text=prompt) \

        .build()

    

    # Create the parallel request

    request = BedrockConverseRequest(

        messages=[message],

        inference_config={

            "maxTokens": 400,

            "temperature": 0.7

        }

    )

    text_requests.append(request)



print(f"📝 Created {len(text_requests)} text requests using ConverseMessageBuilder")

print("🔧 Each request demonstrates:")

print("   - Fluent interface: create_user_message().add_text().build()")

print("   - Automatic validation and formatting")

print("   - Type-safe message construction")



for i, req in enumerate(text_requests, 1):

    print(f"   {i}. {req.request_id}: {text_prompts[i-1][:50]}...")



try:

    # Execute requests in parallel

    print(f"\n🚀 Executing {len(text_requests)} requests in parallel...")

    start_time = time.time()

    

    parallel_response = parallel_manager.converse_parallel(

        requests=text_requests,

        target_regions_per_request=2  # Each request can use up to 2 regions

    )

    

    end_time = time.time()

    sequential_estimate = len(text_requests) * 3  # Rough estimate: 3 seconds per request

    

    print(f"✅ Parallel execution completed in {end_time - start_time:.2f} seconds")

    print(f"⚡ Estimated sequential time: {sequential_estimate:.1f} seconds")

    print(f"🚀 Speed improvement: ~{sequential_estimate / (end_time - start_time):.1f}x faster")

    

    # Display results

    display_parallel_response(parallel_response, "🗣️  Parallel Text Processing Results")

    display_individual_responses(parallel_response, max_responses=2)



except Exception as e:

    print(f"❌ Error in parallel text processing: {e}")

    print(f"   Type: {type(e).__name__}")
## Example 2: Advanced ConverseMessageBuilder Features 🔧



Let's demonstrate advanced message building capabilities including multimodal content and different approaches.
print("🔧 Example 2: Advanced ConverseMessageBuilder Features")

print("=" * 52)



# Demonstrate different ways to create messages

print("🎯 Demonstrating Various Message Creation Approaches:")

print()



# 1. Simple text message

simple_message = create_user_message() \

    .add_text(text="What is artificial intelligence?") \

    .build()



print("1️⃣ Simple Text Message:")

print(f"   Role: {simple_message['role']}")

print(f"   Content blocks: {len(simple_message['content'])}")

print(f"   Text preview: {simple_message['content'][0]['text'][:40]}...")

print()



# 2. Multi-part text message  

multi_text_message = create_user_message() \

    .add_text(text="Please analyze the following:") \

    .add_text(text="1. Current market trends") \

    .add_text(text="2. Future predictions") \

    .build()



print("2️⃣ Multi-part Text Message:")

print(f"   Role: {multi_text_message['role']}")

print(f"   Content blocks: {len(multi_text_message['content'])}")

for i, block in enumerate(multi_text_message['content'], 1):

    print(f"   Block {i}: {block['text']}")

print()



# 3. Create requests for complex analysis tasks

complex_requests = []



# Request 1: Simple analysis

simple_request = BedrockConverseRequest(

    messages=[simple_message],

    inference_config={"maxTokens": 300, "temperature": 0.5}

)

complex_requests.append(simple_request)



# Request 2: Multi-part analysis

multi_request = BedrockConverseRequest(

    messages=[multi_text_message],

    inference_config={"maxTokens": 500, "temperature": 0.6}

)

complex_requests.append(multi_request)



# Request 3: Conversational context

conversation_messages = [

    create_user_message()

        .add_text(text="What are the key principles of machine learning?")

        .build(),

    

    create_assistant_message()

        .add_text(text="Machine learning involves algorithms that learn from data to make predictions or decisions.")

        .build(),

    

    create_user_message()

        .add_text(text="Can you give me a specific example of supervised learning?")

        .build()

]



conversation_request = BedrockConverseRequest(

    messages=conversation_messages,

    inference_config={"maxTokens": 400, "temperature": 0.4}

)

complex_requests.append(conversation_request)



print("3️⃣ Created Complex Request Types:")

print(f"   📝 Simple analysis request")

print(f"   🔢 Multi-part analysis request") 

print(f"   💬 Conversational context request (3 messages)")

print(f"   🔢 Total requests: {len(complex_requests)}")



# Execute the complex requests

try:

    print(f"\n🚀 Executing advanced message types in parallel...")

    start_time = time.time()

    

    complex_response = parallel_manager.converse_parallel(

        requests=complex_requests,

        target_regions_per_request=2

    )

    

    end_time = time.time()

    

    print(f"✅ Advanced message processing completed in {end_time - start_time:.2f} seconds")

    

    # Display results

    display_parallel_response(complex_response, "🔧 Advanced ConverseMessageBuilder Results")

    display_individual_responses(complex_response, max_responses=2)



except Exception as e:

    print(f"❌ Error in advanced message processing: {e}")

    print(f"   Type: {type(e).__name__}")
## Example 3: Multimodal Content with Auto-Detection 🖼️



Now let's demonstrate the power of ConverseMessageBuilder for multimodal content with automatic format detection.
print("🖼️  Example 3: Multimodal Content with Auto-Detection")

print("=" * 52)



# Check if image files exist

eiffel_image_path = "../images/1200px-Tour_Eiffel_Wikimedia_Commons_(cropped).jpg"

tokyo_image_path = "../images/Tokyo_Tower_2023.jpg"



eiffel_exists = Path(eiffel_image_path).exists()

tokyo_exists = Path(tokyo_image_path).exists()



print(f"📁 Image File Availability:")

print(f"   Eiffel Tower: {'✅' if eiffel_exists else '❌'} {eiffel_image_path}")

print(f"   Tokyo Tower: {'✅' if tokyo_exists else '❌'} {tokyo_image_path}")



if not (eiffel_exists and tokyo_exists):

    print("\n⚠️  Some image files not found. Demonstrating text-only multimodal requests.")



# Create multimodal requests using ConverseMessageBuilder

multimodal_requests = []



# Request 1: Text + Image analysis (if images exist)

if eiffel_exists:

    try:

        eiffel_message = create_user_message() \

            .add_text(text="Please analyze the architectural features and historical significance of this landmark.") \

            .add_local_image(path_to_local_file=eiffel_image_path) \

            .build()

        

        eiffel_request = BedrockConverseRequest(

            messages=[eiffel_message],

            inference_config={"maxTokens": 600, "temperature": 0.3}

        )

        multimodal_requests.append(eiffel_request)

        

        print(f"\n✅ Created Eiffel Tower analysis request:")

        print(f"   🖼️  Image loaded with auto-format detection")

        print(f"   📝 Combined with architectural analysis prompt")

        

    except Exception as e:

        print(f"❌ Error creating Eiffel Tower request: {e}")



if tokyo_exists:

    try:

        tokyo_message = create_user_message() \

            .add_text(text="Compare this tower's design to other famous towers around the world.") \

            .add_local_image(path_to_local_file=tokyo_image_path) \

            .build()

        

        tokyo_request = BedrockConverseRequest(

            messages=[tokyo_message],

            inference_config={"maxTokens": 600, "temperature": 0.3}

        )

        multimodal_requests.append(tokyo_request)

        

        print(f"\n✅ Created Tokyo Tower comparison request:")

        print(f"   🖼️  Image loaded with auto-format detection")

        print(f"   🔍 Combined with comparative analysis prompt")

        

    except Exception as e:

        print(f"❌ Error creating Tokyo Tower request: {e}")



# Request 2: Complex multimodal prompt (even without images)

complex_multimodal_message = create_user_message() \

    .add_text(text="I'm interested in understanding modern architecture principles.") \

    .add_text(text="Please explain the key concepts of:") \

    .add_text(text="1. Structural engineering in tall buildings") \

    .add_text(text="2. Aesthetic design considerations") \

    .add_text(text="3. Historical evolution of tower design") \

    .build()



complex_multimodal_request = BedrockConverseRequest(

    messages=[complex_multimodal_message],

    inference_config={"maxTokens": 800, "temperature": 0.4}

)

multimodal_requests.append(complex_multimodal_request)



print(f"\n✅ Created complex multi-part architectural analysis request")

print(f"   📝 Multiple text blocks for structured questioning")

print(f"   🏗️  Demonstrates advanced message composition")



print(f"\n📊 Multimodal Request Summary:")

print(f"   Total requests: {len(multimodal_requests)}")

print(f"   Image-based requests: {sum(1 for _ in [eiffel_exists, tokyo_exists] if _)}")

print(f"   Text-only requests: {len(multimodal_requests) - sum(1 for _ in [eiffel_exists, tokyo_exists] if _)}")



# Execute multimodal requests

if multimodal_requests:

    try:

        print(f"\n🚀 Executing multimodal requests in parallel...")

        start_time = time.time()

        

        multimodal_response = parallel_manager.converse_parallel(

            requests=multimodal_requests,

            target_regions_per_request=2

        )

        

        end_time = time.time()

        

        print(f"✅ Multimodal processing completed in {end_time - start_time:.2f} seconds")

        

        # Display results

        display_parallel_response(multimodal_response, "🖼️  Multimodal Content Processing Results")

        display_individual_responses(multimodal_response, max_responses=2)

        

        print(f"\n🎯 ConverseMessageBuilder Benefits Demonstrated:")

        print(f"   🔍 Automatic image format detection")

        print(f"   📏 Built-in file size validation")

        print(f"   🔗 Fluent interface for complex message composition")

        print(f"   ✅ Type-safe content block creation")

        print(f"   🛡️  Error handling and validation")



    except Exception as e:

        print(f"❌ Error in multimodal processing: {e}")

        print(f"   Type: {type(e).__name__}")

else:

    print("\n⚠️  No multimodal requests created due to missing files or errors.")
## Summary and Best Practices 📋



This comprehensive demo has showcased the powerful capabilities of the modernized ParallelLLMManager system with ConverseMessageBuilder.
print("📋 ParallelLLMManager + ConverseMessageBuilder Demo Summary")

print("=" * 58)



print("\n✅ Key Improvements Demonstrated:")

print("   🔧 Modern ConverseMessageBuilder for request creation")

print("   🚀 Thread-based parallel processing (no asyncio conflicts)")

print("   🌍 Multi-Region Distribution with intelligent load balancing")

print("   🛡️ Robust fault tolerance and error handling")

print("   📊 Comprehensive performance analytics")

print("   🖼️ Seamless multimodal support with auto-detection")

print("   ⚙️ Flexible configuration options")



print("\n🔧 ConverseMessageBuilder Benefits:")

print("   ✅ Fluent interface eliminates manual message construction")

print("   🎯 Automatic format detection for images/documents/videos")

print("   📏 Built-in validation prevents malformed requests")

print("   🏗️ Factory functions for common use cases")

print("   🔗 Method chaining improves code readability")

print("   ✅ Type-safe enums prevent format errors")

print("   🛡️  Comprehensive error handling and validation")



print("\n🚀 Thread-Based Parallel Processing Advantages:")

print("   ✅ Works in all environments (Jupyter, scripts, web apps)")

print("   🔄 No asyncio event loop conflicts")

print("   ⚡ Excellent performance for I/O-bound operations")

print("   🧵 True parallelism with ThreadPoolExecutor")

print("   🛡️  Built-in timeout and resource management")

print("   📊 Comprehensive execution monitoring")



print("\n💡 Best Practices Demonstrated:")

print("   1. 🏗️  Use ConverseMessageBuilder for all request creation")

print("   2. 🎯 Leverage automatic format detection for multimodal content")

print("   3. 🌍 Distribute requests across multiple regions for performance")

print("   4. 🛡️  Choose appropriate failure strategies for your use case")

print("   5. ⚖️  Select load balancing strategies based on workload")

print("   6. 📊 Monitor execution statistics for optimization")

print("   7. ⏱️  Set reasonable timeouts for your specific needs")

print("   8. 🔧 Test different configurations to find optimal settings")



print("\n🚀 Next Steps:")

print("   1. 🔑 Configure AWS credentials and model access")

print("   2. 🎯 Customize models and regions for your use case")

print("   3. ⚙️  Adjust parallel processing configuration")

print("   4. 🏗️  Integrate into your applications using ConverseMessageBuilder")

print("   5. 📊 Monitor and optimize based on your workloads")

print("   6. 🖼️  Explore multimodal capabilities for rich content")



print("\n📚 Key Code Patterns:")

print("   📝 Text Request:")

print("      message = create_user_message().add_text('prompt').build()")

print("   🖼️  Image Request:")

print("      message = create_user_message().add_text('prompt').add_local_image('path').build()")

print("   💬 Conversation:")

print("      messages = [user_msg.build(), assistant_msg.build(), user_msg.build()]")

print("   🚀 Parallel Execution:")

print("      response = manager.converse_parallel(requests, target_regions_per_request=2)")



print("\n🎉 Ready to build high-performance, parallel AI applications!")

print("🚀 ParallelLLMManager + ConverseMessageBuilder = Powerful & Easy!")



# Display final configuration summary

if 'parallel_manager' in locals():

    print(f"\n🔧 Your Current Configuration:")

    print(f"   🤖 Models: {len(parallel_manager.get_available_models())}")

    print(f"   🌍 Regions: {len(parallel_manager.get_available_regions())}")

    config = parallel_manager.get_parallel_config()

    print(f"   🔀 Max Concurrent: {config.max_concurrent_requests}")

    print(f"   ⚖️  Load Balancing: {config.load_balancing_strategy.value}")

    print(f"   🛡️ Failure Strategy: {config.failure_handling_strategy.value}")

    print(f"\n{parallel_manager}")
