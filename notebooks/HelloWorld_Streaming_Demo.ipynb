{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Manager Real Streaming HelloWorld Demonstration\n",
    "\n",
    "This notebook demonstrates the **LLM Manager's new real streaming functionality** using the MessageBuilder and actual AWS Bedrock `converse_stream` API to provide true real-time streaming responses.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- 🚀 **Real Streaming**: Uses actual AWS Bedrock `converse_stream` API with real-time output display\n",
    "- 🔧 **MessageBuilder Integration**: Uses MessageBuilder for all message construction\n",
    "- 📁 **Multi-Modal Support**: Streaming with images, documents, and text content\n",
    "- 🔄 **Stream Recovery**: Intelligent retry with partial content preservation\n",
    "- 📊 **Rich Metadata**: Complete streaming metrics, token usage, and performance data\n",
    "- ⚡ **Real-Time Display**: See content being generated chunk by chunk\n",
    "- 🛡️ **Error Handling**: Comprehensive stream interruption detection and recovery\n",
    "\n",
    "## What's New\n",
    "\n",
    "**Before (Placeholder Implementation):**\n",
    "```python\n",
    "# Used synchronous converse() call internally\n",
    "streaming_response = manager.converse_stream(messages)\n",
    "# Showed only final result\n",
    "```\n",
    "\n",
    "**After (Real Streaming Implementation):**\n",
    "```python\n",
    "# Uses actual AWS Bedrock converse_stream API with EventStream processing\n",
    "streaming_response = manager.converse_stream(messages)\n",
    "# Shows real-time streaming with chunk-by-chunk display\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Iterator, Any\n",
    "\n",
    "# Add the src directory to path for imports\n",
    "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Import the LLMManager and related classes\n",
    "from bestehorn_llmmanager.llm_manager import LLMManager\n",
    "from bestehorn_llmmanager.bedrock.models.llm_manager_structures import AuthConfig, RetryConfig, AuthenticationType, RetryStrategy\n",
    "from bestehorn_llmmanager.bedrock.exceptions.llm_manager_exceptions import LLMManagerError, ConfigurationError, AuthenticationError\n",
    "\n",
    "# Import MessageBuilder components (following established pattern)\n",
    "from bestehorn_llmmanager import create_user_message, create_assistant_message, create_message\n",
    "from bestehorn_llmmanager.message_builder_enums import RolesEnum, ImageFormatEnum, DocumentFormatEnum, VideoFormatEnum\n",
    "\n",
    "# Import streaming components and display utilities\n",
    "from bestehorn_llmmanager.bedrock.models.bedrock_response import StreamingResponse\n",
    "from bestehorn_llmmanager.util.streaming_display import display_streaming_response, display_streaming_summary, display_recovery_information\n",
    "\n",
    "# Configure logging for better visibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "print(\"✅ Imports successful!\")\n",
    "print(f\"📁 Working directory: {Path.cwd()}\")\n",
    "print(\"🚀 Real streaming functionality with MessageBuilder imported and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_file_info(file_path: str, content_type: str = \"file\"):\n",
    "    \"\"\"Display information about a file (following established pattern).\"\"\"\n",
    "    path = Path(file_path)\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"📁 {content_type.title()}: {path.name}\")\n",
    "        print(f\"   📏 Size: {size_mb:.2f} MB ({path.stat().st_size:,} bytes)\")\n",
    "        print(f\"   📍 Path: {path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"❌ {content_type.title()} file not found: {file_path}\")\n",
    "        return False\n",
    "\n",
    "print(\"✅ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the LLMManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Initializing LLMManager...\")\n",
    "\n",
    "# Use known working models (refreshing data if needed)\n",
    "models = [\"Claude 3.5 Sonnet v2\", \"Claude 3 Haiku\", \"Claude 3 Sonnet\"]\n",
    "regions = [\"us-east-1\", \"us-west-2\", \"eu-west-1\"]\n",
    "\n",
    "auth_config = AuthConfig(auth_type=AuthenticationType.PROFILE, profile_name=\"default\")\n",
    "retry_config = RetryConfig(max_retries=3, retry_strategy=RetryStrategy.REGION_FIRST)\n",
    "\n",
    "try:\n",
    "    manager = LLMManager(models=models, regions=regions, auth_config=auth_config, retry_config=retry_config, timeout=30)\n",
    "    print(f\"✅ LLMManager initialized successfully!\")\n",
    "    validation = manager.validate_configuration()\n",
    "    print(f\"   Valid: {'✅' if validation['valid'] else '❌'} {validation['valid']}\")\n",
    "    print(f\"   Model/Region combinations: {validation['model_region_combinations']}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Initial setup failed, refreshing model data...\")\n",
    "    # Refresh model data if needed\n",
    "    from bestehorn_llmmanager.bedrock.UnifiedModelManager import UnifiedModelManager\n",
    "    umm = UnifiedModelManager()\n",
    "    umm.refresh_unified_data()\n",
    "    print(\"✅ Model data refreshed\")\n",
    "    \n",
    "    manager = LLMManager(models=models, regions=regions, auth_config=auth_config, retry_config=retry_config, timeout=30)\n",
    "    print(f\"✅ LLMManager initialized successfully after refresh!\")\n",
    "    validation = manager.validate_configuration()\n",
    "    print(f\"   Valid: {'✅' if validation['valid'] else '❌'} {validation['valid']}\")\n",
    "    print(f\"   Model/Region combinations: {validation['model_region_combinations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Real-Time Streaming with MessageBuilder 🚀\n",
    "\n",
    "Demonstrating real streaming with MessageBuilder for message construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Example 1: Basic Real-Time Streaming with MessageBuilder\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create message using MessageBuilder (following established pattern)\n",
    "message = create_user_message() \\\n",
    "    .add_text(\"Write a short story about a robot learning to paint. Make it about 3 paragraphs and make it engaging.\") \\\n",
    "    .build()\n",
    "\n",
    "print(f\"🔧 Built message using MessageBuilder with {len(message['content'])} content blocks\")\n",
    "print(f\"📝 Prompt: {message['content'][0]['text'][:100]}...\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n🌊 Starting real-time streaming...\")\n",
    "    streaming_response = manager.converse_stream(\n",
    "        messages=[message], \n",
    "        inference_config={\"maxTokens\": 800, \"temperature\": 0.7}\n",
    "    )\n",
    "    \n",
    "    print(\"\\n📺 Real-time streaming output:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Real streaming iteration - content appears as it arrives!\n",
    "    try:\n",
    "        for chunk in streaming_response:\n",
    "            print(chunk, end='', flush=True)  # Real-time display!\n",
    "    except Exception as stream_error:\n",
    "        print(f\"\\n❌ Stream interrupted: {stream_error}\")\n",
    "    \n",
    "    print(f\"\\n{'-' * 50}\")\n",
    "    print(\"✅ Streaming completed!\")\n",
    "    \n",
    "    # Now show model/region info after streaming completes\n",
    "    print(f\"🤖 Model: {streaming_response.model_used}\")\n",
    "    print(f\"🌍 Region: {streaming_response.region_used}\")\n",
    "    \n",
    "    # Show final metadata (available after streaming completes)\n",
    "    print(f\"\\n📊 Streaming Results:\")\n",
    "    print(f\"   Success: {streaming_response.success}\")\n",
    "    print(f\"   Total Duration: {streaming_response.total_duration_ms:.1f}ms\" if streaming_response.total_duration_ms else \"   Duration: N/A\")\n",
    "    print(f\"   Content Parts: {len(streaming_response.content_parts)}\")\n",
    "    print(f\"   Stop Reason: {streaming_response.stop_reason or 'N/A'}\")\n",
    "    \n",
    "    # Token usage (available after completion)\n",
    "    usage = streaming_response.get_usage()\n",
    "    if usage:\n",
    "        print(f\"\\n🎯 Token Usage:\")\n",
    "        print(f\"   Input: {usage.get('input_tokens', 0)}, Output: {usage.get('output_tokens', 0)}, Total: {usage.get('total_tokens', 0)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in basic streaming: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multi-Modal Streaming with Local Image 🖼️\n",
    "\n",
    "Using MessageBuilder with local images for streaming analysis and the new streaming display utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🖼️ Example 2: Multi-Modal Streaming with Local Image\")\n",
    "print(\"=\" * 52)\n",
    "\n",
    "# Use established image path from the existing notebook\n",
    "eiffel_image_path = \"../images/1200px-Tour_Eiffel_Wikimedia_Commons_(cropped).jpg\"\n",
    "\n",
    "if display_file_info(eiffel_image_path, \"image\"):\n",
    "    try:\n",
    "        # Build message using MessageBuilder with local image (following established pattern)\n",
    "        message = create_user_message() \\\n",
    "            .add_text(\"Please analyze this image in detail. Describe the architecture, setting, and notable features. Stream your analysis as you observe different aspects.\") \\\n",
    "            .add_local_image(path_to_local_file=eiffel_image_path, max_size_mb=5.0) \\\n",
    "            .build()\n",
    "        \n",
    "        print(f\"🔧 Built multi-modal message with {len(message['content'])} content blocks using MessageBuilder\")\n",
    "        print(f\"   📸 Image format detected: {message['content'][1]['image']['format']}\")\n",
    "        \n",
    "        print(\"\\n🌊 Starting streaming image analysis...\")\n",
    "        streaming_response = manager.converse_stream(\n",
    "            messages=[message], \n",
    "            inference_config={\"maxTokens\": 1000, \"temperature\": 0.4}\n",
    "        )\n",
    "        \n",
    "        print(\"\\n📺 Real-time streaming output:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Actually consume the stream to trigger API calls\n",
    "        try:\n",
    "            for chunk in streaming_response:\n",
    "                print(chunk, end='', flush=True)  # Real-time display!\n",
    "        except Exception as stream_error:\n",
    "            print(f\"\\n❌ Stream interrupted: {stream_error}\")\n",
    "        \n",
    "        print(f\"\\n{'-' * 50}\")\n",
    "        \n",
    "        # Now use the new streaming display utility after streaming completes\n",
    "        display_streaming_response(\n",
    "            streaming_response=streaming_response,\n",
    "            title=\"🖼️ Image Analysis Streaming Response\",\n",
    "            show_content=False,  # We already showed it in real-time\n",
    "            content_preview_length=200\n",
    "        )\n",
    "        \n",
    "        # Show recovery information if available\n",
    "        display_recovery_information(streaming_response=streaming_response)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in image streaming: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping image example - file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Advanced Streaming with Display Utilities 🔧\n",
    "\n",
    "Demonstrating the new streaming display utilities for comprehensive response analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 Example 3: Advanced Streaming with Display Utilities\")\n",
    "print(\"=\" * 56)\n",
    "\n",
    "# Create a more complex message for detailed analysis\n",
    "message = create_user_message() \\\n",
    "    .add_text(\"Explain the concept of machine learning in detail, including supervised and unsupervised learning, neural networks, and real-world applications. Make it comprehensive but accessible.\") \\\n",
    "    .build()\n",
    "\n",
    "print(f\"🔧 Built comprehensive prompt with {len(message['content'])} content blocks\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n🌊 Starting advanced streaming with display utilities...\")\n",
    "    streaming_response = manager.converse_stream(\n",
    "        messages=[message], \n",
    "        inference_config={\"maxTokens\": 1500, \"temperature\": 0.6}\n",
    "    )\n",
    "    \n",
    "    print(\"\\n📺 Real-time streaming output:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Stream the content in real-time\n",
    "    try:\n",
    "        for chunk in streaming_response:\n",
    "            print(chunk, end='', flush=True)\n",
    "    except Exception as stream_error:\n",
    "        print(f\"\\n❌ Stream interrupted: {stream_error}\")\n",
    "    \n",
    "    print(f\"\\n{'-' * 50}\")\n",
    "    \n",
    "    # Use the comprehensive display utility\n",
    "    display_streaming_response(\n",
    "        streaming_response=streaming_response,\n",
    "        title=\"🔧 Advanced Streaming Response Analysis\",\n",
    "        show_content=False,  # Already displayed in real-time\n",
    "        show_metadata=True,\n",
    "        show_timing=True,\n",
    "        show_usage=True,\n",
    "        show_errors=True\n",
    "    )\n",
    "    \n",
    "    # Show a concise summary\n",
    "    display_streaming_summary(\n",
    "        streaming_response=streaming_response,\n",
    "        title=\"📊 Streaming Performance Summary\"\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in advanced streaming: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
