{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Inference Profile Support Demonstration\n",
                "\n",
                "This notebook demonstrates the **automatic inference profile support** in LLMManager. Starting in v0.4.0, LLMManager automatically detects when models require inference profiles (CRIS profiles) and handles them transparently.\n",
                "\n",
                "## Key Features\n",
                "\n",
                "- üîç **Automatic Detection**: Detects profile requirements from AWS errors\n",
                "- üéØ **Intelligent Selection**: Automatically selects optimal inference profiles\n",
                "- üß† **Learning System**: Learns and remembers successful access methods\n",
                "- ‚ö° **Zero Configuration**: No code changes required - works automatically\n",
                "- üîÑ **Backward Compatible**: Models with direct access continue working unchanged\n",
                "- üìä **Observable**: Comprehensive logging and statistics\n",
                "\n",
                "## What Are Inference Profiles?\n",
                "\n",
                "Inference profiles (also called CRIS profiles) are AWS Bedrock resources that provide access to foundation models. Some newer models (like Claude Sonnet 4.5) require profile-based access instead of direct model ID invocation.\n",
                "\n",
                "**Access Methods:**\n",
                "- **Direct Access**: Using model ID directly (e.g., `anthropic.claude-3-haiku-20240307-v1:0`)\n",
                "- **Regional CRIS**: Using regional inference profile ARN\n",
                "- **Global CRIS**: Using global inference profile ID"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import json\n",
                "from pathlib import Path\n",
                "import logging\n",
                "from datetime import datetime\n",
                "\n",
                "# Add the src directory to path for imports\n",
                "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
                "\n",
                "# Import the LLMManager and related classes\n",
                "from bestehorn_llmmanager.llm_manager import LLMManager\n",
                "from bestehorn_llmmanager.bedrock.models.llm_manager_structures import AuthConfig, RetryConfig, AuthenticationType\n",
                "from bestehorn_llmmanager.bedrock.tracking.access_method_tracker import AccessMethodTracker\n",
                "from bestehorn_llmmanager.bedrock.catalog.bedrock_catalog import BedrockModelCatalog\n",
                "\n",
                "# Configure logging to see profile support in action\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Imports successful!\")\n",
                "print(f\"üìÅ Working directory: {Path.cwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 1: Automatic Profile Detection üîç\n",
                "\n",
                "Let's demonstrate how the system automatically detects and handles models that require inference profiles."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîç Example 1: Automatic Profile Detection\")\n",
                "print(\"=\" * 45)\n",
                "\n",
                "# Initialize LLMManager with a model that requires inference profile\n",
                "# No special configuration needed - it works automatically!\n",
                "manager = LLMManager(\n",
                "    models=[\"Claude Sonnet 4.5\"],  # Requires inference profile\n",
                "    regions=[\"us-east-1\", \"us-west-2\"],\n",
                "    log_level=logging.INFO  # See profile detection in logs\n",
                ")\n",
                "\n",
                "print(\"‚úÖ LLMManager initialized\")\n",
                "print(\"   Model: Claude Sonnet 4.5 (requires inference profile)\")\n",
                "print(\"   Regions: us-east-1, us-west-2\")\n",
                "print(\"\\nüìù Watch the logs below to see automatic profile detection...\\n\")\n",
                "\n",
                "# Simple message\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\"text\": \"Hello! Please introduce yourself briefly.\"}\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "try:\n",
                "    # Send request - system will automatically:\n",
                "    # 1. Try direct access (fails with ValidationException)\n",
                "    # 2. Detect profile requirement\n",
                "    # 3. Select inference profile\n",
                "    # 4. Retry with profile (succeeds)\n",
                "    # 5. Learn preference for future requests\n",
                "    response = manager.converse(messages=messages)\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 45)\n",
                "    print(\"üìä Response Details\")\n",
                "    print(\"=\" * 45)\n",
                "    print(f\"‚úÖ Success: {response.success}\")\n",
                "    print(f\"ü§ñ Model: {response.model_used}\")\n",
                "    print(f\"üåç Region: {response.region_used}\")\n",
                "    print(f\"üîó Access Method: {response.access_method_used}\")\n",
                "    print(f\"üìã Profile Used: {response.inference_profile_used}\")\n",
                "    \n",
                "    if response.inference_profile_used:\n",
                "        print(f\"üÜî Profile ID: {response.inference_profile_id}\")\n",
                "    \n",
                "    print(f\"\\nüí¨ Response: {response.get_content()[:200]}...\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")\n",
                "    print(f\"   Type: {type(e).__name__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 2: Access Method Learning üß†\n",
                "\n",
                "The system learns which access method works for each model/region combination and uses it immediately on subsequent requests."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üß† Example 2: Access Method Learning\")\n",
                "print(\"=\" * 40)\n",
                "\n",
                "# Execute multiple requests to see learning in action\n",
                "print(\"\\nüìù Executing 3 requests to demonstrate learning...\\n\")\n",
                "\n",
                "for i in range(3):\n",
                "    messages = [\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": [\n",
                "                {\"text\": f\"Request {i+1}: What is {2+i} + {3+i}?\"}\n",
                "            ]\n",
                "        }\n",
                "    ]\n",
                "    \n",
                "    response = manager.converse(messages=messages)\n",
                "    \n",
                "    print(f\"Request {i+1}:\")\n",
                "    print(f\"  Access Method: {response.access_method_used}\")\n",
                "    print(f\"  Duration: {response.total_duration_ms:.2f}ms\")\n",
                "    print(f\"  Response: {response.get_content()[:100]}...\")\n",
                "    print()\n",
                "\n",
                "print(\"\\nüí° Notice:\")\n",
                "print(\"   - First request may take longer (profile detection)\")\n",
                "print(\"   - Subsequent requests are faster (uses learned preference)\")\n",
                "print(\"   - All requests use the same access method after learning\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 3: Checking Access Method Statistics üìä\n",
                "\n",
                "Monitor access method usage across your application using the AccessMethodTracker."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üìä Example 3: Access Method Statistics\")\n",
                "print(\"=\" * 40)\n",
                "\n",
                "# Get the singleton tracker instance\n",
                "tracker = AccessMethodTracker.get_instance()\n",
                "\n",
                "# Get comprehensive statistics\n",
                "stats = tracker.get_statistics()\n",
                "\n",
                "print(\"\\nüìà Global Access Method Statistics:\")\n",
                "print(f\"   Total tracked combinations: {stats['total_tracked']}\")\n",
                "print(f\"   Profile-required models: {stats['profile_required_count']}\")\n",
                "print(f\"   Direct-access models: {stats['direct_access_count']}\")\n",
                "\n",
                "if stats.get('access_method_distribution'):\n",
                "    print(f\"\\nüîó Access Method Distribution:\")\n",
                "    for method, count in stats['access_method_distribution'].items():\n",
                "        print(f\"   {method}: {count}\")\n",
                "\n",
                "# Check specific model/region preference\n",
                "model_id = \"anthropic.claude-sonnet-4-20250514-v1:0\"\n",
                "region = \"us-east-1\"\n",
                "\n",
                "requires_profile = tracker.requires_profile(model_id, region)\n",
                "print(f\"\\nüîç Model Analysis:\")\n",
                "print(f\"   Model: {model_id}\")\n",
                "print(f\"   Region: {region}\")\n",
                "print(f\"   Requires Profile: {requires_profile}\")\n",
                "\n",
                "preference = tracker.get_preference(model_id, region)\n",
                "if preference:\n",
                "    print(f\"   Preferred Method: {preference.get_preferred_method()}\")\n",
                "    print(f\"   Learned from Error: {preference.learned_from_error}\")\n",
                "    print(f\"   Last Updated: {preference.last_updated}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 4: Mixed Access Methods üîÑ\n",
                "\n",
                "Demonstrate using models with different access requirements in the same manager."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîÑ Example 4: Mixed Access Methods\")\n",
                "print(\"=\" * 40)\n",
                "\n",
                "# Initialize manager with models that have different access requirements\n",
                "mixed_manager = LLMManager(\n",
                "    models=[\n",
                "        \"Claude Sonnet 4.5\",  # Requires inference profile\n",
                "        \"Claude 3 Haiku\"      # Supports direct access\n",
                "    ],\n",
                "    regions=[\"us-east-1\"],\n",
                "    log_level=logging.INFO\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Manager initialized with mixed access methods\")\n",
                "print(\"   Model 1: Claude Sonnet 4.5 (requires profile)\")\n",
                "print(\"   Model 2: Claude 3 Haiku (supports direct)\")\n",
                "\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\"text\": \"What is 5 + 7?\"}\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "# Execute request - system will use appropriate access method for whichever model succeeds\n",
                "response = mixed_manager.converse(messages=messages)\n",
                "\n",
                "print(f\"\\nüìä Response Details:\")\n",
                "print(f\"   Model Used: {response.model_used}\")\n",
                "print(f\"   Access Method: {response.access_method_used}\")\n",
                "print(f\"   Profile Used: {response.inference_profile_used}\")\n",
                "print(f\"   Response: {response.get_content()}\")\n",
                "\n",
                "print(f\"\\nüí° The system automatically used the correct access method\")\n",
                "print(f\"   for the model that succeeded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 5: Checking Model Availability üîç\n",
                "\n",
                "Use the BedrockModelCatalog to check model availability and access methods."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîç Example 5: Checking Model Availability\")\n",
                "print(\"=\" * 45)\n",
                "\n",
                "# Initialize catalog\n",
                "catalog = BedrockModelCatalog()\n",
                "\n",
                "# Check specific model\n",
                "model_name = \"Claude Sonnet 4.5\"\n",
                "region = \"us-east-1\"\n",
                "\n",
                "print(f\"\\nüìã Checking: {model_name} in {region}\")\n",
                "\n",
                "is_available = catalog.is_model_available(model_name, region)\n",
                "print(f\"   Available: {is_available}\")\n",
                "\n",
                "if is_available:\n",
                "    model_info = catalog.get_model_info(model_name, region)\n",
                "    \n",
                "    print(f\"\\nüìä Model Information:\")\n",
                "    print(f\"   Model ID: {model_info.model_id}\")\n",
                "    print(f\"   Inference Profile: {model_info.inference_profile_id}\")\n",
                "    print(f\"   Access Method: {model_info.access_method.value}\")\n",
                "    print(f\"   Has Direct Access: {model_info.has_direct_access}\")\n",
                "    print(f\"   Has Regional CRIS: {model_info.has_regional_cris}\")\n",
                "    print(f\"   Has Global CRIS: {model_info.has_global_cris}\")\n",
                "    print(f\"   Supports Streaming: {model_info.supports_streaming}\")\n",
                "\n",
                "# Check catalog metadata\n",
                "metadata = catalog.get_catalog_metadata()\n",
                "print(f\"\\nüìö Catalog Metadata:\")\n",
                "print(f\"   Source: {metadata.source.value}\")\n",
                "print(f\"   Retrieved: {metadata.retrieval_timestamp}\")\n",
                "print(f\"   Regions Queried: {len(metadata.api_regions_queried)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 6: Monitoring Access Method Distribution üìà\n",
                "\n",
                "Execute multiple requests and analyze the distribution of access methods used."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üìà Example 6: Access Method Distribution\")\n",
                "print(\"=\" * 42)\n",
                "\n",
                "# Track access methods across multiple requests\n",
                "access_methods = []\n",
                "\n",
                "print(\"\\nüîÑ Executing 10 requests...\\n\")\n",
                "\n",
                "for i in range(10):\n",
                "    messages = [\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": [\n",
                "                {\"text\": f\"Request {i+1}: Count to {i+1}\"}\n",
                "            ]\n",
                "        }\n",
                "    ]\n",
                "    \n",
                "    response = manager.converse(messages=messages)\n",
                "    access_methods.append(response.access_method_used)\n",
                "    print(f\"Request {i+1:2d}: {response.access_method_used:15s} ({response.total_duration_ms:6.2f}ms)\")\n",
                "\n",
                "# Analyze distribution\n",
                "from collections import Counter\n",
                "distribution = Counter(access_methods)\n",
                "\n",
                "print(f\"\\nüìä Access Method Distribution:\")\n",
                "for method, count in distribution.items():\n",
                "    percentage = (count / len(access_methods)) * 100\n",
                "    print(f\"   {method:15s}: {count:2d} ({percentage:5.1f}%)\")\n",
                "\n",
                "print(f\"\\nüí° Observations:\")\n",
                "print(f\"   - After learning, all requests use the same access method\")\n",
                "print(f\"   - This demonstrates the learning system working correctly\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 7: Backward Compatibility ‚úÖ\n",
                "\n",
                "Demonstrate that models with direct access continue working without changes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"‚úÖ Example 7: Backward Compatibility\")\n",
                "print(\"=\" * 40)\n",
                "\n",
                "# Initialize manager with model that supports direct access\n",
                "direct_manager = LLMManager(\n",
                "    models=[\"Claude 3 Haiku\"],  # Supports direct access\n",
                "    regions=[\"us-east-1\"],\n",
                "    log_level=logging.INFO\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Manager initialized with direct-access model\")\n",
                "print(\"   Model: Claude 3 Haiku (supports direct access)\")\n",
                "\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\"text\": \"Hello! What is 10 + 15?\"}\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "response = direct_manager.converse(messages=messages)\n",
                "\n",
                "print(f\"\\nüìä Response Details:\")\n",
                "print(f\"   Success: {response.success}\")\n",
                "print(f\"   Model: {response.model_used}\")\n",
                "print(f\"   Access Method: {response.access_method_used}\")\n",
                "print(f\"   Profile Used: {response.inference_profile_used}\")\n",
                "print(f\"   Response: {response.get_content()}\")\n",
                "\n",
                "print(f\"\\nüí° Notice:\")\n",
                "print(f\"   - Access method is 'direct' (no profile needed)\")\n",
                "print(f\"   - Profile used is False\")\n",
                "print(f\"   - Existing code works without any changes\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 8: Troubleshooting Profile Issues üîß\n",
                "\n",
                "Demonstrate how to diagnose and troubleshoot profile-related issues."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîß Example 8: Troubleshooting Profile Issues\")\n",
                "print(\"=\" * 47)\n",
                "\n",
                "# Enable DEBUG logging for detailed troubleshooting\n",
                "debug_manager = LLMManager(\n",
                "    models=[\"Claude Sonnet 4.5\"],\n",
                "    regions=[\"us-east-1\"],\n",
                "    log_level=logging.DEBUG  # Most detailed logging\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Manager initialized with DEBUG logging\")\n",
                "print(\"   This will show detailed profile retry flow\\n\")\n",
                "\n",
                "messages = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\"text\": \"Hello!\"}\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "response = debug_manager.converse(messages=messages)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 47)\n",
                "print(\"üìä Diagnostic Information\")\n",
                "print(\"=\" * 47)\n",
                "\n",
                "# Response information\n",
                "print(f\"\\n‚úÖ Response Status:\")\n",
                "print(f\"   Success: {response.success}\")\n",
                "print(f\"   Model: {response.model_used}\")\n",
                "print(f\"   Region: {response.region_used}\")\n",
                "print(f\"   Access Method: {response.access_method_used}\")\n",
                "print(f\"   Profile Used: {response.inference_profile_used}\")\n",
                "\n",
                "# Warnings\n",
                "warnings = response.get_warnings()\n",
                "if warnings:\n",
                "    print(f\"\\n‚ö†Ô∏è  Warnings:\")\n",
                "    for warning in warnings:\n",
                "        print(f\"   - {warning}\")\n",
                "\n",
                "# Tracker statistics\n",
                "tracker = AccessMethodTracker.get_instance()\n",
                "stats = tracker.get_statistics()\n",
                "print(f\"\\nüìà Tracker Statistics:\")\n",
                "print(f\"   Total tracked: {stats['total_tracked']}\")\n",
                "print(f\"   Profile required: {stats['profile_required_count']}\")\n",
                "print(f\"   Direct access: {stats['direct_access_count']}\")\n",
                "\n",
                "# Catalog metadata\n",
                "catalog = BedrockModelCatalog()\n",
                "metadata = catalog.get_catalog_metadata()\n",
                "print(f\"\\nüìö Catalog Information:\")\n",
                "print(f\"   Source: {metadata.source.value}\")\n",
                "print(f\"   Retrieved: {metadata.retrieval_timestamp}\")\n",
                "\n",
                "print(f\"\\nüí° Troubleshooting Tips:\")\n",
                "print(f\"   1. Check DEBUG logs above for detailed retry flow\")\n",
                "print(f\"   2. Verify access_method_used matches expected behavior\")\n",
                "print(f\"   3. Check warnings for any profile-related issues\")\n",
                "print(f\"   4. Review tracker statistics for learning patterns\")\n",
                "print(f\"   5. Ensure catalog source is 'api' for latest data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary üìù\n",
                "\n",
                "This notebook demonstrated the automatic inference profile support in LLMManager:\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "1. **Automatic Detection**: System detects profile requirements from AWS errors\n",
                "2. **Intelligent Selection**: Automatically selects optimal inference profiles\n",
                "3. **Learning System**: Learns successful access methods for future optimization\n",
                "4. **Zero Configuration**: No code changes required - works transparently\n",
                "5. **Backward Compatible**: Existing code with direct-access models works unchanged\n",
                "6. **Observable**: Comprehensive logging and statistics for monitoring\n",
                "\n",
                "### Access Method Preference Order\n",
                "\n",
                "1. **Direct Access** - Fastest, lowest latency (preferred when available)\n",
                "2. **Regional CRIS** - Region-specific inference profile\n",
                "3. **Global CRIS** - Cross-region inference profile\n",
                "\n",
                "### Best Practices\n",
                "\n",
                "1. **Use Multiple Regions**: Provide redundancy for profile availability\n",
                "2. **Include Fallback Models**: Mix profile-required and direct-access models\n",
                "3. **Enable Appropriate Logging**: Use INFO or DEBUG for development, WARNING for production\n",
                "4. **Monitor Statistics**: Track access method distribution using AccessMethodTracker\n",
                "5. **Refresh Catalog**: Periodically refresh model data for latest availability\n",
                "\n",
                "### Additional Resources\n",
                "\n",
                "- **Documentation**: See `docs/forLLMConsumption.md` for complete API reference\n",
                "- **Troubleshooting**: See `docs/INFERENCE_PROFILE_TROUBLESHOOTING.md` for detailed troubleshooting guide\n",
                "- **Examples**: See other notebooks for more usage patterns\n",
                "\n",
                "### Performance Impact\n",
                "\n",
                "- **First Request**: May include one additional retry for profile detection\n",
                "- **Subsequent Requests**: No overhead - uses learned preference\n",
                "- **Learning**: Automatic and transparent - no manual configuration needed\n",
                "\n",
                "The inference profile support makes working with newer AWS Bedrock models seamless and transparent!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}